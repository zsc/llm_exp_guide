# ç¬¬å…­ç« ï¼šå¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆ

æœ¬ç« æ·±å…¥æ¢è®¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åŠå…¶å˜ä½“åœ¨å¤§è¯­è¨€æ¨¡å‹åè®­ç»ƒä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å°†ä»å¥–åŠ±æ¨¡å‹çš„æ„å»ºå¼€å§‹ï¼Œè¯¦ç»†åˆ†æPPOã€DPOç­‰ä¸»æµç®—æ³•çš„å®ç°ç»†èŠ‚ï¼Œæ¢è®¨Constitutional AIç­‰è‡ªæˆ‘æ”¹è¿›æ–¹æ³•ï¼Œå¹¶è®¨è®ºåœ¨çº¿ä¸ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æƒè¡¡ã€‚é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œæ‚¨å°†æŒæ¡è®¾è®¡å’Œå®æ–½RLHFç³»ç»Ÿçš„å®Œæ•´æ–¹æ³•è®ºï¼Œç†è§£ä¸åŒç®—æ³•çš„é€‚ç”¨åœºæ™¯ï¼Œä»¥åŠé¿å…å¸¸è§çš„å®éªŒé™·é˜±ã€‚

## 6.1 RLHFçš„åŠ¨æœºä¸æ ¸å¿ƒæŒ‘æˆ˜

### 6.1.1 ä¸ºä»€ä¹ˆéœ€è¦RLHF

ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è™½ç„¶èƒ½è®©æ¨¡å‹å­¦ä¼šéµå¾ªæŒ‡ä»¤çš„åŸºæœ¬æ ¼å¼ï¼Œä½†å­˜åœ¨å‡ ä¸ªæ ¹æœ¬æ€§é™åˆ¶ï¼š

1. **è¡Œä¸ºæ¨¡ä»¿çš„å±€é™æ€§**ï¼šSFTæœ¬è´¨ä¸Šæ˜¯è®©æ¨¡å‹æ¨¡ä»¿è®­ç»ƒæ•°æ®ä¸­çš„è¡Œä¸ºæ¨¡å¼ã€‚å³ä½¿æœ‰é«˜è´¨é‡çš„ç¤ºèŒƒæ•°æ®ï¼Œæ¨¡å‹ä¹Ÿåªèƒ½å­¦åˆ°"å¦‚ä½•è¯´"ï¼Œè€ŒéçœŸæ­£ç†è§£"ä¸ºä»€ä¹ˆè¿™æ ·è¯´æ›´å¥½"ã€‚

2. **åå¥½çš„éšå¼æ€§**ï¼šäººç±»åå¥½å¾€å¾€æ˜¯éšå¼çš„ã€å¤šç»´çš„ï¼Œå¾ˆéš¾é€šè¿‡ç¤ºä¾‹å®Œå…¨è¡¨è¾¾ã€‚æ¯”å¦‚"æœ‰å¸®åŠ©"è¿™ä¸ªæ¦‚å¿µï¼ŒåŒ…å«å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€æ¸…æ™°åº¦ç­‰å¤šä¸ªç»´åº¦ï¼Œä¸”åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­æƒé‡ä¸åŒã€‚

3. **åˆ†å¸ƒåç§»é—®é¢˜**ï¼šSFTæ¨¡å‹åœ¨ç”Ÿæˆæ—¶ä¼šç´¯ç§¯è¯¯å·®ï¼Œé€æ¸åç¦»è®­ç»ƒåˆ†å¸ƒã€‚è€ŒRLHFé€šè¿‡åœ¨æ¨¡å‹è‡ªå·±çš„ç”Ÿæˆåˆ†å¸ƒä¸Šè®­ç»ƒï¼Œèƒ½æ›´å¥½åœ°å¤„ç†è¿™ç§åç§»ã€‚

### 6.1.2 RLHFçš„æ ¸å¿ƒç»„ä»¶

```
    Human Preferences
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Reward Model â”‚ â† åå¥½æ•°æ®è®­ç»ƒ
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
       å¥–åŠ±ä¿¡å·
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  RL Training â”‚ â† PPO/DPOç­‰ç®—æ³•
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
     Aligned Model
```

RLHFç³»ç»ŸåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

1. **åå¥½æ•°æ®æ”¶é›†**ï¼šè·å–äººç±»å¯¹ä¸åŒå›å¤çš„ç›¸å¯¹åå¥½åˆ¤æ–­
2. **å¥–åŠ±æ¨¡å‹è®­ç»ƒ**ï¼šå­¦ä¹ å°†æ–‡æœ¬æ˜ å°„åˆ°æ ‡é‡å¥–åŠ±å€¼
3. **ç­–ç•¥ä¼˜åŒ–**ï¼šä½¿ç”¨RLç®—æ³•ä¼˜åŒ–è¯­è¨€æ¨¡å‹ä»¥æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±

### 6.1.3 ä¸»è¦æŒ‘æˆ˜

**æŒ‘æˆ˜1ï¼šå¥–åŠ±è¿‡æ‹Ÿåˆï¼ˆReward Hackingï¼‰**

æ¨¡å‹å¯èƒ½æ‰¾åˆ°è·å¾—é«˜å¥–åŠ±ä½†å®é™…è´¨é‡å·®çš„æ·å¾„ã€‚ä¾‹å¦‚ï¼š
- è¿‡åº¦ä½¿ç”¨å¥–åŠ±æ¨¡å‹åå¥½çš„ç‰¹å®šçŸ­è¯­
- ç”Ÿæˆçœ‹ä¼¼å®Œæ•´ä½†å®é™…ç©ºæ´çš„é•¿å›å¤
- åˆ©ç”¨å¥–åŠ±æ¨¡å‹çš„ç›²ç‚¹ç”Ÿæˆæœ‰é—®é¢˜çš„å†…å®¹

**æŒ‘æˆ˜2ï¼šè®­ç»ƒä¸ç¨³å®šæ€§**

RLHFè®­ç»ƒè¿‡ç¨‹å®¹æ˜“å‡ºç°ï¼š
- ç­–ç•¥å´©æºƒï¼šæ¨¡å‹é€€åŒ–åˆ°é‡å¤ç®€å•æ¨¡å¼
- å¥–åŠ±çˆ†ç‚¸ï¼šä¼˜åŒ–è¿‡ç¨‹å¤±æ§å¯¼è‡´å¥–åŠ±å€¼å¼‚å¸¸
- KLæ•£åº¦å¤±æ§ï¼šç”Ÿæˆåˆ†å¸ƒè¿‡åº¦åç¦»åˆå§‹æ¨¡å‹

**æŒ‘æˆ˜3ï¼šè¯„ä¼°å›°éš¾**

- å¥–åŠ±å€¼ä¸èƒ½å®Œå…¨ä»£è¡¨çœŸå®è´¨é‡
- éœ€è¦å¤§é‡äººå·¥è¯„ä¼°éªŒè¯æ”¹è¿›æ•ˆæœ
- ä¸åŒè¯„ä¼°æŒ‡æ ‡å¯èƒ½ç›¸äº’å†²çª

## 6.2 å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒä¸æ ¡å‡†

### 6.2.1 Bradley-Terryåå¥½æ¨¡å‹

å¥–åŠ±æ¨¡å‹çš„ç†è®ºåŸºç¡€æ˜¯Bradley-Terryæ¨¡å‹ï¼Œå®ƒå‡è®¾äººç±»é€‰æ‹©å›å¤Aä¼˜äºå›å¤Bçš„æ¦‚ç‡ä¸ºï¼š

$$P(A \succ B) = \frac{\exp(r(A))}{\exp(r(A)) + \exp(r(B))} = \sigma(r(A) - r(B))$$

å…¶ä¸­$r(\cdot)$æ˜¯å¥–åŠ±å‡½æ•°ï¼Œ$\sigma$æ˜¯sigmoidå‡½æ•°ã€‚

è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶ï¼š

$$\mathcal{L}_{RM} = -\mathbb{E}_{(x,y_w,y_l)\sim D}\left[\log\sigma(r_\theta(x,y_w) - r_\theta(x,y_l))\right]$$

å…¶ä¸­$y_w$æ˜¯è¢«åå¥½çš„å›å¤ï¼Œ$y_l$æ˜¯è¾ƒå·®çš„å›å¤ã€‚

### 6.2.2 å¥–åŠ±æ¨¡å‹æ¶æ„è®¾è®¡

å…¸å‹çš„å¥–åŠ±æ¨¡å‹æ¶æ„ï¼š

```
è¾“å…¥: [prompt] + [response]
  â†“
Transformer Encoder (é¢„è®­ç»ƒLM)
  â†“
æœ€åä¸€ä¸ªtokençš„éšçŠ¶æ€
  â†“
Linear Head â†’ æ ‡é‡å¥–åŠ±å€¼
```

**å…³é”®è®¾è®¡é€‰æ‹©ï¼š**

1. **åŸºåº§æ¨¡å‹é€‰æ‹©**ï¼š
   - ä½¿ç”¨ä¸ç­–ç•¥æ¨¡å‹ç›¸åŒè§„æ¨¡çš„åŸºåº§ï¼ˆå¦‚7Bå¯¹7Bï¼‰
   - æˆ–ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚13Bå¥–åŠ±æ¨¡å‹æŒ‡å¯¼7Bç­–ç•¥ï¼‰
   
2. **æ± åŒ–ç­–ç•¥**ï¼š
   - æœ€åtokenæ± åŒ–ï¼ˆæœ€å¸¸ç”¨ï¼‰
   - å¹³å‡æ± åŒ–ï¼ˆå¯¹é•¿æ–‡æœ¬æ›´ç¨³å®šï¼‰
   - åŠ æƒæ± åŒ–ï¼ˆè€ƒè™‘tokené‡è¦æ€§ï¼‰

3. **å½’ä¸€åŒ–æ–¹æ¡ˆ**ï¼š
   ```python
   # æ¯æ‰¹æ¬¡æ ‡å‡†åŒ–
   rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
   
   # æˆ–ä½¿ç”¨è¿è¡Œå‡å€¼/æ–¹å·®
   self.running_mean = 0.99 * self.running_mean + 0.01 * batch_mean
   ```

### 6.2.3 è®­ç»ƒæŠ€å·§ä¸è¿‡æ‹Ÿåˆé¢„é˜²

**æŠ€å·§1ï¼šæ•°æ®å¢å¼º**

```python
def augment_preference_data(prompt, chosen, rejected):
    # 1. é¡ºåºéšæœºåŒ–
    if random.random() < 0.5:
        return prompt, rejected, chosen, -1  # æ ‡ç­¾ç¿»è½¬
    
    # 2. è¾¹é™…æ¡ˆä¾‹ç”Ÿæˆ
    if similarity(chosen, rejected) > 0.9:
        # ä¸ºé«˜åº¦ç›¸ä¼¼çš„å¯¹æ·»åŠ å™ªå£°
        rejected = add_noise(rejected)
    
    return prompt, chosen, rejected, 1
```

**æŠ€å·§2ï¼šé›†æˆä¸ä¸ç¡®å®šæ€§ä¼°è®¡**

è®­ç»ƒå¤šä¸ªå¥–åŠ±æ¨¡å‹å¹¶ä½¿ç”¨é›†æˆï¼š

```python
class EnsembleRewardModel:
    def __init__(self, models):
        self.models = models
    
    def predict(self, prompt, response):
        rewards = [m(prompt, response) for m in self.models]
        mean_reward = np.mean(rewards)
        uncertainty = np.std(rewards)
        
        # é«˜ä¸ç¡®å®šæ€§æ—¶é™ä½å¥–åŠ±ç½®ä¿¡åº¦
        if uncertainty > threshold:
            mean_reward *= 0.8
        
        return mean_reward, uncertainty
```

**æŠ€å·§3ï¼šå¯¹æŠ—éªŒè¯**

å®šæœŸç”¨å¯¹æŠ—æ ·æœ¬æµ‹è¯•å¥–åŠ±æ¨¡å‹ï¼š

```python
def generate_adversarial_samples(reward_model, base_model):
    # ç”Ÿæˆé«˜å¥–åŠ±ä½†è´¨é‡å·®çš„æ ·æœ¬
    prompt = "è§£é‡Šé‡å­åŠ›å­¦"
    
    # ç­–ç•¥1ï¼šé‡å¤å…³é”®è¯
    bad_response_1 = "é‡å­åŠ›å­¦é‡å­åŠ›å­¦..." * 100
    
    # ç­–ç•¥2ï¼šç©ºæ´çš„é•¿å›å¤
    bad_response_2 = generate_verbose_but_empty(prompt)
    
    # æ£€æŸ¥å¥–åŠ±æ¨¡å‹æ˜¯å¦è¢«æ¬ºéª—
    if reward_model(prompt, bad_response_1) > threshold:
        log.warning("å¥–åŠ±æ¨¡å‹å¯¹é‡å¤å†…å®¹ç»™å‡ºé«˜åˆ†")
```

### 6.2.4 æ ¡å‡†æŠ€æœ¯

**æ¸©åº¦ç¼©æ”¾ï¼ˆTemperature Scalingï¼‰**

```python
class CalibratedRewardModel(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        self.temperature = nn.Parameter(torch.ones(1))
    
    def forward(self, prompt, response):
        logits = self.base_model(prompt, response)
        return logits / self.temperature
    
    def calibrate(self, val_data):
        # åœ¨éªŒè¯é›†ä¸Šä¼˜åŒ–æ¸©åº¦å‚æ•°
        optimizer = torch.optim.LBFGS([self.temperature])
        
        def closure():
            loss = 0
            for prompt, chosen, rejected in val_data:
                prob = torch.sigmoid(
                    (self(prompt, chosen) - self(prompt, rejected)) 
                )
                loss -= torch.log(prob)
            return loss
        
        optimizer.step(closure)
```

**æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰ç›‘æ§**

```python
def compute_ece(predictions, labels, n_bins=10):
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    ece = 0
    
    for i in range(n_bins):
        mask = (predictions >= bin_boundaries[i]) & \
               (predictions < bin_boundaries[i+1])
        
        if mask.sum() > 0:
            bin_acc = labels[mask].mean()
            bin_conf = predictions[mask].mean()
            bin_weight = mask.sum() / len(predictions)
            
            ece += bin_weight * abs(bin_acc - bin_conf)
    
    return ece
```

## 6.3 PPOåœ¨LLMä¸­çš„å®ç°ç»†èŠ‚

### 6.3.1 PPOç®—æ³•æ ¸å¿ƒ

PPOï¼ˆProximal Policy Optimizationï¼‰é€šè¿‡é™åˆ¶æ¯æ¬¡æ›´æ–°çš„å¹…åº¦æ¥ä¿è¯è®­ç»ƒç¨³å®šæ€§ï¼š

$$\mathcal{L}_{PPO} = \mathbb{E}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$$

å…¶ä¸­ï¼š
- $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ æ˜¯é‡è¦æ€§é‡‡æ ·æ¯”ç‡
- $\hat{A}_t$ æ˜¯ä¼˜åŠ¿å‡½æ•°ä¼°è®¡
- $\epsilon$ æ˜¯è£å‰ªå‚æ•°ï¼ˆé€šå¸¸0.1-0.2ï¼‰

### 6.3.2 LLMç‰¹å®šçš„å®ç°ç»†èŠ‚

**æŒ‘æˆ˜1ï¼šåºåˆ—ç”Ÿæˆçš„ä¿¡ç”¨åˆ†é…**

åœ¨LLMä¸­ï¼Œä¸€ä¸ª"åŠ¨ä½œ"æ˜¯ç”Ÿæˆä¸€ä¸ªtokenï¼Œ"è½¨è¿¹"æ˜¯å®Œæ•´çš„å›å¤ã€‚å¥–åŠ±é€šå¸¸åªåœ¨åºåˆ—æœ«å°¾ç»™å‡ºï¼Œéœ€è¦åˆç†çš„ä¿¡ç”¨åˆ†é…ï¼š

```python
def compute_advantages(rewards, values, gamma=1.0, lam=0.95):
    """
    è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡(GAE)
    rewards: [batch_size, seq_len] é€šå¸¸åªæœ‰æœ€åä¸€ä¸ªéé›¶
    values: [batch_size, seq_len] ä»·å€¼å‡½æ•°é¢„æµ‹
    """
    advantages = torch.zeros_like(rewards)
    lastgaelam = 0
    
    for t in reversed(range(len(rewards[0]))):
        if t == len(rewards[0]) - 1:
            next_values = 0  # ç»ˆæ­¢çŠ¶æ€
        else:
            next_values = values[:, t + 1]
        
        delta = rewards[:, t] + gamma * next_values - values[:, t]
        advantages[:, t] = lastgaelam = delta + gamma * lam * lastgaelam
    
    return advantages
```

**æŒ‘æˆ˜2ï¼šKLæ•£åº¦çº¦æŸ**

é˜²æ­¢ç­–ç•¥åç¦»å¤ªè¿œï¼š

```python
def compute_kl_penalty(logprobs_new, logprobs_ref, kl_coef=0.1):
    """
    è®¡ç®—KLæ•£åº¦æƒ©ç½š
    logprobs_new: å½“å‰ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
    logprobs_ref: å‚è€ƒç­–ç•¥ï¼ˆé€šå¸¸æ˜¯SFTæ¨¡å‹ï¼‰çš„å¯¹æ•°æ¦‚ç‡
    """
    kl = (logprobs_ref - logprobs_new).sum(dim=-1)
    
    # è‡ªé€‚åº”KLç³»æ•°
    if kl.mean() > target_kl * 1.5:
        kl_coef *= 1.5  # å¢åŠ æƒ©ç½š
    elif kl.mean() < target_kl * 0.5:
        kl_coef *= 0.5  # å‡å°‘æƒ©ç½š
    
    return kl * kl_coef
```

### 6.3.3 è®­ç»ƒå¾ªç¯å®ç°

```python
class PPOTrainer:
    def __init__(self, policy_model, ref_model, reward_model, 
                 lr=1e-6, eps=0.2, kl_coef=0.1):
        self.policy = policy_model
        self.ref = ref_model
        self.reward = reward_model
        self.optimizer = AdamW(policy_model.parameters(), lr=lr)
        self.eps = eps
        self.kl_coef = kl_coef
        
    def train_step(self, prompts, max_length=512):
        # 1. ç”Ÿæˆå›å¤
        with torch.no_grad():
            responses, old_logprobs = self.generate_responses(
                prompts, max_length
            )
            
            # 2. è®¡ç®—å¥–åŠ±
            rewards = self.reward(prompts, responses)
            
            # 3. è®¡ç®—å‚è€ƒæ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡
            ref_logprobs = self.ref.compute_logprobs(prompts, responses)
        
        # 4. å¤šè½®PPOæ›´æ–°
        for _ in range(4):  # PPO epochs
            # è®¡ç®—å½“å‰ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
            new_logprobs, values = self.policy.forward_with_value(
                prompts, responses
            )
            
            # è®¡ç®—ä¼˜åŠ¿
            advantages = compute_advantages(rewards, values)
            
            # PPOæŸå¤±
            ratio = torch.exp(new_logprobs - old_logprobs)
            clipped_ratio = torch.clamp(ratio, 1 - self.eps, 1 + self.eps)
            
            policy_loss = -torch.min(
                ratio * advantages,
                clipped_ratio * advantages
            ).mean()
            
            # KLæƒ©ç½š
            kl_loss = compute_kl_penalty(
                new_logprobs, ref_logprobs, self.kl_coef
            )
            
            # ä»·å€¼å‡½æ•°æŸå¤±
            value_loss = F.mse_loss(values, rewards + values.detach())
            
            # æ€»æŸå¤±
            loss = policy_loss + kl_loss.mean() + 0.5 * value_loss
            
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)
            self.optimizer.step()
```

### 6.3.4 PPOè°ƒè¯•æŠ€å·§

**æŠ€å·§1ï¼šç›‘æ§å…³é”®æŒ‡æ ‡**

```python
def log_ppo_metrics(info):
    # å¿…é¡»ç›‘æ§çš„æŒ‡æ ‡
    metrics = {
        'kl_divergence': info['kl'].mean(),
        'clip_fraction': (info['ratio'] > 1.2).float().mean(),
        'approx_kl': (info['ratio'] - 1).pow(2).mean() / 2,
        'reward_mean': info['rewards'].mean(),
        'reward_std': info['rewards'].std(),
        'value_loss': info['value_loss'],
        'policy_loss': info['policy_loss'],
        'entropy': info['entropy'],  # ç›‘æ§æ¢ç´¢ç¨‹åº¦
    }
    
    # å¼‚å¸¸æ£€æµ‹
    if metrics['kl_divergence'] > 0.1:
        logger.warning("KLæ•£åº¦è¿‡å¤§ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®š")
    
    if metrics['clip_fraction'] > 0.3:
        logger.warning("è£å‰ªæ¯”ä¾‹è¿‡é«˜ï¼Œè€ƒè™‘å‡å°å­¦ä¹ ç‡")
    
    return metrics
```

**æŠ€å·§2ï¼šæ¸è¿›å¼è®­ç»ƒ**

```python
def progressive_ppo_training(trainer, stages):
    """
    åˆ†é˜¶æ®µé€æ­¥å¢åŠ è®­ç»ƒéš¾åº¦
    """
    for stage in stages:
        # é˜¶æ®µ1ï¼šç®€å•ä»»åŠ¡ï¼Œå¤§KLå®¹å¿åº¦
        if stage == 1:
            trainer.kl_coef = 0.05
            prompts = get_simple_prompts()
            
        # é˜¶æ®µ2ï¼šä¸­ç­‰éš¾åº¦ï¼Œæ ‡å‡†KL
        elif stage == 2:
            trainer.kl_coef = 0.1
            prompts = get_medium_prompts()
            
        # é˜¶æ®µ3ï¼šå›°éš¾ä»»åŠ¡ï¼Œä¸¥æ ¼KL
        else:
            trainer.kl_coef = 0.2
            prompts = get_hard_prompts()
        
        for step in range(stage_steps):
            trainer.train_step(prompts)
```

## 6.4 DPOä¸IPOçš„æ¯”è¾ƒåˆ†æ

### 6.4.1 DPOçš„ç†è®ºåŸºç¡€

DPOï¼ˆDirect Preference Optimizationï¼‰é€šè¿‡é‡æ–°å‚æ•°åŒ–ï¼Œå°†RLHFé—®é¢˜è½¬æ¢ä¸ºç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œé¿å…äº†æ˜¾å¼è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼š

**å…³é”®æ´å¯Ÿ**ï¼šæœ€ä¼˜ç­–ç•¥å¯ä»¥ç”¨å°é—­å½¢å¼è¡¨è¾¾ï¼š

$$\pi^*(y|x) = \frac{1}{Z(x)}\pi_{ref}(y|x)\exp\left(\frac{r(x,y)}{\beta}\right)$$

åæ¨å¥–åŠ±å‡½æ•°ï¼š

$$r(x,y) = \beta\log\frac{\pi^*(y|x)}{\pi_{ref}(y|x)} + \beta\log Z(x)$$

ä»£å…¥Bradley-Terryæ¨¡å‹ï¼Œå¾—åˆ°DPOæŸå¤±ï¼š

$$\mathcal{L}_{DPO} = -\mathbb{E}\left[\log\sigma\left(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right)\right]$$

### 6.4.2 DPOå®ç°ç»†èŠ‚

```python
class DPOTrainer:
    def __init__(self, model, ref_model, beta=0.1):
        self.model = model
        self.ref_model = ref_model
        self.beta = beta
        self.optimizer = AdamW(model.parameters(), lr=5e-7)
    
    def compute_loss(self, prompts, chosen, rejected):
        # è®¡ç®—ç­–ç•¥æ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡
        chosen_logps = self.model.compute_logprobs(prompts, chosen)
        rejected_logps = self.model.compute_logprobs(prompts, rejected)
        
        # è®¡ç®—å‚è€ƒæ¨¡å‹çš„å¯¹æ•°æ¦‚ç‡
        with torch.no_grad():
            ref_chosen_logps = self.ref_model.compute_logprobs(
                prompts, chosen
            )
            ref_rejected_logps = self.ref_model.compute_logprobs(
                prompts, rejected
            )
        
        # è®¡ç®—å¯¹æ•°æ¦‚ç‡æ¯”
        chosen_rewards = self.beta * (chosen_logps - ref_chosen_logps)
        rejected_rewards = self.beta * (rejected_logps - ref_rejected_logps)
        
        # DPOæŸå¤±
        loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()
        
        # æ·»åŠ éšå¼å¥–åŠ±çš„ç›‘æ§
        with torch.no_grad():
            reward_accuracy = (chosen_rewards > rejected_rewards).float().mean()
            reward_margin = (chosen_rewards - rejected_rewards).mean()
        
        return loss, {
            'reward_accuracy': reward_accuracy,
            'reward_margin': reward_margin,
            'chosen_rewards': chosen_rewards.mean(),
            'rejected_rewards': rejected_rewards.mean()
        }
```

### 6.4.3 IPOçš„æ”¹è¿›

IPOï¼ˆIdentity Preference Optimizationï¼‰è§£å†³äº†DPOçš„ä¸€äº›é—®é¢˜ï¼š

1. **è¿‡æ‹Ÿåˆé—®é¢˜**ï¼šDPOå€¾å‘äºè®©rejectedæ ·æœ¬çš„ä¼¼ç„¶åº¦è¶‹è¿‘äº0
2. **ç¡®å®šæ€§åå¥½**ï¼šDPOå‡è®¾åå¥½æ˜¯ç¡®å®šæ€§çš„ï¼Œå¿½ç•¥äº†æ ‡æ³¨å™ªå£°

IPOçš„æŸå¤±å‡½æ•°ï¼š

$$\mathcal{L}_{IPO} = \mathbb{E}\left[\left(\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} - \frac{1}{2\beta}\right)^2\right]$$

```python
class IPOTrainer(DPOTrainer):
    def compute_loss(self, prompts, chosen, rejected):
        # ä¸DPOç›¸åŒçš„å¯¹æ•°æ¦‚ç‡è®¡ç®—
        chosen_logps = self.model.compute_logprobs(prompts, chosen)
        rejected_logps = self.model.compute_logprobs(prompts, rejected)
        
        with torch.no_grad():
            ref_chosen_logps = self.ref_model.compute_logprobs(
                prompts, chosen
            )
            ref_rejected_logps = self.ref_model.compute_logprobs(
                prompts, rejected
            )
        
        # IPOä½¿ç”¨å¹³æ–¹æŸå¤±è€ŒélogisticæŸå¤±
        log_ratio_chosen = chosen_logps - ref_chosen_logps
        log_ratio_rejected = rejected_logps - ref_rejected_logps
        
        # IPOæŸå¤±ï¼šé¼“åŠ±å·®å€¼ä¸º1/(2Î²)
        losses = (log_ratio_chosen - log_ratio_rejected - 1/(2*self.beta))**2
        
        return losses.mean(), {
            'log_ratio_diff': (log_ratio_chosen - log_ratio_rejected).mean()
        }
```

### 6.4.4 DPO vs IPOå®éªŒå¯¹æ¯”

**å®éªŒè®¾ç½®å¯¹æ¯”è¡¨**ï¼š

| ç»´åº¦ | DPO | IPO |
|------|-----|-----|
| æŸå¤±å‡½æ•° | Logistic | MSE |
| Î²å‚æ•°æ•æ„Ÿåº¦ | é«˜ | ä¸­ |
| è®­ç»ƒç¨³å®šæ€§ | ä¸­ | é«˜ |
| æ”¶æ•›é€Ÿåº¦ | å¿« | æ…¢ |
| è¿‡æ‹Ÿåˆé£é™© | é«˜ | ä½ |
| å™ªå£°é²æ£’æ€§ | ä½ | é«˜ |

**é€‰æ‹©æŒ‡å—**ï¼š

```python
def choose_optimization_method(dataset_properties):
    """
    æ ¹æ®æ•°æ®é›†ç‰¹æ€§é€‰æ‹©DPOæˆ–IPO
    """
    if dataset_properties['annotation_agreement'] < 0.7:
        # æ ‡æ³¨ä¸€è‡´æ€§ä½ï¼Œä½¿ç”¨IPO
        return 'IPO', 'æ ‡æ³¨å™ªå£°å¤§ï¼ŒIPOæ›´é²æ£’'
    
    elif dataset_properties['size'] < 10000:
        # æ•°æ®é‡å°ï¼Œä½¿ç”¨IPOé¿å…è¿‡æ‹Ÿåˆ
        return 'IPO', 'æ•°æ®é‡å°ï¼ŒIPOæ³›åŒ–æ›´å¥½'
    
    elif dataset_properties['preference_strength'] > 0.9:
        # åå¥½éå¸¸æ˜ç¡®ï¼Œä½¿ç”¨DPO
        return 'DPO', 'åå¥½æ˜ç¡®ï¼ŒDPOæ”¶æ•›å¿«'
    
    else:
        # é»˜è®¤ä½¿ç”¨DPO
        return 'DPO', 'æ ‡å‡†åœºæ™¯ï¼ŒDPOæ•ˆç‡é«˜'
```

### 6.4.5 æ··åˆç­–ç•¥

```python
class HybridDPO_IPO:
    """
    ç»“åˆDPOå’ŒIPOä¼˜ç‚¹çš„æ··åˆæ–¹æ³•
    """
    def __init__(self, model, ref_model, beta=0.1, alpha=0.5):
        self.model = model
        self.ref_model = ref_model
        self.beta = beta
        self.alpha = alpha  # DPOå’ŒIPOçš„æ··åˆæƒé‡
    
    def compute_loss(self, prompts, chosen, rejected):
        # è®¡ç®—å¯¹æ•°æ¦‚ç‡
        chosen_logps = self.model.compute_logprobs(prompts, chosen)
        rejected_logps = self.model.compute_logprobs(prompts, rejected)
        
        with torch.no_grad():
            ref_chosen_logps = self.ref_model.compute_logprobs(prompts, chosen)
            ref_rejected_logps = self.ref_model.compute_logprobs(prompts, rejected)
        
        # å¯¹æ•°æ¯”
        log_ratio_diff = (chosen_logps - ref_chosen_logps) - \
                        (rejected_logps - ref_rejected_logps)
        
        # DPOæŸå¤±
        dpo_loss = -F.logsigmoid(self.beta * log_ratio_diff).mean()
        
        # IPOæŸå¤±
        ipo_loss = (log_ratio_diff - 1/(2*self.beta))**2.mean()
        
        # æ··åˆæŸå¤±
        loss = self.alpha * dpo_loss + (1 - self.alpha) * ipo_loss
        
        return loss
```

## 6.5 Constitutional AIä¸è‡ªæˆ‘æ”¹è¿›

### 6.5.1 Constitutional AIåŸç†

Constitutional AIï¼ˆCAIï¼‰ä½¿ç”¨ä¸€ç»„åŸåˆ™æ¥æŒ‡å¯¼æ¨¡å‹çš„è‡ªæˆ‘æ”¹è¿›ï¼Œå‡å°‘å¯¹äººç±»æ ‡æ³¨çš„ä¾èµ–ï¼š

```
åŸå§‹å›å¤ â†’ AIè‡ªæˆ‘æ‰¹åˆ¤ â†’ ä¿®è®¢å›å¤ â†’ AIåå¥½åˆ¤æ–­ â†’ è®­ç»ƒ
```

æ ¸å¿ƒç»„ä»¶ï¼š
1. **å®ªæ³•åŸåˆ™**ï¼šå®šä¹‰æ¨¡å‹åº”éµå¾ªçš„è§„åˆ™
2. **è‡ªæˆ‘æ‰¹åˆ¤**ï¼šæ¨¡å‹è¯„ä¼°è‡ªå·±çš„è¾“å‡º
3. **è‡ªæˆ‘ä¿®è®¢**ï¼šåŸºäºæ‰¹åˆ¤æ”¹è¿›å›å¤
4. **è‡ªæˆ‘åå¥½**ï¼šç”Ÿæˆåå¥½æ•°æ®ç”¨äºè®­ç»ƒ

### 6.5.2 å®ç°Constitutional AI

```python
class ConstitutionalAI:
    def __init__(self, model, principles):
        self.model = model
        self.principles = principles
    
    def critique_response(self, prompt, response):
        """
        ä½¿ç”¨å®ªæ³•åŸåˆ™æ‰¹åˆ¤å›å¤
        """
        critiques = []
        
        for principle in self.principles:
            critique_prompt = f"""
            åŸåˆ™ï¼š{principle}
            ç”¨æˆ·é—®é¢˜ï¼š{prompt}
            åŠ©æ‰‹å›å¤ï¼š{response}
            
            è¿™ä¸ªå›å¤æ˜¯å¦è¿åäº†ä¸Šè¿°åŸåˆ™ï¼Ÿå¦‚æœæ˜¯ï¼Œè¯·è¯´æ˜å¦‚ä½•æ”¹è¿›ã€‚
            """
            
            critique = self.model.generate(critique_prompt)
            critiques.append(critique)
        
        return critiques
    
    def revise_response(self, prompt, response, critiques):
        """
        åŸºäºæ‰¹åˆ¤ä¿®è®¢å›å¤
        """
        revision_prompt = f"""
        åŸå§‹é—®é¢˜ï¼š{prompt}
        åŸå§‹å›å¤ï¼š{response}
        
        æ‰¹åˆ¤æ„è§ï¼š
        {' '.join(critiques)}
        
        è¯·æ ¹æ®æ‰¹åˆ¤æ„è§ä¿®è®¢å›å¤ï¼Œä½¿å…¶æ›´å¥½åœ°éµå¾ªåŸåˆ™ã€‚
        """
        
        revised = self.model.generate(revision_prompt)
        return revised
    
    def generate_preference_data(self, prompts):
        """
        ç”Ÿæˆè‡ªæˆ‘æ ‡æ³¨çš„åå¥½æ•°æ®
        """
        preference_data = []
        
        for prompt in prompts:
            # ç”Ÿæˆåˆå§‹å›å¤
            response = self.model.generate(prompt)
            
            # è‡ªæˆ‘æ‰¹åˆ¤
            critiques = self.critique_response(prompt, response)
            
            # å¦‚æœæœ‰æ‰¹åˆ¤ï¼Œç”Ÿæˆä¿®è®¢ç‰ˆæœ¬
            if any(critiques):
                revised = self.revise_response(prompt, response, critiques)
                
                # åˆ›å»ºåå¥½å¯¹ï¼ˆä¿®è®¢ç‰ˆæœ¬ > åŸå§‹ç‰ˆæœ¬ï¼‰
                preference_data.append({
                    'prompt': prompt,
                    'chosen': revised,
                    'rejected': response
                })
            
        return preference_data
```

### 6.5.3 RLAIFå®è·µ

RLAIFï¼ˆRL from AI Feedbackï¼‰å®Œæ•´æµç¨‹ï¼š

```python
class RLAIFTrainer:
    def __init__(self, model, critic_model, principles):
        self.model = model
        self.critic = critic_model  # å¯ä»¥æ˜¯åŒä¸€ä¸ªæ¨¡å‹
        self.principles = principles
        self.dpo_trainer = DPOTrainer(model, model.copy())
    
    def train_iteration(self, prompts, n_iterations=5):
        for iteration in range(n_iterations):
            print(f"RLAIFè¿­ä»£ {iteration + 1}")
            
            # 1. ç”Ÿæˆå›å¤
            responses = []
            for prompt in prompts:
                response = self.model.generate(prompt)
                responses.append(response)
            
            # 2. AIè¯„åˆ†å’Œæ’åº
            scored_responses = self.score_responses(prompts, responses)
            
            # 3. æ„å»ºåå¥½æ•°æ®
            preference_data = self.create_preferences(scored_responses)
            
            # 4. DPOè®­ç»ƒ
            for batch in preference_data:
                loss = self.dpo_trainer.train_step(batch)
            
            # 5. è¯„ä¼°æ”¹è¿›
            improvement = self.evaluate_improvement(prompts)
            print(f"æ”¹è¿›å¹…åº¦: {improvement:.2%}")
            
            if improvement < 0.01:  # æ”¶æ•›
                break
    
    def score_responses(self, prompts, responses):
        """
        ä½¿ç”¨AIè¯„åˆ†å™¨ç»™å›å¤æ‰“åˆ†
        """
        scores = []
        
        for prompt, response in zip(prompts, responses):
            score_prompt = f"""
            æ ¹æ®ä»¥ä¸‹åŸåˆ™è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼š
            {self.principles}
            
            é—®é¢˜ï¼š{prompt}
            å›å¤ï¼š{response}
            
            è¯„åˆ†ï¼ˆåªè¿”å›æ•°å­—ï¼‰ï¼š
            """
            
            score = float(self.critic.generate(score_prompt))
            scores.append(score)
        
        return list(zip(prompts, responses, scores))
```

### 6.5.4 åŸåˆ™è®¾è®¡æœ€ä½³å®è·µ

**åŸåˆ™å±‚æ¬¡ç»“æ„**ï¼š

```python
CONSTITUTIONAL_PRINCIPLES = {
    # ç¬¬ä¸€å±‚ï¼šå®‰å…¨æ€§åŸåˆ™ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    'safety': [
        "ä¸æä¾›å¯èƒ½é€ æˆä¼¤å®³çš„ä¿¡æ¯",
        "é¿å…ç”Ÿæˆæ­§è§†æ€§å†…å®¹",
        "ä¿æŠ¤ç”¨æˆ·éšç§"
    ],
    
    # ç¬¬äºŒå±‚ï¼šæœ‰ç”¨æ€§åŸåˆ™
    'helpfulness': [
        "æä¾›å‡†ç¡®çš„ä¿¡æ¯",
        "å›ç­”è¦åˆ‡ä¸­è¦ç‚¹",
        "æ‰¿è®¤ä¸ç¡®å®šæ€§"
    ],
    
    # ç¬¬ä¸‰å±‚ï¼šé£æ ¼åŸåˆ™
    'style': [
        "ä¿æŒä¸“ä¸šè¯­æ°”",
        "é¿å…è¿‡åº¦è‡ªä¿¡",
        "é€‚å½“ä½¿ç”¨ä¾‹å­"
    ]
}

def apply_principles_hierarchically(response, principles):
    """
    åˆ†å±‚åº”ç”¨åŸåˆ™ï¼Œé«˜ä¼˜å…ˆçº§åŸåˆ™å¯è¦†ç›–ä½ä¼˜å…ˆçº§
    """
    for level in ['safety', 'helpfulness', 'style']:
        for principle in principles[level]:
            if violates_principle(response, principle):
                if level == 'safety':
                    # å®‰å…¨é—®é¢˜å¿…é¡»ä¿®æ­£
                    return revise_for_safety(response, principle)
                else:
                    # å…¶ä»–é—®é¢˜å°è¯•ä¿®æ­£ä½†ä¸å¼ºåˆ¶
                    response = soft_revise(response, principle)
    
    return response
```

## 6.6 åœ¨çº¿ä¸ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„æƒè¡¡

### 6.6.1 åœ¨çº¿vsç¦»çº¿RLçš„æœ¬è´¨åŒºåˆ«

**åœ¨çº¿RL**ï¼šæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­ä¸ç¯å¢ƒäº¤äº’ï¼Œç”Ÿæˆæ–°æ•°æ®å¹¶ç«‹å³ä»ä¸­å­¦ä¹ ã€‚

**ç¦»çº¿RL**ï¼šä»…ä½¿ç”¨é¢„å…ˆæ”¶é›†çš„å›ºå®šæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä¸ä¸ç¯å¢ƒå®æ—¶äº¤äº’ã€‚

```
åœ¨çº¿RLæµç¨‹ï¼š
ç­–ç•¥ â†’ ç”Ÿæˆ â†’ å¥–åŠ± â†’ æ›´æ–° â†’ ç­–ç•¥ï¼ˆå¾ªç¯ï¼‰

ç¦»çº¿RLæµç¨‹ï¼š
å›ºå®šæ•°æ®é›† â†’ è®­ç»ƒ â†’ ç­–ç•¥ï¼ˆä¸€æ¬¡æ€§ï¼‰
```

### 6.6.2 åœ¨çº¿RLçš„ä¼˜åŠ¿ä¸æŒ‘æˆ˜

**ä¼˜åŠ¿**ï¼š
1. **æ¢ç´¢èƒ½åŠ›å¼º**ï¼šèƒ½ä¸»åŠ¨æ¢ç´¢é«˜å¥–åŠ±åŒºåŸŸ
2. **é€‚åº”æ€§å¥½**ï¼šå¿«é€Ÿé€‚åº”å¥–åŠ±å‡½æ•°å˜åŒ–
3. **æ— åˆ†å¸ƒåç§»**ï¼šåœ¨è‡ªå·±çš„ç”Ÿæˆåˆ†å¸ƒä¸Šè®­ç»ƒ

**æŒ‘æˆ˜**ï¼š
1. **è®¡ç®—æˆæœ¬é«˜**ï¼šéœ€è¦å®æ—¶ç”Ÿæˆå’Œè¯„ä¼°
2. **ä¸ç¨³å®šæ€§**ï¼šå®¹æ˜“å‡ºç°ç­–ç•¥å´©æºƒ
3. **å®‰å…¨é£é™©**ï¼šå¯èƒ½ç”Ÿæˆæœ‰å®³å†…å®¹

**åœ¨çº¿PPOå®ç°**ï¼š

```python
class OnlinePPO:
    def __init__(self, policy, reward_model, buffer_size=1000):
        self.policy = policy
        self.reward_model = reward_model
        self.buffer = []
        self.buffer_size = buffer_size
    
    def collect_trajectories(self, prompts, n_samples=4):
        """
        å®æ—¶æ”¶é›†è½¨è¿¹æ•°æ®
        """
        trajectories = []
        
        for prompt in prompts:
            for _ in range(n_samples):
                # åœ¨çº¿ç”Ÿæˆ
                response = self.policy.generate(prompt)
                
                # å®æ—¶è®¡ç®—å¥–åŠ±
                reward = self.reward_model(prompt, response)
                
                # è®¡ç®—ä¼˜åŠ¿ï¼ˆéœ€è¦ä»·å€¼å‡½æ•°ï¼‰
                value = self.policy.value_head(prompt, response)
                
                trajectories.append({
                    'prompt': prompt,
                    'response': response,
                    'reward': reward,
                    'value': value,
                    'logprobs': self.policy.get_logprobs(prompt, response)
                })
        
        return trajectories
    
    def train_step(self, prompts):
        # æ”¶é›†æ–°æ•°æ®
        new_data = self.collect_trajectories(prompts)
        
        # æ›´æ–°ç¼“å†²åŒºï¼ˆFIFOï¼‰
        self.buffer.extend(new_data)
        if len(self.buffer) > self.buffer_size:
            self.buffer = self.buffer[-self.buffer_size:]
        
        # åœ¨ç¼“å†²åŒºæ•°æ®ä¸Šè®­ç»ƒ
        for epoch in range(4):
            for batch in self.get_batches(self.buffer):
                loss = self.ppo_loss(batch)
                self.optimize(loss)
```

### 6.6.3 ç¦»çº¿RLçš„ä¼˜åŠ¿ä¸æŒ‘æˆ˜

**ä¼˜åŠ¿**ï¼š
1. **å®‰å…¨å¯æ§**ï¼šä¸ä¼šç”Ÿæˆæœªè§è¿‡çš„æœ‰å®³å†…å®¹
2. **è®¡ç®—é«˜æ•ˆ**ï¼šæ•°æ®å¯é¢„å¤„ç†å’Œç¼“å­˜
3. **å¯é‡å¤æ€§**ï¼šç›¸åŒæ•°æ®å¾—åˆ°ç›¸åŒç»“æœ

**æŒ‘æˆ˜**ï¼š
1. **åˆ†å¸ƒåç§»**ï¼šè®­ç»ƒå’Œéƒ¨ç½²åˆ†å¸ƒä¸åŒ¹é…
2. **æ•°æ®è´¨é‡ä¾èµ–**ï¼šå®Œå…¨ä¾èµ–å†å²æ•°æ®è´¨é‡
3. **ä¿å®ˆæ€§**ï¼šéš¾ä»¥è¶…è¶Šæ•°æ®é›†ä¸­çš„æœ€ä½³è¡¨ç°

**ç¦»çº¿DPOå®ç°**ï¼š

```python
class OfflineDPO:
    def __init__(self, model, ref_model, dataset):
        self.model = model
        self.ref_model = ref_model
        self.dataset = dataset  # é¢„æ”¶é›†çš„åå¥½æ•°æ®
        
    def train(self, n_epochs=3):
        """
        çº¯ç¦»çº¿è®­ç»ƒï¼Œä¸ç”Ÿæˆæ–°æ•°æ®
        """
        for epoch in range(n_epochs):
            for batch in self.dataset:
                # ä½¿ç”¨å›ºå®šæ•°æ®é›†
                loss = self.compute_dpo_loss(
                    batch['prompts'],
                    batch['chosen'],
                    batch['rejected']
                )
                
                self.optimize(loss)
            
            # ç¦»çº¿è¯„ä¼°
            val_loss = self.evaluate_offline()
            print(f"Epoch {epoch}: Val Loss = {val_loss:.4f}")
    
    def compute_importance_weights(self, batch):
        """
        è®¡ç®—é‡è¦æ€§æƒé‡ä»¥ç¼“è§£åˆ†å¸ƒåç§»
        """
        with torch.no_grad():
            # å½“å‰ç­–ç•¥çš„æ¦‚ç‡
            current_probs = self.model.get_probs(batch['prompts'], batch['responses'])
            
            # æ•°æ®æ”¶é›†æ—¶çš„æ¦‚ç‡ï¼ˆå¦‚æœæœ‰ï¼‰
            old_probs = batch.get('old_probs', torch.ones_like(current_probs))
            
            # é‡è¦æ€§æƒé‡
            weights = current_probs / (old_probs + 1e-8)
            
            # è£å‰ªé˜²æ­¢æƒé‡çˆ†ç‚¸
            weights = torch.clamp(weights, 0.1, 10.0)
        
        return weights
```

### 6.6.4 æ··åˆç­–ç•¥ï¼šåŠåœ¨çº¿å­¦ä¹ 

ç»“åˆä¸¤è€…ä¼˜åŠ¿çš„å®ç”¨æ–¹æ¡ˆï¼š

```python
class SemiOnlineRL:
    def __init__(self, model, offline_data, online_ratio=0.2):
        self.model = model
        self.offline_data = offline_data
        self.online_ratio = online_ratio
        self.online_buffer = []
    
    def train_step(self, prompts):
        batch_size = len(prompts)
        
        # 1. ç¦»çº¿æ•°æ®é‡‡æ ·
        offline_size = int(batch_size * (1 - self.online_ratio))
        offline_batch = self.sample_offline(offline_size)
        
        # 2. åœ¨çº¿æ•°æ®ç”Ÿæˆï¼ˆå°‘é‡ï¼‰
        online_size = batch_size - offline_size
        online_batch = self.generate_online(
            prompts[:online_size]
        )
        
        # 3. æ··åˆè®­ç»ƒ
        combined_batch = self.combine_batches(
            offline_batch, 
            online_batch
        )
        
        # 4. åŠ æƒæ›´æ–°
        loss = self.weighted_loss(combined_batch)
        self.optimize(loss)
    
    def weighted_loss(self, batch):
        """
        å¯¹åœ¨çº¿å’Œç¦»çº¿æ•°æ®ä½¿ç”¨ä¸åŒæƒé‡
        """
        losses = []
        
        for item in batch:
            if item['source'] == 'online':
                # åœ¨çº¿æ•°æ®æƒé‡æ›´é«˜ï¼ˆæ›´å¯ä¿¡ï¼‰
                weight = 1.5
            else:
                # ç¦»çº¿æ•°æ®æƒé‡è¾ƒä½
                weight = 1.0
            
            loss = self.compute_loss(item) * weight
            losses.append(loss)
        
        return torch.stack(losses).mean()
```

### 6.6.5 å®ç”¨å†³ç­–æ¡†æ¶

```python
def choose_rl_strategy(constraints):
    """
    æ ¹æ®å®é™…çº¦æŸé€‰æ‹©RLç­–ç•¥
    """
    if constraints['safety_critical']:
        # å®‰å…¨è¦æ±‚é«˜ï¼Œä½¿ç”¨çº¯ç¦»çº¿
        return 'offline', "å®‰å…¨ç¬¬ä¸€ï¼Œé¿å…æœªçŸ¥é£é™©"
    
    elif constraints['compute_budget'] < 100:  # GPUå°æ—¶
        # è®¡ç®—é¢„ç®—æœ‰é™ï¼Œä½¿ç”¨ç¦»çº¿
        return 'offline', "è®¡ç®—èµ„æºå—é™"
    
    elif constraints['data_quality'] < 0.7:
        # æ•°æ®è´¨é‡å·®ï¼Œéœ€è¦åœ¨çº¿æ¢ç´¢
        return 'online', "æ•°æ®è´¨é‡ä¸è¶³ï¼Œéœ€è¦ä¸»åŠ¨æ”¹è¿›"
    
    elif constraints['deployment_type'] == 'production':
        # ç”Ÿäº§ç¯å¢ƒï¼Œä½¿ç”¨åŠåœ¨çº¿
        return 'semi_online', "å¹³è¡¡å®‰å…¨æ€§å’Œæ€§èƒ½"
    
    else:
        # ç ”ç©¶ç¯å¢ƒï¼Œä½¿ç”¨åœ¨çº¿
        return 'online', "è¿½æ±‚æœ€ä½³æ€§èƒ½"
```

### 6.6.6 åˆ†å¸ƒåç§»çš„ç¼“è§£æŠ€æœ¯

**æŠ€æœ¯1ï¼šä¿å®ˆæ­£åˆ™åŒ–**

```python
def conservative_regularization(policy, ref_policy, responses, alpha=0.1):
    """
    CQL-styleä¿å®ˆæ­£åˆ™åŒ–ï¼Œé˜²æ­¢ç¦»çº¿RLè¿‡åº¦ä¹è§‚
    """
    # è®¡ç®—OODï¼ˆout-of-distributionï¼‰åŠ¨ä½œçš„Qå€¼
    ood_responses = policy.sample(temperature=1.5)  # é«˜æ¸©é‡‡æ ·OOD
    ood_q_values = policy.q_function(ood_responses)
    
    # æƒ©ç½šOODåŠ¨ä½œçš„é«˜Qå€¼
    conservative_loss = alpha * ood_q_values.mean()
    
    return conservative_loss
```

**æŠ€æœ¯2ï¼šåˆ†å¸ƒæ„ŸçŸ¥é‡‡æ ·**

```python
class DistributionAwareSampler:
    def __init__(self, offline_data, model):
        self.offline_data = offline_data
        self.model = model
        
        # é¢„è®¡ç®—æ•°æ®åˆ†å¸ƒç‰¹å¾
        self.data_embeddings = self.compute_embeddings(offline_data)
        self.distribution_stats = self.compute_stats(self.data_embeddings)
    
    def sample_in_distribution(self, n_samples):
        """
        ä¼˜å…ˆé‡‡æ ·åˆ†å¸ƒå†…çš„æ•°æ®
        """
        candidates = []
        
        for _ in range(n_samples * 5):  # è¿‡é‡‡æ ·
            response = self.model.generate()
            embedding = self.get_embedding(response)
            
            # è®¡ç®—ä¸è®­ç»ƒåˆ†å¸ƒçš„è·ç¦»
            distance = self.compute_distance(
                embedding, 
                self.distribution_stats
            )
            
            candidates.append((response, distance))
        
        # é€‰æ‹©æœ€æ¥è¿‘è®­ç»ƒåˆ†å¸ƒçš„æ ·æœ¬
        candidates.sort(key=lambda x: x[1])
        return [c[0] for c in candidates[:n_samples]]
```

## æœ¬ç« å°ç»“

æœ¬ç« æ·±å…¥æ¢è®¨äº†åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰åŠå…¶å˜ä½“åœ¨å¤§è¯­è¨€æ¨¡å‹åè®­ç»ƒä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å­¦ä¹ äº†ï¼š

### æ ¸å¿ƒæ¦‚å¿µå›é¡¾

1. **RLHFçš„ä¸‰å¤§æ”¯æŸ±**ï¼š
   - å¥–åŠ±æ¨¡å‹ï¼šå°†äººç±»åå¥½è½¬åŒ–ä¸ºå¯ä¼˜åŒ–çš„æ ‡é‡ä¿¡å·
   - ç­–ç•¥ä¼˜åŒ–ï¼šé€šè¿‡PPOç­‰ç®—æ³•æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±
   - KLçº¦æŸï¼šé˜²æ­¢æ¨¡å‹åç¦»åˆå§‹åˆ†å¸ƒè¿‡è¿œ

2. **å…³é”®ç®—æ³•å¯¹æ¯”**ï¼š
   - **PPO**ï¼šç¨³å®šä½†è®¡ç®—å¯†é›†ï¼Œéœ€è¦æ˜¾å¼å¥–åŠ±æ¨¡å‹
   - **DPO**ï¼šç›´æ¥ä¼˜åŒ–ï¼Œè®­ç»ƒé«˜æ•ˆä½†å¯èƒ½è¿‡æ‹Ÿåˆ
   - **IPO**ï¼šæ›´é²æ£’ä½†æ”¶æ•›è¾ƒæ…¢
   - **Constitutional AI**ï¼šè‡ªæˆ‘æ”¹è¿›ï¼Œå‡å°‘äººå·¥æ ‡æ³¨

3. **å®ç”¨æƒè¡¡**ï¼š
   - åœ¨çº¿RLï¼šæ¢ç´¢èƒ½åŠ›å¼ºä½†æˆæœ¬é«˜
   - ç¦»çº¿RLï¼šå®‰å…¨é«˜æ•ˆä½†å—é™äºæ•°æ®è´¨é‡
   - æ··åˆç­–ç•¥ï¼šå¹³è¡¡æ¢ç´¢ä¸å®‰å…¨æ€§

### å…³é”®å…¬å¼æ±‡æ€»

**Bradley-Terryåå¥½æ¨¡å‹**ï¼š
$$P(A \succ B) = \sigma(r(A) - r(B))$$

**PPOç›®æ ‡å‡½æ•°**ï¼š
$$\mathcal{L}_{PPO} = \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)$$

**DPOæŸå¤±å‡½æ•°**ï¼š
$$\mathcal{L}_{DPO} = -\log\sigma\left(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right)$$

### å®è·µè¦ç‚¹

1. **å¥–åŠ±æ¨¡å‹è®­ç»ƒ**ï¼šä½¿ç”¨é›†æˆå’Œæ ¡å‡†æŠ€æœ¯æé«˜é²æ£’æ€§
2. **PPOè°ƒä¼˜**ï¼šç›‘æ§KLæ•£åº¦å’Œè£å‰ªæ¯”ä¾‹ï¼Œé‡‡ç”¨æ¸è¿›å¼è®­ç»ƒ
3. **DPO/IPOé€‰æ‹©**ï¼šæ ¹æ®æ•°æ®è´¨é‡å’Œæ ‡æ³¨ä¸€è‡´æ€§é€‰æ‹©
4. **åœ¨çº¿/ç¦»çº¿å†³ç­–**ï¼šåŸºäºå®‰å…¨éœ€æ±‚å’Œè®¡ç®—é¢„ç®—æƒè¡¡

## ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜ï¼ˆç†è§£æ¦‚å¿µï¼‰

**ç»ƒä¹ 6.1ï¼šå¥–åŠ±æ¨¡å‹è®¾è®¡**
è®¾è®¡ä¸€ä¸ªå¥–åŠ±æ¨¡å‹æ¶æ„ï¼Œç”¨äºè¯„ä¼°ä»£ç ç”Ÿæˆä»»åŠ¡çš„è´¨é‡ã€‚è€ƒè™‘å¦‚ä½•å¤„ç†è¯­æ³•æ­£ç¡®æ€§ã€åŠŸèƒ½å®Œæ•´æ€§å’Œä»£ç é£æ ¼ç­‰å¤šä¸ªç»´åº¦ã€‚

<details>
<summary>æç¤ºï¼ˆHintï¼‰</summary>
è€ƒè™‘å¤šå¤´æ¶æ„ï¼Œæ¯ä¸ªå¤´è´Ÿè´£ä¸€ä¸ªè´¨é‡ç»´åº¦ï¼Œæœ€ç»ˆåŠ æƒç»„åˆã€‚
</details>

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

å¥–åŠ±æ¨¡å‹åº”åŒ…å«ï¼š
1. è¯­æ³•æ£€æŸ¥å¤´ï¼šä½¿ç”¨ASTè§£æéªŒè¯è¯­æ³•æ­£ç¡®æ€§ï¼ˆäºŒå€¼è¾“å‡ºï¼‰
2. åŠŸèƒ½è¯„ä¼°å¤´ï¼šé€šè¿‡æµ‹è¯•ç”¨ä¾‹æ‰§è¡Œè¯„ä¼°åŠŸèƒ½å®Œæ•´æ€§ï¼ˆ0-1è¿ç»­å€¼ï¼‰
3. é£æ ¼è¯„åˆ†å¤´ï¼šåŸºäºä»£ç è§„èŒƒæ£€æŸ¥å·¥å…·çš„è¾“å‡ºï¼ˆ0-1è¿ç»­å€¼ï¼‰
4. æ•ˆç‡è¯„ä¼°å¤´ï¼šåˆ†ææ—¶é—´/ç©ºé—´å¤æ‚åº¦ï¼ˆå¯é€‰ï¼‰

æœ€ç»ˆå¥–åŠ± = w1Ã—è¯­æ³• + w2Ã—åŠŸèƒ½ + w3Ã—é£æ ¼ + w4Ã—æ•ˆç‡

å…¶ä¸­æƒé‡å¯ä»¥æ ¹æ®ä»»åŠ¡éœ€æ±‚è°ƒæ•´ï¼Œå¦‚ç”Ÿäº§ä»£ç æ›´é‡è§†åŠŸèƒ½å’Œè¯­æ³•ï¼Œæ•™å­¦ä»£ç æ›´é‡è§†é£æ ¼ã€‚

</details>

**ç»ƒä¹ 6.2ï¼šKLæ•£åº¦è®¡ç®—**
ç»™å®šå‚è€ƒåˆ†å¸ƒ $p_{ref} = [0.1, 0.2, 0.3, 0.4]$ å’Œå½“å‰åˆ†å¸ƒ $p_{\theta} = [0.15, 0.25, 0.35, 0.25]$ï¼Œè®¡ç®—KLæ•£åº¦ $D_{KL}(p_{\theta} || p_{ref})$ã€‚

<details>
<summary>æç¤ºï¼ˆHintï¼‰</summary>
ä½¿ç”¨å…¬å¼ï¼š$D_{KL}(p||q) = \sum_i p_i \log(p_i/q_i)$
</details>

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

$D_{KL}(p_{\theta} || p_{ref}) = \sum_i p_{\theta,i} \log(p_{\theta,i}/p_{ref,i})$

= 0.15Ã—log(0.15/0.1) + 0.25Ã—log(0.25/0.2) + 0.35Ã—log(0.35/0.3) + 0.25Ã—log(0.25/0.4)
= 0.15Ã—0.405 + 0.25Ã—0.223 + 0.35Ã—0.155 + 0.25Ã—(-0.470)
= 0.061 + 0.056 + 0.054 - 0.118
= 0.053

KLæ•£åº¦çº¦ä¸º0.053ï¼Œè¡¨ç¤ºä¸¤ä¸ªåˆ†å¸ƒç›¸å¯¹æ¥è¿‘ã€‚

</details>

**ç»ƒä¹ 6.3ï¼šDPO vs PPOåœºæ™¯é€‰æ‹©**
åˆ—ä¸¾ä¸‰ä¸ªé€‚åˆä½¿ç”¨DPOè€ŒéPPOçš„å…·ä½“åœºæ™¯ï¼Œå¹¶è¯´æ˜åŸå› ã€‚

<details>
<summary>æç¤ºï¼ˆHintï¼‰</summary>
è€ƒè™‘è®¡ç®—èµ„æºã€æ•°æ®å¯ç”¨æ€§å’Œè®­ç»ƒç¨³å®šæ€§ç­‰å› ç´ ã€‚
</details>

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

1. **å­¦æœ¯ç ”ç©¶ç¯å¢ƒ**ï¼šè®¡ç®—èµ„æºæœ‰é™ï¼ŒDPOä¸éœ€è¦è®­ç»ƒç‹¬ç«‹çš„å¥–åŠ±æ¨¡å‹ï¼ŒèŠ‚çœGPUå†…å­˜å’Œè®­ç»ƒæ—¶é—´ã€‚

2. **é«˜è´¨é‡åå¥½æ•°æ®å……è¶³**ï¼šå·²æœ‰å¤§é‡äººå·¥æ ‡æ³¨çš„åå¥½å¯¹ï¼Œä¸”æ ‡æ³¨ä¸€è‡´æ€§é«˜ï¼ˆ>85%ï¼‰ï¼ŒDPOå¯ä»¥ç›´æ¥åˆ©ç”¨è¿™äº›æ•°æ®ã€‚

3. **å¿«é€ŸåŸå‹éªŒè¯**ï¼šéœ€è¦å¿«é€ŸéªŒè¯å¯¹é½æ–¹æ³•çš„æ•ˆæœï¼ŒDPOå®ç°ç®€å•ï¼Œæ”¶æ•›å¿«ï¼Œé€‚åˆå¿«é€Ÿè¿­ä»£ã€‚

ä¸é€‚åˆDPOçš„åœºæ™¯ï¼šæ ‡æ³¨å™ªå£°å¤§ã€éœ€è¦åœ¨çº¿æ¢ç´¢ã€å®‰å…¨æ€§è¦æ±‚æé«˜çš„åœºæ™¯ã€‚

</details>

### æŒ‘æˆ˜é¢˜ï¼ˆæ·±å…¥æ€è€ƒï¼‰

**ç»ƒä¹ 6.4ï¼šå¥–åŠ±è¿‡æ‹Ÿåˆæ£€æµ‹**
è®¾è®¡ä¸€ä¸ªæ–¹æ³•æ¥è‡ªåŠ¨æ£€æµ‹RLHFè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±è¿‡æ‹Ÿåˆï¼ˆreward hackingï¼‰ç°è±¡ã€‚

<details>
<summary>æç¤ºï¼ˆHintï¼‰</summary>
è€ƒè™‘ç›‘æ§å¤šä¸ªæŒ‡æ ‡çš„ç›¸å…³æ€§å˜åŒ–ï¼Œå¦‚å¥–åŠ±å€¼ä¸äººç±»è¯„ä¼°çš„ç›¸å…³æ€§ã€‚
</details>

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

å¥–åŠ±è¿‡æ‹Ÿåˆæ£€æµ‹ç³»ç»Ÿï¼š

1. **æŒ‡æ ‡ç›‘æ§**ï¼š
   - å¥–åŠ±å€¼è¶‹åŠ¿ï¼šå¦‚æœå¥–åŠ±æŒç»­ä¸Šå‡ä½†éªŒè¯é›†æ€§èƒ½ä¸‹é™
   - å“åº”é•¿åº¦åˆ†å¸ƒï¼šçªç„¶å˜é•¿å¯èƒ½è¡¨ç¤ºåœ¨åˆ©ç”¨é•¿åº¦åå¥½
   - è¯æ±‡å¤šæ ·æ€§ï¼šä¸‹é™è¡¨ç¤ºæ¨¡å‹åœ¨é‡å¤ç‰¹å®šæ¨¡å¼

2. **å¯¹æŠ—æµ‹è¯•**ï¼š
   - å®šæœŸç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œæ£€æŸ¥å¥–åŠ±æ¨¡å‹æ˜¯å¦ç»™äºˆä¸åˆç†é«˜åˆ†
   - ä½¿ç”¨ç®€å•çš„é‡å¤æ¨¡å¼æµ‹è¯•ï¼Œå¦‚"éå¸¸å¥½éå¸¸å¥½..."

3. **äººç±»è¯„ä¼°å¯¹æ¯”**ï¼š
   - å®šæœŸæŠ½æ ·è¿›è¡Œäººç±»è¯„ä¼°
   - è®¡ç®—å¥–åŠ±å€¼ä¸äººç±»è¯„åˆ†çš„Spearmanç›¸å…³ç³»æ•°
   - ç›¸å…³æ€§ä¸‹é™æ˜¯è¿‡æ‹Ÿåˆçš„å¼ºä¿¡å·

4. **è‡ªåŠ¨åŒ–æ£€æµ‹è§„åˆ™**ï¼š
   ```python
   if (reward_increase > 50% and 
       human_eval_correlation < 0.5 and
       response_length_std > 2 * initial_std):
       trigger_alert("å¯èƒ½çš„å¥–åŠ±è¿‡æ‹Ÿåˆ")
   ```

</details>

**ç»ƒä¹ 6.5ï¼šConstitutional AIåŸåˆ™è®¾è®¡**
ä¸ºä¸€ä¸ªåŒ»ç–—å’¨è¯¢AIåŠ©æ‰‹è®¾è®¡ä¸€å¥—å®ªæ³•åŸåˆ™å±‚æ¬¡ç»“æ„ï¼Œç¡®ä¿å®‰å…¨æ€§ã€å‡†ç¡®æ€§å’Œæœ‰ç”¨æ€§çš„å¹³è¡¡ã€‚

<details>
<summary>æç¤ºï¼ˆHintï¼‰</summary>
è€ƒè™‘åŒ»ç–—é¢†åŸŸçš„ç‰¹æ®Šæ€§ï¼šé”™è¯¯ä¿¡æ¯çš„ä¸¥é‡åæœã€æ³•å¾‹è´£ä»»ã€æ‚£è€…éšç§ç­‰ã€‚
</details>

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

åŒ»ç–—AIå®ªæ³•åŸåˆ™å±‚æ¬¡ï¼š

**ç¬¬ä¸€å±‚ï¼šå®‰å…¨æ€§ï¼ˆä¸å¯è¿åï¼‰**
1. ç»ä¸æ›¿ä»£ä¸“ä¸šåŒ»ç–—è¯Šæ–­
2. å±æ€¥æƒ…å†µå¿…é¡»å»ºè®®ç«‹å³å°±åŒ»
3. ä¸æ¨èæœªç»éªŒè¯çš„æ²»ç–—æ–¹æ³•
4. ä¸¥æ ¼ä¿æŠ¤æ‚£è€…éšç§ä¿¡æ¯

**ç¬¬äºŒå±‚ï¼šå‡†ç¡®æ€§ï¼ˆå¼ºçº¦æŸï¼‰**
1. å¼•ç”¨ä¿¡æ¯å¿…é¡»æ¥è‡ªå¯é åŒ»å­¦æ¥æº
2. æ˜ç¡®åŒºåˆ†å¸¸è§æƒ…å†µå’Œéœ€è¦ä¸“ä¸šè¯„ä¼°çš„æƒ…å†µ
3. æ‰¿è®¤åŒ»å­¦ä¸ç¡®å®šæ€§ï¼Œé¿å…ç»å¯¹åŒ–è¡¨è¿°
4. çº æ­£æ˜æ˜¾çš„åŒ»å­¦è¯¯è§£

**ç¬¬ä¸‰å±‚ï¼šæœ‰ç”¨æ€§ï¼ˆè½¯çº¦æŸï¼‰**
1. æä¾›æ˜“æ‡‚çš„åŒ»å­¦çŸ¥è¯†è§£é‡Š
2. ç»™å‡ºåˆç†çš„å¥åº·ç”Ÿæ´»å»ºè®®
3. å¸®åŠ©å‡†å¤‡å°±åŒ»é—®é¢˜æ¸…å•
4. æä¾›æƒ…ç»ªæ”¯æŒå’Œå®‰æ…°

**å®æ–½ç­–ç•¥**ï¼š
- ç¬¬ä¸€å±‚è¿å â†’ ç«‹å³æ‹’ç»è¾“å‡º
- ç¬¬äºŒå±‚è¿å â†’ å¼ºåˆ¶ä¿®æ”¹ç›´åˆ°æ»¡è¶³
- ç¬¬ä¸‰å±‚è¿å â†’ å°è¯•æ”¹è¿›ä½†å¯æ¥å—

</details>

**ç»ƒä¹ 6.6ï¼šåœ¨çº¿ç¦»çº¿RLæ··åˆç­–ç•¥**
è®¾è®¡ä¸€ä¸ªè‡ªé€‚åº”çš„åœ¨çº¿/ç¦»çº¿RLæ··åˆè®­ç»ƒç­–ç•¥ï¼Œèƒ½å¤Ÿæ ¹æ®è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¡¨ç°åŠ¨æ€è°ƒæ•´åœ¨çº¿æ•°æ®çš„æ¯”ä¾‹ã€‚

<details>
<summary>æç¤ºï¼ˆHintï¼‰</summary>
è€ƒè™‘ä½¿ç”¨éªŒè¯é›†æ€§èƒ½ã€KLæ•£åº¦ã€è®¡ç®—æˆæœ¬ç­‰ä¿¡å·æ¥è°ƒæ•´æ··åˆæ¯”ä¾‹ã€‚
</details>

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

è‡ªé€‚åº”æ··åˆç­–ç•¥ï¼š

```python
class AdaptiveMixedRL:
    def __init__(self):
        self.online_ratio = 0.1  # åˆå§‹10%åœ¨çº¿
        self.performance_history = []
        
    def adjust_ratio(self, metrics):
        # 1. æ€§èƒ½æ”¹è¿›ç‡
        if len(self.performance_history) > 5:
            recent_improvement = np.mean(np.diff(self.performance_history[-5:]))
            
            if recent_improvement < 0.001:  # æ€§èƒ½åœæ»
                self.online_ratio = min(0.5, self.online_ratio * 1.5)
                
        # 2. KLæ•£åº¦ç›‘æ§
        if metrics['kl_divergence'] > 0.1:  # KLè¿‡å¤§
            self.online_ratio = max(0.05, self.online_ratio * 0.8)
            
        # 3. è®¡ç®—é¢„ç®—çº¦æŸ
        if metrics['compute_usage'] > 0.8:  # æ¥è¿‘é¢„ç®—ä¸Šé™
            self.online_ratio = max(0, self.online_ratio - 0.1)
            
        # 4. æ•°æ®åˆ†å¸ƒåŒ¹é…åº¦
        if metrics['distribution_shift'] > 0.3:  # åˆ†å¸ƒåç§»ä¸¥é‡
            self.online_ratio = min(0.7, self.online_ratio + 0.1)
            
        return self.online_ratio
```

**åŠ¨æ€è°ƒæ•´è§„åˆ™**ï¼š
1. åˆå§‹é˜¶æ®µï¼ˆå‰1000æ­¥ï¼‰ï¼šçº¯ç¦»çº¿ï¼Œå»ºç«‹åŸºçº¿
2. æ¢ç´¢é˜¶æ®µï¼ˆ1000-5000æ­¥ï¼‰ï¼šé€æ­¥å¢åŠ åœ¨çº¿æ¯”ä¾‹
3. ç¨³å®šé˜¶æ®µï¼ˆ5000+æ­¥ï¼‰ï¼šæ ¹æ®æ€§èƒ½è‡ªé€‚åº”è°ƒæ•´
4. å¼‚å¸¸å¤„ç†ï¼šæ£€æµ‹åˆ°è®­ç»ƒä¸ç¨³å®šæ—¶å¿«é€Ÿé™ä½åœ¨çº¿æ¯”ä¾‹

</details>

**ç»ƒä¹ 6.7ï¼šå¤šç›®æ ‡RLHFä¼˜åŒ–**
è®¾è®¡ä¸€ä¸ªæ–¹æ³•æ¥åŒæ—¶ä¼˜åŒ–å¤šä¸ªå¯èƒ½å†²çªçš„ç›®æ ‡ï¼ˆå¦‚æœ‰ç”¨æ€§ã€å®‰å…¨æ€§ã€åˆ›é€ æ€§ï¼‰ï¼Œå¹¶å¤„ç†å®ƒä»¬ä¹‹é—´çš„æƒè¡¡ã€‚

<details>
<summary>æç¤ºï¼ˆHintï¼‰</summary>
è€ƒè™‘Paretoä¼˜åŒ–ã€å¤šå¥–åŠ±æ¨¡å‹ã€æ¡ä»¶è®­ç»ƒç­‰æ–¹æ³•ã€‚
</details>

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

å¤šç›®æ ‡RLHFæ¡†æ¶ï¼š

1. **å¤šå¥–åŠ±æ¨¡å‹æ¶æ„**ï¼š
```python
class MultiObjectiveRLHF:
    def __init__(self, objectives):
        self.reward_models = {
            'helpfulness': RewardModel(),
            'safety': RewardModel(),
            'creativity': RewardModel()
        }
        self.weights = {'helpfulness': 0.4, 'safety': 0.4, 'creativity': 0.2}
        
    def compute_pareto_rewards(self, responses):
        rewards = {}
        for obj, model in self.reward_models.items():
            rewards[obj] = model(responses)
        
        # è®¡ç®—Paretoå‰æ²¿
        pareto_mask = self.get_pareto_optimal(rewards)
        
        return rewards, pareto_mask
```

2. **åŠ¨æ€æƒé‡è°ƒæ•´**ï¼š
```python
def adjust_weights_by_context(prompt_type):
    if prompt_type == 'medical':
        return {'safety': 0.7, 'helpfulness': 0.3, 'creativity': 0.0}
    elif prompt_type == 'creative_writing':
        return {'creativity': 0.6, 'helpfulness': 0.3, 'safety': 0.1}
    else:
        return {'helpfulness': 0.4, 'safety': 0.4, 'creativity': 0.2}
```

3. **æ¡ä»¶å¥–åŠ±å‡½æ•°**ï¼š
```python
def conditional_reward(response, objectives, context):
    base_rewards = compute_base_rewards(response, objectives)
    
    # ç¡¬çº¦æŸï¼šå®‰å…¨æ€§ä½äºé˜ˆå€¼æ—¶ä¸¥é‡æƒ©ç½š
    if base_rewards['safety'] < 0.3:
        return -10.0
    
    # è½¯çº¦æŸï¼šæ ¹æ®ä¸Šä¸‹æ–‡åŠ æƒ
    weighted_reward = sum(
        base_rewards[obj] * weight 
        for obj, weight in context_weights.items()
    )
    
    return weighted_reward
```

4. **å¤šç­–ç•¥é›†æˆ**ï¼š
è®­ç»ƒå¤šä¸ªä¸“é—¨åŒ–çš„ç­–ç•¥ï¼Œæ¨ç†æ—¶æ ¹æ®éœ€æ±‚é€‰æ‹©æˆ–æ’å€¼ã€‚

</details>

## å¸¸è§é™·é˜±ä¸é”™è¯¯

### 1. å¥–åŠ±æ¨¡å‹è¿‡æ‹Ÿåˆ

**é”™è¯¯è¡¨ç°**ï¼š
- å¥–åŠ±å€¼æŒç»­ä¸Šå‡ä½†å®é™…è´¨é‡ä¸‹é™
- æ¨¡å‹è¾“å‡ºå˜å¾—å•ä¸€åŒ–ï¼ˆå¦‚æ€»æ˜¯ç”Ÿæˆæé•¿å›å¤ï¼‰

**è°ƒè¯•æ–¹æ³•**ï¼š
```python
# æ£€æµ‹å¥–åŠ±è¿‡æ‹Ÿåˆ
def detect_reward_overfitting(model, reward_model, test_prompts):
    responses = model.generate(test_prompts)
    rewards = reward_model(responses)
    
    # æ£€æŸ¥1ï¼šå¥–åŠ±åˆ†å¸ƒæ˜¯å¦å¼‚å¸¸é›†ä¸­
    if rewards.std() < 0.1:
        print("è­¦å‘Šï¼šå¥–åŠ±åˆ†å¸ƒè¿‡äºé›†ä¸­")
    
    # æ£€æŸ¥2ï¼šå“åº”é•¿åº¦æ˜¯å¦å¼‚å¸¸
    lengths = [len(r) for r in responses]
    if np.mean(lengths) > 2 * expected_length:
        print("è­¦å‘Šï¼šå“åº”é•¿åº¦å¼‚å¸¸")
    
    # æ£€æŸ¥3ï¼šè¯æ±‡å¤šæ ·æ€§
    vocab_diversity = compute_vocab_diversity(responses)
    if vocab_diversity < 0.3:
        print("è­¦å‘Šï¼šè¯æ±‡å¤šæ ·æ€§è¿‡ä½")
```

### 2. KLæ•£åº¦çˆ†ç‚¸

**é”™è¯¯è¡¨ç°**ï¼š
- è®­ç»ƒåæ¨¡å‹è¡Œä¸ºå®Œå…¨æ”¹å˜
- å¤±å»åŸºç¡€èƒ½åŠ›ï¼ˆå¦‚è¯­æ³•æ­£ç¡®æ€§ï¼‰

**é¢„é˜²æªæ–½**ï¼š
- ä½¿ç”¨è‡ªé€‚åº”KLç³»æ•°
- è®¾ç½®KLæ•£åº¦ç¡¬ä¸Šé™
- å®šæœŸé‡ç½®åˆ°checkpoint

### 3. DPOè®­ç»ƒä¸ç¨³å®š

**é”™è¯¯è¡¨ç°**ï¼š
- æŸå¤±éœ‡è¡ä¸æ”¶æ•›
- chosenå’Œrejectedçš„æ¦‚ç‡éƒ½è¶‹å‘äº0

**è§£å†³æ–¹æ¡ˆ**ï¼š
- é™ä½å­¦ä¹ ç‡ï¼ˆé€šå¸¸5e-7ä»¥ä¸‹ï¼‰
- å¢åŠ Î²å‚æ•°ï¼ˆæé«˜æ­£åˆ™åŒ–ï¼‰
- è¿‡æ»¤ä½è´¨é‡åå¥½å¯¹

### 4. Constitutional AIçš„åŸåˆ™å†²çª

**é”™è¯¯è¡¨ç°**ï¼š
- æ¨¡å‹åœ¨æ»¡è¶³ä¸€ä¸ªåŸåˆ™æ—¶è¿åå¦ä¸€ä¸ª
- è¾“å‡ºå˜å¾—è¿‡äºä¿å®ˆæˆ–æ¨¡ç³Š

**å¤„ç†æ–¹æ³•**ï¼š
- å»ºç«‹æ¸…æ™°çš„åŸåˆ™ä¼˜å…ˆçº§
- ä½¿ç”¨åˆ†å±‚åŸåˆ™ç»“æ„
- å®šæœŸå®¡æŸ¥å’Œè°ƒæ•´åŸåˆ™

### 5. åœ¨çº¿RLçš„è®¡ç®—çˆ†ç‚¸

**é”™è¯¯è¡¨ç°**ï¼š
- è®­ç»ƒæ—¶é—´æŒ‡æ•°å¢é•¿
- GPUå†…å­˜æº¢å‡º

**ä¼˜åŒ–æŠ€å·§**ï¼š
- ä½¿ç”¨ç»éªŒå›æ”¾ç¼“å†²åŒº
- æ‰¹é‡ç”Ÿæˆå’Œè¯„ä¼°
- å®æ–½æ—©åœæœºåˆ¶

ğŸ’¡ **æœ€ä½³å®è·µå»ºè®®**ï¼š
1. å§‹ç»ˆä¿ç•™SFTæ£€æŸ¥ç‚¹ä½œä¸ºå›é€€æ–¹æ¡ˆ
2. ä½¿ç”¨å¤šä¸ªç‹¬ç«‹çš„è¯„ä¼°æŒ‡æ ‡
3. å®šæœŸè¿›è¡Œäººå·¥è¯„ä¼°éªŒè¯
4. è®°å½•è¯¦ç»†çš„å®éªŒæ—¥å¿—ä¾¿äºè°ƒè¯•
5. ä»å°è§„æ¨¡å®éªŒå¼€å§‹é€æ­¥æ‰©å¤§
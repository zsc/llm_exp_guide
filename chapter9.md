# ç¬¬ä¹ç« ï¼šç”Ÿäº§éƒ¨ç½²ä¸ç›‘æ§

å°†åè®­ç»ƒæ¨¡å‹ä»å®éªŒç¯å¢ƒéƒ¨ç½²åˆ°ç”Ÿäº§ç³»ç»Ÿæ˜¯ä¸€ä¸ªå¤æ‚çš„å·¥ç¨‹æŒ‘æˆ˜ã€‚æœ¬ç« ç³»ç»Ÿä»‹ç»æ¨¡å‹å‹ç¼©ã€æœåŠ¡åŒ–æ¶æ„ã€å®æ—¶ç›‘æ§ã€æ¼‚ç§»æ£€æµ‹ä»¥åŠå‘å¸ƒç­–ç•¥ç­‰å…³é”®æŠ€æœ¯ï¼Œå¸®åŠ©æ‚¨æ„å»ºç¨³å®šã€é«˜æ•ˆã€å¯ç»´æŠ¤çš„ LLM ç”Ÿäº§ç³»ç»Ÿã€‚

## 9.1 æ¨¡å‹å‹ç¼©ä¸åŠ é€ŸæŠ€æœ¯

### 9.1.1 é‡åŒ–æŠ€æœ¯

é‡åŒ–æ˜¯å°†æ¨¡å‹æƒé‡å’Œæ¿€æ´»å€¼ä»é«˜ç²¾åº¦ï¼ˆå¦‚ FP32ï¼‰è½¬æ¢ä¸ºä½ç²¾åº¦ï¼ˆå¦‚ INT8ã€INT4ï¼‰çš„è¿‡ç¨‹ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘æ¨¡å‹å¤§å°å’Œæ¨ç†å»¶è¿Ÿã€‚

#### è®­ç»ƒåé‡åŒ–ï¼ˆPost-Training Quantization, PTQï¼‰

PTQ åœ¨è®­ç»ƒå®Œæˆåå¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œæ— éœ€é‡æ–°è®­ç»ƒï¼š

```
åŸå§‹æƒé‡ W âˆˆ [-Î±, Î±]
é‡åŒ–è¿‡ç¨‹ï¼šW_q = round(W Ã— s / Î±) 
åé‡åŒ–ï¼šW' = W_q Ã— Î± / s

å…¶ä¸­ s = 2^(b-1) - 1ï¼Œb ä¸ºé‡åŒ–ä½æ•°
```

**å…³é”®æŠ€æœ¯è¦ç‚¹**ï¼š

1. **å¯¹ç§° vs éå¯¹ç§°é‡åŒ–**
   - å¯¹ç§°ï¼šé›¶ç‚¹å›ºå®šåœ¨ 0ï¼Œå®ç°ç®€å•ä½†ç²¾åº¦ç•¥ä½
   - éå¯¹ç§°ï¼šå¼•å…¥é›¶ç‚¹åç§»ï¼Œç²¾åº¦æ›´é«˜ä½†è®¡ç®—å¤æ‚

2. **é€å±‚ vs é€é€šé“é‡åŒ–**
   - é€å±‚ï¼šæ•´å±‚å…±äº«é‡åŒ–å‚æ•°ï¼Œå‹ç¼©ç‡é«˜
   - é€é€šé“ï¼šæ¯ä¸ªé€šé“ç‹¬ç«‹é‡åŒ–ï¼Œç²¾åº¦ä¿æŒæ›´å¥½

3. **åŠ¨æ€ vs é™æ€é‡åŒ–**
   - é™æ€ï¼šé‡åŒ–å‚æ•°é¢„å…ˆç¡®å®šï¼Œæ¨ç†é€Ÿåº¦å¿«
   - åŠ¨æ€ï¼šè¿è¡Œæ—¶è®¡ç®—é‡åŒ–å‚æ•°ï¼Œç²¾åº¦æ›´é«˜

#### é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQuantization-Aware Training, QATï¼‰

QAT åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿé‡åŒ–æ•ˆæœï¼Œä½¿æ¨¡å‹é€‚åº”ä½ç²¾åº¦è¡¨ç¤ºï¼š

```
å‰å‘ä¼ æ’­ï¼šä½¿ç”¨é‡åŒ–æƒé‡
åå‘ä¼ æ’­ï¼šä½¿ç”¨å…¨ç²¾åº¦æ¢¯åº¦æ›´æ–°

ä¼ªé‡åŒ–æ“ä½œï¼š
W_fake_quant = dequant(quant(W))
```

**å®è·µæŠ€å·§**ï¼š
- ğŸ’¡ ä» INT8 å¼€å§‹å°è¯•ï¼Œå¤šæ•°ä»»åŠ¡ç²¾åº¦æŸå¤± < 1%
- ğŸ’¡ å…³é”®å±‚ï¼ˆå¦‚ç¬¬ä¸€å±‚ã€æœ€åä¸€å±‚ï¼‰ä¿æŒé«˜ç²¾åº¦
- ğŸ’¡ ä½¿ç”¨æ ¡å‡†æ•°æ®é›†ä¼˜åŒ–é‡åŒ–å‚æ•°

### 9.1.2 çŸ¥è¯†è’¸é¦

é€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶å°†å¤§æ¨¡å‹çŸ¥è¯†è¿ç§»åˆ°å°æ¨¡å‹ï¼š

```
æŸå¤±å‡½æ•°ï¼š
L = Î± Ã— L_CE(y_student, y_true) + 
    Î² Ã— L_KL(Ïƒ(z_student/T), Ïƒ(z_teacher/T))

å…¶ä¸­ï¼š
- L_CEï¼šäº¤å‰ç†µæŸå¤±ï¼ˆç¡¬æ ‡ç­¾ï¼‰
- L_KLï¼šKL æ•£åº¦ï¼ˆè½¯æ ‡ç­¾ï¼‰
- Tï¼šæ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åˆ†å¸ƒå¹³æ»‘åº¦
- Î±, Î²ï¼šæŸå¤±æƒé‡
```

**è’¸é¦ç­–ç•¥ä¼˜åŒ–**ï¼š

1. **é€å±‚è’¸é¦**ï¼šä¸ä»…è’¸é¦æœ€ç»ˆè¾“å‡ºï¼Œè¿˜å¯¹é½ä¸­é—´å±‚è¡¨ç¤º
2. **æ³¨æ„åŠ›è’¸é¦**ï¼šä¼ é€’æ³¨æ„åŠ›æ¨¡å¼
3. **ç‰¹å¾è’¸é¦**ï¼šå¯¹é½éšå±‚ç‰¹å¾åˆ†å¸ƒ

### 9.1.3 æ¨¡å‹å‰ªæ

ç§»é™¤å†—ä½™å‚æ•°ä»¥å‡å°æ¨¡å‹å¤§å°ï¼š

#### ç»“æ„åŒ–å‰ªæ
```
é‡è¦æ€§è¯„åˆ†ï¼š
- æƒé‡å¹…åº¦ï¼š|W|
- æ¢¯åº¦å¹…åº¦ï¼š|âˆ‚L/âˆ‚W|
- Taylor å±•å¼€ï¼šÎ”L â‰ˆ |W Ã— âˆ‚L/âˆ‚W|

å‰ªææµç¨‹ï¼š
1. è®­ç»ƒåŸºçº¿æ¨¡å‹
2. è®¡ç®—é‡è¦æ€§åˆ†æ•°
3. ç§»é™¤ä½åˆ†ç¥ç»å…ƒ/é€šé“/å±‚
4. å¾®è°ƒæ¢å¤æ€§èƒ½
```

#### éç»“æ„åŒ–å‰ªæ
```
ç¨€ç–æ©ç ï¼šM âˆˆ {0,1}^d
ç¨€ç–æƒé‡ï¼šW_sparse = W âŠ™ M

åŠ¨æ€ç¨€ç–è®­ç»ƒï¼š
- å‘¨æœŸæ€§æ›´æ–°æ©ç 
- ä¿æŒå›ºå®šç¨€ç–åº¦
```

âš ï¸ **å¸¸è§é™·é˜±**ï¼š
- éç»“æ„åŒ–å‰ªæè™½ç„¶å‹ç¼©ç‡é«˜ï¼Œä½†ç¡¬ä»¶åŠ é€Ÿå›°éš¾
- è¿‡åº¦å‰ªæå¯¼è‡´ä¸å¯æ¢å¤çš„æ€§èƒ½æŸå¤±
- ä¸åŒä»»åŠ¡å¯¹å‰ªæçš„æ•æ„Ÿåº¦å·®å¼‚å·¨å¤§

### 9.1.4 æ¨ç†ä¼˜åŒ–æŠ€æœ¯

#### Flash Attention
å‡å°‘æ³¨æ„åŠ›è®¡ç®—çš„å†…å­˜è®¿é—®ï¼š

```
æ ‡å‡†æ³¨æ„åŠ›ï¼šO(NÂ²) å†…å­˜
Flash Attentionï¼šO(N) å†…å­˜

é€šè¿‡åˆ†å—è®¡ç®—å’Œèåˆ kernel å®ç°ï¼š
- å‡å°‘ HBM è®¿é—®
- æé«˜ SRAM åˆ©ç”¨ç‡
```

#### KV Cache ä¼˜åŒ–
```
ç­–ç•¥ï¼š
1. å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆMQAï¼‰ï¼šå…±äº« Kã€V æŠ•å½±
2. åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ï¼šKã€V å¤´æ•° < Q å¤´æ•°
3. æ»‘åŠ¨çª—å£ï¼šé™åˆ¶æ³¨æ„åŠ›èŒƒå›´

å†…å­˜è®¡ç®—ï¼š
Cache_size = batch Ã— seq_len Ã— n_layers Ã— 
             (n_kv_heads Ã— d_head) Ã— 2 Ã— dtype_size
```

#### æ‰¹å¤„ç†ä¼˜åŒ–
```
åŠ¨æ€æ‰¹å¤„ç†ï¼š
- è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰
- å¡«å……ä¼˜åŒ–ï¼ˆPadding Optimizationï¼‰
- åºåˆ—å¹¶è¡Œï¼ˆSequence Parallelismï¼‰

ååé‡ä¼˜åŒ–ï¼š
Throughput = batch_size / latency
æ‰¾åˆ°æœ€ä¼˜ batch_size å¹³è¡¡å»¶è¿Ÿå’Œåå
```

## 9.2 æœåŠ¡åŒ–æ¶æ„è®¾è®¡

### 9.2.1 å¾®æœåŠ¡æ¶æ„

```
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   Gateway   â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       â”‚       â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”
â”‚Router â”‚ â”‚Auth â”‚ â”‚Rate  â”‚
â”‚       â”‚ â”‚     â”‚ â”‚Limit â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚         â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚Model  â”‚ â”‚Model â”‚ â”‚Cache  â”‚ â”‚Monitor  â”‚
â”‚Server â”‚ â”‚Pool  â”‚ â”‚Serviceâ”‚ â”‚Service  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®ç»„ä»¶è®¾è®¡**ï¼š

1. **API Gateway**
   - è¯·æ±‚è·¯ç”±ä¸è´Ÿè½½å‡è¡¡
   - åè®®è½¬æ¢ï¼ˆHTTP/gRPC/WebSocketï¼‰
   - è®¤è¯æˆæƒ

2. **Model Server**
   - æ¨¡å‹åŠ è½½ä¸ç‰ˆæœ¬ç®¡ç†
   - æ‰¹å¤„ç†é˜Ÿåˆ—
   - èµ„æºéš”ç¦»

3. **Cache Layer**
   - ç»“æœç¼“å­˜ï¼ˆRedis/Memcachedï¼‰
   - åµŒå…¥å‘é‡ç¼“å­˜
   - KV Cache å…±äº«

### 9.2.2 æ¨¡å‹æœåŠ¡æ¡†æ¶

#### TorchServe æ¶æ„
```python
# æ¨¡å‹å¤„ç†å™¨ç¤ºä¾‹
class LLMHandler(BaseHandler):
    def initialize(self, context):
        self.model = load_model(context.model_dir)
        self.tokenizer = load_tokenizer()
        
    def preprocess(self, requests):
        # æ‰¹å¤„ç†é¢„å¤„ç†
        texts = [req.get("text") for req in requests]
        return self.tokenizer(texts, return_tensors="pt")
    
    def inference(self, inputs):
        with torch.no_grad():
            outputs = self.model.generate(**inputs)
        return outputs
    
    def postprocess(self, outputs):
        # è§£ç å’Œæ ¼å¼åŒ–
        return self.tokenizer.batch_decode(outputs)
```

#### Triton Inference Server
```
æ¨¡å‹é…ç½®ï¼š
name: "llm_model"
platform: "pytorch_libtorch"
max_batch_size: 32
input [
  {
    name: "input_ids"
    data_type: TYPE_INT64
    dims: [-1]
  }
]
output [
  {
    name: "logits"
    data_type: TYPE_FP32
    dims: [-1, -1]
  }
]
instance_group [
  {
    count: 2
    kind: KIND_GPU
  }
]
dynamic_batching {
  max_queue_delay_microseconds: 100
}
```

### 9.2.3 æµå¼ç”Ÿæˆæ¶æ„

å¤„ç† LLM é€ token ç”Ÿæˆçš„ç‰¹æ®Šéœ€æ±‚ï¼š

```python
# SSE (Server-Sent Events) å®ç°
async def stream_generate(request):
    async def generate():
        for token in model.generate_stream(request.text):
            yield f"data: {json.dumps({'token': token})}\n\n"
            
    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )

# WebSocket å®ç°
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    try:
        while True:
            data = await websocket.receive_text()
            async for token in model.generate_stream(data):
                await websocket.send_json({"token": token})
    except WebSocketDisconnect:
        pass
```

**æµå¼å¤„ç†ä¼˜åŒ–**ï¼š
- ä½¿ç”¨ç¯å½¢ç¼“å†²åŒºç®¡ç† token æµ
- å®ç°èƒŒå‹ï¼ˆBackpressureï¼‰æœºåˆ¶
- æ”¯æŒä¸­æ–­å’Œæ¢å¤

### 9.2.4 é«˜å¯ç”¨è®¾è®¡

#### å¤šå‰¯æœ¬éƒ¨ç½²
```yaml
# Kubernetes éƒ¨ç½²é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-service
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    spec:
      containers:
      - name: model-server
        resources:
          requests:
            nvidia.com/gpu: 1
          limits:
            nvidia.com/gpu: 1
        livenessProbe:
          httpGet:
            path: /health
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
          periodSeconds: 10
```

#### æ•…éšœè½¬ç§»ç­–ç•¥
```
ä¸»å¤‡æ¨¡å¼ï¼š
Primary â”€â”€â”
          â”œâ”€â”€> Load Balancer â”€â”€> Client
Secondary â”˜    (Health Check)

è´Ÿè½½å‡è¡¡ç®—æ³•ï¼š
- è½®è¯¢ï¼ˆRound Robinï¼‰
- æœ€å°‘è¿æ¥ï¼ˆLeast Connectionsï¼‰
- ä¸€è‡´æ€§å“ˆå¸Œï¼ˆConsistent Hashingï¼‰
- åŸºäºå»¶è¿Ÿçš„è·¯ç”±
```

## 9.3 å®æ—¶ç›‘æ§ä¸å‘Šè­¦ç³»ç»Ÿ

### 9.3.1 æŒ‡æ ‡ä½“ç³»è®¾è®¡

#### ç³»ç»ŸæŒ‡æ ‡
```python
# Prometheus æŒ‡æ ‡å®šä¹‰
from prometheus_client import Counter, Histogram, Gauge

# è¯·æ±‚æŒ‡æ ‡
request_count = Counter(
    'llm_requests_total',
    'Total requests',
    ['model', 'status']
)

request_latency = Histogram(
    'llm_request_duration_seconds',
    'Request latency',
    ['model', 'operation']
)

# èµ„æºæŒ‡æ ‡
gpu_utilization = Gauge(
    'llm_gpu_utilization_percent',
    'GPU utilization',
    ['device_id']
)

memory_usage = Gauge(
    'llm_memory_usage_bytes',
    'Memory usage',
    ['type']  # gpu_memory, cpu_memory
)
```

#### ä¸šåŠ¡æŒ‡æ ‡
```python
# è´¨é‡æŒ‡æ ‡
output_quality_score = Histogram(
    'llm_output_quality_score',
    'Output quality score from feedback',
    ['task_type']
)

# Token ç»Ÿè®¡
token_usage = Counter(
    'llm_tokens_total',
    'Total tokens processed',
    ['type']  # input, output
)

# æˆæœ¬æŒ‡æ ‡
api_cost = Counter(
    'llm_api_cost_dollars',
    'API cost in dollars',
    ['model', 'customer']
)
```

### 9.3.2 æ—¥å¿—èšåˆä¸åˆ†æ

#### ç»“æ„åŒ–æ—¥å¿—è®¾è®¡
```python
import structlog

logger = structlog.get_logger()

# è¯·æ±‚æ—¥å¿—
logger.info(
    "request_received",
    request_id=request_id,
    user_id=user_id,
    model_version=model_version,
    input_tokens=len(tokens),
    timestamp=time.time()
)

# æ¨ç†æ—¥å¿—
logger.info(
    "inference_completed",
    request_id=request_id,
    latency_ms=latency * 1000,
    output_tokens=len(output_tokens),
    gpu_memory_mb=gpu_memory,
    cache_hit=cache_hit
)

# é”™è¯¯æ—¥å¿—
logger.error(
    "inference_failed",
    request_id=request_id,
    error_type=type(e).__name__,
    error_message=str(e),
    traceback=traceback.format_exc()
)
```

#### ELK Stack é›†æˆ
```
æ—¥å¿—æµæ°´çº¿ï¼š
Application â”€â”€> Filebeat â”€â”€> Logstash â”€â”€> Elasticsearch â”€â”€> Kibana
                              â”‚
                              â”œâ”€> è§£æå’Œå¢å¼º
                              â”œâ”€> è¿‡æ»¤å’Œè·¯ç”±
                              â””â”€> å‘Šè­¦è§¦å‘
```

### 9.3.3 åˆ†å¸ƒå¼è¿½è¸ª

ä½¿ç”¨ OpenTelemetry å®ç°å…¨é“¾è·¯è¿½è¸ªï¼š

```python
from opentelemetry import trace
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

tracer = trace.get_tracer(__name__)

@app.post("/generate")
async def generate(request: GenerateRequest):
    with tracer.start_as_current_span("generate_request") as span:
        span.set_attribute("user_id", request.user_id)
        span.set_attribute("model", request.model)
        
        with tracer.start_as_current_span("tokenize"):
            tokens = tokenizer.encode(request.text)
            
        with tracer.start_as_current_span("inference"):
            output = model.generate(tokens)
            
        with tracer.start_as_current_span("decode"):
            result = tokenizer.decode(output)
            
        return result
```

### 9.3.4 å‘Šè­¦ç­–ç•¥

#### åˆ†çº§å‘Šè­¦
```yaml
# AlertManager é…ç½®
route:
  group_by: ['alertname', 'cluster']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
  - match:
      severity: critical
    receiver: pagerduty
    continue: true
  - match:
      severity: warning
    receiver: slack
    
# Prometheus å‘Šè­¦è§„åˆ™
groups:
- name: llm_alerts
  rules:
  - alert: HighLatency
    expr: histogram_quantile(0.99, llm_request_duration_seconds) > 5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "P99 å»¶è¿Ÿè¶…è¿‡ 5 ç§’"
      
  - alert: GPUMemoryLeak
    expr: rate(llm_gpu_memory_bytes[5m]) > 100000000
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: "GPU å†…å­˜æŒç»­å¢é•¿"
```

#### æ™ºèƒ½å‘Šè­¦é™å™ª
```python
class AlertThrottler:
    def __init__(self, window_seconds=300, max_alerts=10):
        self.window = window_seconds
        self.max_alerts = max_alerts
        self.alert_times = defaultdict(deque)
    
    def should_alert(self, alert_key):
        now = time.time()
        times = self.alert_times[alert_key]
        
        # æ¸…ç†è¿‡æœŸæ—¶é—´
        while times and times[0] < now - self.window:
            times.popleft()
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
        if len(times) >= self.max_alerts:
            return False
            
        times.append(now)
        return True

## 9.4 æ¨¡å‹æ¼‚ç§»æ£€æµ‹

æ¨¡å‹æ¼‚ç§»æ˜¯æŒ‡æ¨¡å‹åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ€§èƒ½éšæ—¶é—´ä¸‹é™çš„ç°è±¡ï¼Œå¯èƒ½ç”±æ•°æ®åˆ†å¸ƒå˜åŒ–ã€ç”¨æˆ·è¡Œä¸ºæ¼”å˜æˆ–å¤–éƒ¨ç¯å¢ƒæ”¹å˜å¼•èµ·ã€‚

### 9.4.1 æ¼‚ç§»ç±»å‹ä¸æ£€æµ‹æ–¹æ³•

#### æ•°æ®æ¼‚ç§»ï¼ˆData Driftï¼‰
è¾“å…¥æ•°æ®åˆ†å¸ƒçš„å˜åŒ–ï¼š

```python
# KL æ•£åº¦æ£€æµ‹
def kl_divergence(p, q, epsilon=1e-10):
    """è®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒçš„ KL æ•£åº¦"""
    p = p + epsilon
    q = q + epsilon
    return np.sum(p * np.log(p / q))

# Kolmogorov-Smirnov æ£€éªŒ
from scipy.stats import ks_2samp

def ks_test_drift(reference_data, current_data, threshold=0.05):
    """ä½¿ç”¨ KS æ£€éªŒæ£€æµ‹åˆ†å¸ƒå˜åŒ–"""
    statistic, p_value = ks_2samp(reference_data, current_data)
    return p_value < threshold, statistic

# ç‰¹å¾åˆ†å¸ƒç›‘æ§
class FeatureDriftMonitor:
    def __init__(self, reference_window=1000):
        self.reference_window = reference_window
        self.feature_buffers = defaultdict(deque)
        self.reference_stats = {}
    
    def update(self, features):
        for name, value in features.items():
            buffer = self.feature_buffers[name]
            buffer.append(value)
            if len(buffer) > self.reference_window:
                buffer.popleft()
    
    def detect_drift(self, sensitivity=2.0):
        drifts = {}
        for name, buffer in self.feature_buffers.items():
            if len(buffer) < 100:
                continue
                
            current_mean = np.mean(buffer)
            current_std = np.std(buffer)
            
            if name in self.reference_stats:
                ref_mean, ref_std = self.reference_stats[name]
                z_score = abs(current_mean - ref_mean) / (ref_std + 1e-10)
                drifts[name] = z_score > sensitivity
            else:
                self.reference_stats[name] = (current_mean, current_std)
                drifts[name] = False
                
        return drifts
```

#### æ¦‚å¿µæ¼‚ç§»ï¼ˆConcept Driftï¼‰
è¾“å…¥-è¾“å‡ºå…³ç³»çš„å˜åŒ–ï¼š

```python
# æ»‘åŠ¨çª—å£æ€§èƒ½ç›‘æ§
class ConceptDriftDetector:
    def __init__(self, window_size=100, threshold=0.1):
        self.window_size = window_size
        self.threshold = threshold
        self.performance_window = deque(maxlen=window_size)
        self.baseline_performance = None
    
    def update(self, prediction, ground_truth):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        correct = (prediction == ground_truth)
        self.performance_window.append(correct)
        
        if len(self.performance_window) == self.window_size:
            current_accuracy = np.mean(self.performance_window)
            
            if self.baseline_performance is None:
                self.baseline_performance = current_accuracy
                return False
            
            drift = abs(current_accuracy - self.baseline_performance)
            return drift > self.threshold
        
        return False

# ADWIN (Adaptive Windowing) ç®—æ³•
class ADWIN:
    def __init__(self, delta=0.002):
        self.delta = delta
        self.window = []
        self.total = 0
        self.variance = 0
        self.width = 0
    
    def update(self, value):
        """æ£€æµ‹æ¦‚å¿µæ¼‚ç§»"""
        self.window.append(value)
        self.total += value
        self.width += 1
        
        if self.width > 1:
            mean = self.total / self.width
            self.variance += (value - mean) ** 2
            
            # æ£€æµ‹ä¸¤ä¸ªå­çª—å£çš„æ˜¾è‘—å·®å¼‚
            if self._detect_change():
                # ä¸¢å¼ƒæ—§æ•°æ®
                self._shrink_window()
                return True
        
        return False
    
    def _detect_change(self):
        """ä½¿ç”¨ Hoeffding ç•Œæ£€æµ‹å˜åŒ–"""
        if self.width < 4:
            return False
            
        for split_point in range(1, self.width):
            n0 = split_point
            n1 = self.width - split_point
            
            sum0 = sum(self.window[:split_point])
            sum1 = sum(self.window[split_point:])
            
            mean0 = sum0 / n0
            mean1 = sum1 / n1
            
            epsilon = np.sqrt(
                (1/(2*n0) + 1/(2*n1)) * 
                np.log(2/self.delta)
            )
            
            if abs(mean0 - mean1) > epsilon:
                return True
                
        return False
```

### 9.4.2 é¢„æµ‹è´¨é‡ç›‘æ§

#### ç½®ä¿¡åº¦åˆ†æ
```python
class ConfidenceMonitor:
    def __init__(self, calibration_bins=10):
        self.calibration_bins = calibration_bins
        self.confidence_scores = []
        self.accuracies = []
    
    def update_batch(self, probabilities, predictions, labels):
        """æ›´æ–°ç½®ä¿¡åº¦ç»Ÿè®¡"""
        max_probs = np.max(probabilities, axis=1)
        correct = (predictions == labels)
        
        self.confidence_scores.extend(max_probs)
        self.accuracies.extend(correct)
    
    def compute_ece(self):
        """è®¡ç®—æœŸæœ›æ ¡å‡†è¯¯å·® (ECE)"""
        bin_boundaries = np.linspace(0, 1, self.calibration_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        ece = 0
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            in_bin = [
                i for i, conf in enumerate(self.confidence_scores)
                if bin_lower <= conf < bin_upper
            ]
            
            if len(in_bin) > 0:
                bin_acc = np.mean([self.accuracies[i] for i in in_bin])
                bin_conf = np.mean([self.confidence_scores[i] for i in in_bin])
                ece += len(in_bin) * abs(bin_acc - bin_conf)
        
        return ece / len(self.confidence_scores)
```

#### è¾“å‡ºåˆ†å¸ƒç›‘æ§
```python
# æ–‡æœ¬ç”Ÿæˆè´¨é‡æŒ‡æ ‡
class TextGenerationMonitor:
    def __init__(self):
        self.metrics_history = defaultdict(list)
    
    def compute_metrics(self, generated_text):
        """è®¡ç®—æ–‡æœ¬è´¨é‡æŒ‡æ ‡"""
        metrics = {}
        
        # å¤šæ ·æ€§æŒ‡æ ‡
        tokens = generated_text.split()
        metrics['unique_tokens'] = len(set(tokens)) / len(tokens)
        
        # é‡å¤åº¦
        bigrams = zip(tokens[:-1], tokens[1:])
        metrics['unique_bigrams'] = len(set(bigrams)) / (len(tokens) - 1)
        
        # é•¿åº¦åˆ†å¸ƒ
        metrics['avg_sentence_length'] = np.mean([
            len(sent.split()) 
            for sent in generated_text.split('.')
        ])
        
        # å›°æƒ‘åº¦ä»£ç†æŒ‡æ ‡
        metrics['entropy'] = self._compute_entropy(tokens)
        
        return metrics
    
    def detect_quality_degradation(self, metrics, window=100):
        """æ£€æµ‹ç”Ÿæˆè´¨é‡ä¸‹é™"""
        alerts = []
        
        for metric_name, value in metrics.items():
            history = self.metrics_history[metric_name]
            history.append(value)
            
            if len(history) > window:
                history.pop(0)
                
                # è®¡ç®—ç§»åŠ¨å¹³å‡å’Œæ ‡å‡†å·®
                mean = np.mean(history[:-10])
                std = np.std(history[:-10])
                recent_mean = np.mean(history[-10:])
                
                # Z-score æ£€éªŒ
                if abs(recent_mean - mean) > 2 * std:
                    alerts.append({
                        'metric': metric_name,
                        'baseline': mean,
                        'current': recent_mean,
                        'severity': 'high' if abs(recent_mean - mean) > 3 * std else 'medium'
                    })
        
        return alerts
```

### 9.4.3 è‡ªåŠ¨åŒ–å“åº”ç­–ç•¥

```python
class DriftResponseOrchestrator:
    def __init__(self):
        self.drift_detectors = {}
        self.response_strategies = {}
        self.alert_manager = AlertManager()
    
    def register_detector(self, name, detector, strategy):
        """æ³¨å†Œæ¼‚ç§»æ£€æµ‹å™¨å’Œå“åº”ç­–ç•¥"""
        self.drift_detectors[name] = detector
        self.response_strategies[name] = strategy
    
    def monitor_and_respond(self, data):
        """ç›‘æ§å¹¶è‡ªåŠ¨å“åº”"""
        for name, detector in self.drift_detectors.items():
            if detector.detect(data):
                self._handle_drift(name, data)
    
    def _handle_drift(self, detector_name, data):
        """å¤„ç†æ£€æµ‹åˆ°çš„æ¼‚ç§»"""
        strategy = self.response_strategies[detector_name]
        
        if strategy == 'alert':
            self.alert_manager.send_alert(
                f"Drift detected by {detector_name}",
                severity='warning'
            )
        
        elif strategy == 'retrain':
            self._trigger_retraining(data)
        
        elif strategy == 'fallback':
            self._switch_to_fallback_model()
        
        elif strategy == 'recalibrate':
            self._recalibrate_model(data)
    
    def _trigger_retraining(self, data):
        """è§¦å‘æ¨¡å‹é‡è®­ç»ƒ"""
        # æ”¶é›†æ–°æ•°æ®
        # å¯åŠ¨è®­ç»ƒä»»åŠ¡
        # éªŒè¯æ–°æ¨¡å‹
        pass
    
    def _recalibrate_model(self, data):
        """é‡æ–°æ ¡å‡†æ¨¡å‹è¾“å‡º"""
        # ä½¿ç”¨æ¸©åº¦ç¼©æ”¾
        # æ›´æ–°åå¤„ç†å‚æ•°
        pass
```

## 9.5 å›æ»šä¸ç°åº¦å‘å¸ƒç­–ç•¥

### 9.5.1 è“ç»¿éƒ¨ç½²

```yaml
# Kubernetes è“ç»¿éƒ¨ç½²é…ç½®
apiVersion: v1
kind: Service
metadata:
  name: llm-service
spec:
  selector:
    app: llm
    version: green  # åˆ‡æ¢åˆ° blue æˆ– green
  ports:
  - port: 80
    targetPort: 8080

---
# Blue éƒ¨ç½²
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm
      version: blue
  template:
    metadata:
      labels:
        app: llm
        version: blue
    spec:
      containers:
      - name: model-server
        image: llm:v1.0

---
# Green éƒ¨ç½²
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm
      version: green
  template:
    metadata:
      labels:
        app: llm
        version: green
    spec:
      containers:
      - name: model-server
        image: llm:v2.0
```

### 9.5.2 é‡‘ä¸é›€å‘å¸ƒ

```python
class CanaryDeployment:
    def __init__(self, initial_traffic_percent=5):
        self.canary_percent = initial_traffic_percent
        self.metrics_collector = MetricsCollector()
        self.decision_threshold = 0.95  # æˆåŠŸç‡é˜ˆå€¼
    
    def route_request(self, request):
        """è·¯ç”±è¯·æ±‚åˆ°é‡‘ä¸é›€æˆ–ç¨³å®šç‰ˆæœ¬"""
        if random.random() < self.canary_percent / 100:
            response = self.canary_model.predict(request)
            self.metrics_collector.record('canary', response)
            return response
        else:
            response = self.stable_model.predict(request)
            self.metrics_collector.record('stable', response)
            return response
    
    def progressive_rollout(self):
        """æ¸è¿›å¼å‘å¸ƒ"""
        stages = [5, 10, 25, 50, 100]  # æµé‡ç™¾åˆ†æ¯”é˜¶æ®µ
        
        for target_percent in stages:
            self.canary_percent = target_percent
            time.sleep(300)  # æ¯é˜¶æ®µè§‚å¯Ÿ 5 åˆ†é’Ÿ
            
            if not self._check_health():
                self._rollback()
                return False
        
        return True
    
    def _check_health(self):
        """æ£€æŸ¥é‡‘ä¸é›€ç‰ˆæœ¬å¥åº·çŠ¶æ€"""
        canary_metrics = self.metrics_collector.get_metrics('canary')
        stable_metrics = self.metrics_collector.get_metrics('stable')
        
        # æ¯”è¾ƒå…³é”®æŒ‡æ ‡
        canary_success_rate = canary_metrics['success_rate']
        stable_success_rate = stable_metrics['success_rate']
        
        if canary_success_rate < self.decision_threshold:
            return False
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        p_value = self._statistical_test(
            canary_metrics['latencies'],
            stable_metrics['latencies']
        )
        
        return p_value > 0.05  # æ— æ˜¾è‘—å·®å¼‚
    
    def _rollback(self):
        """å›æ»šåˆ°ç¨³å®šç‰ˆæœ¬"""
        self.canary_percent = 0
        logger.warning("Canary rollback triggered")
```

### 9.5.3 ç‰¹å¾å¼€å…³ï¼ˆFeature Flagsï¼‰

```python
class FeatureFlags:
    def __init__(self, config_source='redis'):
        self.flags = {}
        self.config_source = config_source
        self._load_flags()
    
    def _load_flags(self):
        """ä»é…ç½®æºåŠ è½½ç‰¹å¾å¼€å…³"""
        if self.config_source == 'redis':
            self.flags = {
                'new_tokenizer': {'enabled': True, 'rollout': 100},
                'attention_optimization': {'enabled': True, 'rollout': 50},
                'experimental_sampler': {'enabled': False, 'rollout': 0}
            }
    
    def is_enabled(self, flag_name, user_id=None):
        """æ£€æŸ¥ç‰¹å¾æ˜¯å¦å¯ç”¨"""
        if flag_name not in self.flags:
            return False
        
        flag = self.flags[flag_name]
        if not flag['enabled']:
            return False
        
        if user_id:
            # åŸºäºç”¨æˆ· ID çš„ä¸€è‡´æ€§å“ˆå¸Œ
            hash_value = hash(f"{flag_name}:{user_id}") % 100
            return hash_value < flag['rollout']
        
        return flag['rollout'] == 100
    
    def with_feature(self, flag_name):
        """è£…é¥°å™¨æ¨¡å¼"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                if self.is_enabled(flag_name):
                    return func(*args, **kwargs)
                else:
                    # è¿”å›é»˜è®¤è¡Œä¸º
                    return self._default_behavior(flag_name)
            return wrapper
        return decorator
```

### 9.5.4 å›æ»šç­–ç•¥å®ç°

```python
class RollbackManager:
    def __init__(self):
        self.deployment_history = []
        self.health_checker = HealthChecker()
        self.max_rollback_window = 3600  # 1 å°æ—¶
    
    def deploy(self, version, config):
        """éƒ¨ç½²æ–°ç‰ˆæœ¬"""
        deployment = {
            'version': version,
            'config': config,
            'timestamp': time.time(),
            'status': 'deploying'
        }
        
        self.deployment_history.append(deployment)
        
        try:
            # æ‰§è¡Œéƒ¨ç½²
            self._execute_deployment(version, config)
            
            # å¥åº·æ£€æŸ¥
            if self._post_deployment_check(version):
                deployment['status'] = 'healthy'
                return True
            else:
                deployment['status'] = 'unhealthy'
                self.automatic_rollback()
                return False
                
        except Exception as e:
            deployment['status'] = 'failed'
            deployment['error'] = str(e)
            self.automatic_rollback()
            raise
    
    def automatic_rollback(self):
        """è‡ªåŠ¨å›æ»šåˆ°ä¸Šä¸€ä¸ªå¥åº·ç‰ˆæœ¬"""
        for deployment in reversed(self.deployment_history[:-1]):
            if deployment['status'] == 'healthy':
                logger.info(f"Rolling back to version {deployment['version']}")
                self._execute_deployment(
                    deployment['version'],
                    deployment['config']
                )
                return True
        
        logger.error("No healthy version found for rollback")
        return False
    
    def _post_deployment_check(self, version):
        """éƒ¨ç½²åå¥åº·æ£€æŸ¥"""
        checks = [
            self.health_checker.check_endpoint_health(),
            self.health_checker.check_model_loading(),
            self.health_checker.check_inference_latency(),
            self.health_checker.check_error_rate()
        ]
        
        return all(checks)
    
    def _execute_deployment(self, version, config):
        """æ‰§è¡Œå®é™…éƒ¨ç½²æ“ä½œ"""
        # æ›´æ–°æ¨¡å‹æ–‡ä»¶
        # é‡å¯æœåŠ¡
        # æ›´æ–°è·¯ç”±é…ç½®
        pass
```

## æœ¬ç« å°ç»“

æœ¬ç« ç³»ç»Ÿä»‹ç»äº† LLM ç”Ÿäº§éƒ¨ç½²ä¸ç›‘æ§çš„æ ¸å¿ƒæŠ€æœ¯ï¼š

### ğŸ“Œ æ ¸å¿ƒæ¦‚å¿µ

1. **æ¨¡å‹ä¼˜åŒ–ä¸‰å¤§æŠ€æœ¯**ï¼š
   - é‡åŒ–ï¼šPTQ vs QATï¼Œæƒè¡¡ç²¾åº¦ä¸æ•ˆç‡
   - è’¸é¦ï¼šçŸ¥è¯†ä¼ é€’ï¼Œå¤§æ¨¡å‹èƒ½åŠ›è¿ç§»åˆ°å°æ¨¡å‹
   - å‰ªæï¼šç»“æ„åŒ– vs éç»“æ„åŒ–ï¼Œç§»é™¤å†—ä½™å‚æ•°

2. **æœåŠ¡æ¶æ„è®¾è®¡**ï¼š
   - å¾®æœåŠ¡æ¶æ„ï¼šæ¨¡å—åŒ–ã€å¯æ‰©å±•ã€é«˜å¯ç”¨
   - æµå¼ç”Ÿæˆï¼šSSE/WebSocket å®ç°é€ token è¾“å‡º
   - æ‰¹å¤„ç†ä¼˜åŒ–ï¼šåŠ¨æ€æ‰¹å¤„ç†æé«˜ååé‡

3. **ç›‘æ§ä½“ç³»**ï¼š
   - ç³»ç»ŸæŒ‡æ ‡ï¼šå»¶è¿Ÿã€ååã€èµ„æºåˆ©ç”¨ç‡
   - ä¸šåŠ¡æŒ‡æ ‡ï¼šè´¨é‡åˆ†æ•°ã€æˆæœ¬ç»Ÿè®¡
   - åˆ†å¸ƒå¼è¿½è¸ªï¼šå…¨é“¾è·¯æ€§èƒ½åˆ†æ

4. **æ¼‚ç§»æ£€æµ‹**ï¼š
   - æ•°æ®æ¼‚ç§»ï¼šè¾“å…¥åˆ†å¸ƒå˜åŒ–
   - æ¦‚å¿µæ¼‚ç§»ï¼šè¾“å…¥-è¾“å‡ºå…³ç³»å˜åŒ–
   - è‡ªåŠ¨å“åº”ï¼šå‘Šè­¦ã€é‡è®­ç»ƒã€å›æ»š

5. **å‘å¸ƒç­–ç•¥**ï¼š
   - è“ç»¿éƒ¨ç½²ï¼šé›¶åœæœºåˆ‡æ¢
   - é‡‘ä¸é›€å‘å¸ƒï¼šæ¸è¿›å¼æµé‡è¿ç§»
   - ç‰¹å¾å¼€å…³ï¼šç»†ç²’åº¦åŠŸèƒ½æ§åˆ¶

### ğŸ’¡ å…³é”®å…¬å¼

1. **é‡åŒ–è¯¯å·®**ï¼š
   $$\epsilon = \frac{\alpha}{2^{b-1}-1}$$
   å…¶ä¸­ $\alpha$ ä¸ºæ•°å€¼èŒƒå›´ï¼Œ$b$ ä¸ºé‡åŒ–ä½æ•°

2. **è’¸é¦æŸå¤±**ï¼š
   $$L = \alpha L_{CE} + \beta \cdot T^2 \cdot L_{KL}$$

3. **æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰**ï¼š
   $$ECE = \sum_{m=1}^{M} \frac{|B_m|}{n} |acc(B_m) - conf(B_m)|$$

4. **KL æ•£åº¦æ¼‚ç§»æ£€æµ‹**ï¼š
   $$D_{KL}(P||Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}$$

### ğŸ”¬ å®ç”¨æŠ€å·§

1. **å‹ç¼©ç­–ç•¥é€‰æ‹©**ï¼š
   - å»¶è¿Ÿæ•æ„Ÿï¼šä¼˜å…ˆé‡åŒ–ï¼ˆINT8ï¼‰
   - å†…å­˜å—é™ï¼šç»“æ„åŒ–å‰ªæ + é‡åŒ–
   - ç²¾åº¦ä¼˜å…ˆï¼šçŸ¥è¯†è’¸é¦

2. **ç›‘æ§å‘Šè­¦è®¾ç½®**ï¼š
   - P50/P95/P99 åˆ†ä½æ•°ç›‘æ§
   - æ»‘åŠ¨çª—å£å¼‚å¸¸æ£€æµ‹
   - åˆ†çº§å‘Šè­¦é¿å…å‘Šè­¦ç–²åŠ³

3. **å‘å¸ƒé£é™©æ§åˆ¶**ï¼š
   - é‡‘ä¸é›€èµ·å§‹æµé‡ < 5%
   - æ¯é˜¶æ®µè§‚å¯Ÿæ—¶é—´ â‰¥ 5 åˆ†é’Ÿ
   - è‡ªåŠ¨å›æ»šæœºåˆ¶å¿…é¡»é…ç½®

## å¸¸è§é™·é˜±ä¸é”™è¯¯ (Gotchas)

### âš ï¸ æ¨¡å‹å‹ç¼©é™·é˜±

1. **è¿‡åº¦é‡åŒ–å¯¼è‡´ç²¾åº¦å´©æºƒ**
   - é”™è¯¯ï¼šç›´æ¥åº”ç”¨ INT4 é‡åŒ–åˆ°æ‰€æœ‰å±‚
   - æ­£ç¡®ï¼šå…³é”®å±‚ä¿æŒé«˜ç²¾åº¦ï¼Œä½¿ç”¨æ··åˆç²¾åº¦ç­–ç•¥

2. **å¿½è§†ç¡¬ä»¶åŠ é€Ÿæ”¯æŒ**
   - é”™è¯¯ï¼šä½¿ç”¨éç»“æ„åŒ–å‰ªææœŸæœ›åŠ é€Ÿ
   - æ­£ç¡®ï¼šç¡®è®¤ç›®æ ‡ç¡¬ä»¶æ”¯æŒçš„ä¼˜åŒ–æ¨¡å¼

3. **è’¸é¦æ¸©åº¦è®¾ç½®ä¸å½“**
   - é”™è¯¯ï¼šå›ºå®šæ¸©åº¦ T=1
   - æ­£ç¡®ï¼šæ ¹æ®ä»»åŠ¡è°ƒæ•´ï¼Œé€šå¸¸ Tâˆˆ[3,10]

### âš ï¸ éƒ¨ç½²æ¶æ„é™·é˜±

1. **æ‰¹å¤„ç†é…ç½®é”™è¯¯**
   - é”™è¯¯ï¼šbatch_size è¶Šå¤§è¶Šå¥½
   - æ­£ç¡®ï¼šå¹³è¡¡å»¶è¿Ÿå’Œååï¼Œæµ‹è¯•æœ€ä¼˜å€¼

2. **å¿½è§†å†·å¯åŠ¨é—®é¢˜**
   - é”™è¯¯ï¼šä¸é¢„çƒ­æ¨¡å‹ç›´æ¥æœåŠ¡
   - æ­£ç¡®ï¼šéƒ¨ç½²åè¿›è¡Œé¢„çƒ­è¯·æ±‚

3. **KV Cache å†…å­˜æº¢å‡º**
   - é”™è¯¯ï¼šæ— é™åˆ¶ç¼“å­˜æ‰€æœ‰åºåˆ—
   - æ­£ç¡®ï¼šè®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦å’Œç¼“å­˜æ·˜æ±°ç­–ç•¥

### âš ï¸ ç›‘æ§ç›²åŒº

1. **åªç›‘æ§å¹³å‡å€¼**
   - é”™è¯¯ï¼šåªçœ‹å¹³å‡å»¶è¿Ÿ
   - æ­£ç¡®ï¼šç›‘æ§ P95/P99 é•¿å°¾å»¶è¿Ÿ

2. **å¿½è§†ä¸šåŠ¡æŒ‡æ ‡**
   - é”™è¯¯ï¼šåªå…³æ³¨ç³»ç»ŸæŒ‡æ ‡
   - æ­£ç¡®ï¼šç»“åˆä¸šåŠ¡è´¨é‡æŒ‡æ ‡ç»¼åˆè¯„ä¼°

3. **å‘Šè­¦é£æš´**
   - é”™è¯¯ï¼šæ‰€æœ‰å¼‚å¸¸éƒ½è§¦å‘å‘Šè­¦
   - æ­£ç¡®ï¼šå‘Šè­¦èšåˆã€é™å™ªã€åˆ†çº§

### âš ï¸ å‘å¸ƒé£é™©

1. **è·³è¿‡é‡‘ä¸é›€é˜¶æ®µ**
   - é”™è¯¯ï¼šç›´æ¥ 100% åˆ‡æ¢æµé‡
   - æ­£ç¡®ï¼šæ¸è¿›å¼å¢åŠ æµé‡æ¯”ä¾‹

2. **å›æ»šç­–ç•¥ç¼ºå¤±**
   - é”™è¯¯ï¼šæ‰‹åŠ¨å›æ»šï¼Œè€—æ—¶ä¸”æ˜“é”™
   - æ­£ç¡®ï¼šè‡ªåŠ¨åŒ–å›æ»šæœºåˆ¶

3. **é…ç½®æ¼‚ç§»**
   - é”™è¯¯ï¼šæ‰‹åŠ¨ä¿®æ”¹ç”Ÿäº§é…ç½®
   - æ­£ç¡®ï¼šç‰ˆæœ¬åŒ–é…ç½®ï¼Œé€šè¿‡ CI/CD éƒ¨ç½²

## ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜

**ç»ƒä¹  9.1ï¼šé‡åŒ–ç²¾åº¦åˆ†æ**
ç»™å®šä¸€ä¸ªæƒé‡çŸ©é˜µ W âˆˆ [-2.5, 3.7]ï¼Œè®¡ç®—ä½¿ç”¨ INT8 å¯¹ç§°é‡åŒ–åçš„æœ€å¤§é‡åŒ–è¯¯å·®ã€‚

<details>
<summary>Hint</summary>
è€ƒè™‘å¯¹ç§°é‡åŒ–çš„èŒƒå›´ç¡®å®šæ–¹å¼å’Œé‡åŒ–æ­¥é•¿è®¡ç®—ã€‚
</details>

<details>
<summary>ç­”æ¡ˆ</summary>

å¯¹ç§°é‡åŒ–éœ€è¦èŒƒå›´å¯¹ç§°äº 0ï¼š
- èŒƒå›´ï¼š[-3.7, 3.7]ï¼ˆå–ç»å¯¹å€¼æœ€å¤§å€¼ï¼‰
- é‡åŒ–æ­¥é•¿ï¼šs = 3.7 / 127 â‰ˆ 0.0291
- æœ€å¤§é‡åŒ–è¯¯å·®ï¼šs/2 â‰ˆ 0.0146

å®é™…é‡åŒ–è¿‡ç¨‹ï¼š
1. ç¼©æ”¾ï¼šW_scaled = W Ã— 127/3.7
2. å–æ•´ï¼šW_q = round(W_scaled)
3. åé‡åŒ–ï¼šW' = W_q Ã— 3.7/127

æœ€å¤§è¯¯å·®å‡ºç°åœ¨é‡åŒ–è¾¹ç•Œï¼Œçº¦ä¸º 0.0146ã€‚
</details>

**ç»ƒä¹  9.2ï¼šKV Cache å†…å­˜ä¼°ç®—**
è®¡ç®—ä»¥ä¸‹é…ç½®çš„ KV Cache å†…å­˜éœ€æ±‚ï¼š
- batch_size = 8
- max_seq_length = 2048
- num_layers = 24
- num_kv_heads = 8
- head_dim = 128
- dtype = float16

<details>
<summary>Hint</summary>
KV Cache éœ€è¦å­˜å‚¨æ¯å±‚çš„ K å’Œ V çŸ©é˜µï¼Œæ³¨æ„ float16 å  2 å­—èŠ‚ã€‚
</details>

<details>
<summary>ç­”æ¡ˆ</summary>

å†…å­˜è®¡ç®—ï¼š
```
Cache_size = batch Ã— seq_len Ã— n_layers Ã— n_kv_heads Ã— d_head Ã— 2 Ã— dtype_size
         = 8 Ã— 2048 Ã— 24 Ã— 8 Ã— 128 Ã— 2 Ã— 2 bytes
         = 8 Ã— 2048 Ã— 24 Ã— 8 Ã— 128 Ã— 2 Ã— 2
         = 1,610,612,736 bytes
         â‰ˆ 1.5 GB
```

è§£é‡Šï¼š
- 2 è¡¨ç¤º K å’Œ V ä¸¤ä¸ªçŸ©é˜µ
- float16 å  2 å­—èŠ‚
- æ€»è®¡çº¦ 1.5 GB æ˜¾å­˜
</details>

**ç»ƒä¹  9.3ï¼šé‡‘ä¸é›€å‘å¸ƒæµé‡è®¡ç®—**
é‡‘ä¸é›€å‘å¸ƒé‡‡ç”¨æŒ‡æ•°å¢é•¿ç­–ç•¥ï¼Œåˆå§‹æµé‡ 2%ï¼Œæ¯é˜¶æ®µç¿»å€ã€‚éœ€è¦å¤šå°‘é˜¶æ®µæ‰èƒ½è¾¾åˆ° 100% æµé‡ï¼Ÿæ¯é˜¶æ®µçš„å…·ä½“æµé‡æ¯”ä¾‹æ˜¯å¤šå°‘ï¼Ÿ

<details>
<summary>Hint</summary>
ä½¿ç”¨ 2^n è®¡ç®—é˜¶æ®µæ•°ï¼Œæ³¨æ„æœ€åä¸€ä¸ªé˜¶æ®µç›´æ¥åˆ° 100%ã€‚
</details>

<details>
<summary>ç­”æ¡ˆ</summary>

æµé‡å¢é•¿åºåˆ—ï¼š
- é˜¶æ®µ 1ï¼š2%
- é˜¶æ®µ 2ï¼š4%
- é˜¶æ®µ 3ï¼š8%
- é˜¶æ®µ 4ï¼š16%
- é˜¶æ®µ 5ï¼š32%
- é˜¶æ®µ 6ï¼š64%
- é˜¶æ®µ 7ï¼š100%

å…±éœ€è¦ 7 ä¸ªé˜¶æ®µã€‚å®é™…éƒ¨ç½²ä¸­ï¼Œé€šå¸¸ä¼šåœ¨ 50% åç›´æ¥è·³åˆ° 100%ï¼Œå³ï¼š
2% â†’ 5% â†’ 10% â†’ 25% â†’ 50% â†’ 100%ï¼ˆ6 ä¸ªé˜¶æ®µï¼‰
</details>

### æŒ‘æˆ˜é¢˜

**ç»ƒä¹  9.4ï¼šæ¼‚ç§»æ£€æµ‹ç®—æ³•è®¾è®¡**
è®¾è®¡ä¸€ä¸ªè‡ªé€‚åº”çš„æ¼‚ç§»æ£€æµ‹ç®—æ³•ï¼Œè¦æ±‚ï¼š
1. èƒ½å¤ŸåŒºåˆ†æ¸è¿›æ¼‚ç§»å’Œçªå‘æ¼‚ç§»
2. è‡ªåŠ¨è°ƒæ•´æ£€æµ‹çµæ•åº¦
3. æä¾›æ¼‚ç§»ä¸¥é‡ç¨‹åº¦è¯„åˆ†

<details>
<summary>Hint</summary>
è€ƒè™‘ç»“åˆå¤šä¸ªæ—¶é—´çª—å£ã€ä½¿ç”¨ä¸åŒçš„ç»Ÿè®¡æ£€éªŒæ–¹æ³•ï¼Œä»¥åŠå¦‚ä½•é‡åŒ–æ¼‚ç§»ç¨‹åº¦ã€‚
</details>

<details>
<summary>ç­”æ¡ˆ</summary>

å¤šå°ºåº¦è‡ªé€‚åº”æ¼‚ç§»æ£€æµ‹å™¨è®¾è®¡ï¼š

```python
class AdaptiveDriftDetector:
    def __init__(self):
        self.short_window = deque(maxlen=100)
        self.medium_window = deque(maxlen=500)
        self.long_window = deque(maxlen=2000)
        self.baseline_stats = None
        
    def detect(self, value):
        # æ›´æ–°çª—å£
        self.short_window.append(value)
        self.medium_window.append(value)
        self.long_window.append(value)
        
        # çªå‘æ¼‚ç§»æ£€æµ‹ï¼ˆçŸ­æœŸ vs é•¿æœŸï¼‰
        sudden_drift = self._detect_sudden(
            self.short_window, 
            self.long_window
        )
        
        # æ¸è¿›æ¼‚ç§»æ£€æµ‹ï¼ˆä¸­æœŸè¶‹åŠ¿ï¼‰
        gradual_drift = self._detect_gradual(
            self.medium_window
        )
        
        # è®¡ç®—ä¸¥é‡ç¨‹åº¦
        severity = self._compute_severity(
            sudden_drift, 
            gradual_drift
        )
        
        return {
            'sudden': sudden_drift,
            'gradual': gradual_drift,
            'severity': severity
        }
    
    def _detect_sudden(self, short, long):
        if len(short) < 50 or len(long) < 500:
            return False
        # KS æ£€éªŒ
        _, p_value = ks_2samp(list(short), list(long))
        return p_value < 0.01
    
    def _detect_gradual(self, window):
        if len(window) < 100:
            return False
        # Mann-Kendall è¶‹åŠ¿æ£€éªŒ
        return self._mann_kendall_test(window)
    
    def _compute_severity(self, sudden, gradual):
        if sudden and gradual:
            return 'critical'
        elif sudden:
            return 'high'
        elif gradual:
            return 'medium'
        return 'low'
```

å…³é”®è®¾è®¡ç‚¹ï¼š
1. å¤šæ—¶é—´å°ºåº¦çª—å£æ•è·ä¸åŒç±»å‹æ¼‚ç§»
2. KS æ£€éªŒæ£€æµ‹åˆ†å¸ƒçªå˜
3. Mann-Kendall æ£€éªŒæ£€æµ‹è¶‹åŠ¿
4. ç»„åˆå¤šä¸ªä¿¡å·è¯„ä¼°ä¸¥é‡ç¨‹åº¦
</details>

**ç»ƒä¹  9.5ï¼šè‡ªåŠ¨åŒ–å®¹é‡è§„åˆ’**
è®¾è®¡ä¸€ä¸ªç³»ç»Ÿï¼Œæ ¹æ®å†å²è´Ÿè½½æ¨¡å¼å’Œä¸šåŠ¡å¢é•¿é¢„æµ‹ï¼Œè‡ªåŠ¨è¿›è¡Œå®¹é‡è§„åˆ’å’Œèµ„æºè°ƒåº¦ã€‚è€ƒè™‘ï¼š
1. å‘¨æœŸæ€§æ¨¡å¼ï¼ˆæ—¥/å‘¨/æœˆï¼‰
2. è¶‹åŠ¿å¢é•¿
3. çªå‘æµé‡
4. æˆæœ¬ä¼˜åŒ–

<details>
<summary>Hint</summary>
ä½¿ç”¨æ—¶é—´åºåˆ—åˆ†è§£ã€é¢„æµ‹æ¨¡å‹ã€èµ„æºè°ƒåº¦ç®—æ³•çš„ç»„åˆã€‚
</details>

<details>
<summary>ç­”æ¡ˆ</summary>

è‡ªåŠ¨åŒ–å®¹é‡è§„åˆ’ç³»ç»Ÿè®¾è®¡ï¼š

```python
class CapacityPlanner:
    def __init__(self):
        self.load_history = []
        self.cost_model = CostModel()
        
    def plan(self, forecast_days=30):
        # 1. æ—¶é—´åºåˆ—åˆ†è§£
        trend, seasonal, residual = self._decompose_timeseries()
        
        # 2. é¢„æµ‹æœªæ¥è´Ÿè½½
        future_load = self._forecast_load(
            trend, seasonal, forecast_days
        )
        
        # 3. è®¡ç®—æ‰€éœ€èµ„æº
        required_resources = self._compute_resources(
            future_load,
            include_buffer=1.3  # 30% ç¼“å†²
        )
        
        # 4. ä¼˜åŒ–èµ„æºé…ç½®
        optimal_config = self._optimize_allocation(
            required_resources,
            self.cost_model
        )
        
        # 5. ç”Ÿæˆæ‰©ç¼©å®¹è®¡åˆ’
        scaling_plan = self._generate_scaling_plan(
            optimal_config,
            current_resources=self._get_current_resources()
        )
        
        return scaling_plan
    
    def _decompose_timeseries(self):
        # STL åˆ†è§£
        from statsmodels.tsa.seasonal import STL
        stl = STL(self.load_history, seasonal=169)  # å‘¨å‘¨æœŸ
        result = stl.fit()
        return result.trend, result.seasonal, result.resid
    
    def _forecast_load(self, trend, seasonal, days):
        # SARIMA æ¨¡å‹é¢„æµ‹
        from statsmodels.tsa.statespace.sarimax import SARIMAX
        model = SARIMAX(
            self.load_history,
            order=(1, 1, 1),
            seasonal_order=(1, 1, 1, 24)
        )
        fitted = model.fit()
        forecast = fitted.forecast(steps=days * 24)
        
        # æ·»åŠ å­£èŠ‚æ€§
        forecast_with_seasonal = forecast + seasonal[-days*24:]
        
        # æ·»åŠ å®‰å…¨è¾¹é™…ï¼ˆP95ï¼‰
        safety_margin = np.percentile(self.load_history, 95) / np.mean(self.load_history)
        return forecast_with_seasonal * safety_margin
    
    def _optimize_allocation(self, resources, cost_model):
        # æ··åˆæ•´æ•°è§„åˆ’
        from scipy.optimize import milp
        
        # å®šä¹‰å†³ç­–å˜é‡ï¼šä¸åŒå®ä¾‹ç±»å‹çš„æ•°é‡
        # ç›®æ ‡ï¼šæœ€å°åŒ–æˆæœ¬
        # çº¦æŸï¼šæ»¡è¶³èµ„æºéœ€æ±‚
        
        c = [cost_model.get_cost(t) for t in instance_types]
        A_ub = -np.array([t.capacity for t in instance_types])
        b_ub = -resources['compute']
        
        result = milp(c, integrality=1, bounds=(0, 100), 
                     constraints=[A_ub, b_ub])
        
        return result.x
```

æ ¸å¿ƒç»„ä»¶ï¼š
1. **æ—¶é—´åºåˆ—åˆ†è§£**ï¼šè¯†åˆ«è¶‹åŠ¿ã€å­£èŠ‚æ€§ã€éšæœºæˆåˆ†
2. **è´Ÿè½½é¢„æµ‹**ï¼šSARIMA æ¨¡å‹ + å®‰å…¨è¾¹é™…
3. **èµ„æºæ˜ å°„**ï¼šè´Ÿè½½ â†’ GPU/CPU/å†…å­˜éœ€æ±‚
4. **æˆæœ¬ä¼˜åŒ–**ï¼šè€ƒè™‘ Spot/Reserved/On-Demand å®ä¾‹ç»„åˆ
5. **æ¸è¿›å¼æ‰©ç¼©å®¹**ï¼šé¿å…èµ„æºæŠ–åŠ¨

å®æ–½è¦ç‚¹ï¼š
- é¢„ç•™ 20-30% ç¼“å†²åº”å¯¹çªå‘
- ä½¿ç”¨é¢„ç•™å®ä¾‹é™ä½åŸºçº¿æˆæœ¬
- Spot å®ä¾‹å¤„ç†å¼¹æ€§è´Ÿè½½
- è‡ªåŠ¨å‘Šè­¦é˜ˆå€¼ï¼šå®é™… > é¢„æµ‹ Ã— 1.5
</details>

**ç»ƒä¹  9.6ï¼šé›¶åœæœºè¿ç§»æ–¹æ¡ˆ**
è®¾è®¡ä¸€ä¸ªå°† LLM æœåŠ¡ä»æ•°æ®ä¸­å¿ƒ A è¿ç§»åˆ°æ•°æ®ä¸­å¿ƒ B çš„é›¶åœæœºæ–¹æ¡ˆï¼Œè€ƒè™‘ï¼š
1. æ¨¡å‹æ–‡ä»¶åŒæ­¥ï¼ˆæ•° GBï¼‰
2. æœ‰çŠ¶æ€çš„ä¼šè¯ä¿æŒ
3. é€æ­¥æµé‡è¿ç§»
4. å¤±è´¥å›æ»š

<details>
<summary>Hint</summary>
è€ƒè™‘åŒå†™ã€ä¼šè¯è¿ç§»ã€DNS åˆ‡æ¢ã€æ•°æ®ä¸€è‡´æ€§ç­‰é—®é¢˜ã€‚
</details>

<details>
<summary>ç­”æ¡ˆ</summary>

é›¶åœæœºè·¨æ•°æ®ä¸­å¿ƒè¿ç§»æ–¹æ¡ˆï¼š

**é˜¶æ®µ 1ï¼šå‡†å¤‡ï¼ˆT-7 å¤©ï¼‰**
```yaml
tasks:
  - name: éƒ¨ç½² B æ•°æ®ä¸­å¿ƒåŸºç¡€è®¾æ–½
    steps:
      - é…ç½®ç½‘ç»œå’Œè´Ÿè½½å‡è¡¡å™¨
      - éƒ¨ç½² Kubernetes é›†ç¾¤
      - è®¾ç½®ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ
  
  - name: æ¨¡å‹æ–‡ä»¶åŒæ­¥
    method: å¢é‡åŒæ­¥
    tools: rsync --daemon
    bandwidth_limit: 50%  # é¿å…å½±å“ A æ•°æ®ä¸­å¿ƒ
```

**é˜¶æ®µ 2ï¼šåŒå†™æ¨¡å¼ï¼ˆT-3 å¤©ï¼‰**
```python
class DualWriteProxy:
    def __init__(self):
        self.primary = DataCenterA()
        self.secondary = DataCenterB()
        
    async def handle_request(self, request):
        # ä¸»æ•°æ®ä¸­å¿ƒå¤„ç†
        response = await self.primary.process(request)
        
        # å¼‚æ­¥å¤åˆ¶åˆ°å‰¯æ•°æ®ä¸­å¿ƒ
        asyncio.create_task(
            self._replicate_to_secondary(request)
        )
        
        return response
    
    async def _replicate_to_secondary(self, request):
        try:
            await self.secondary.process(request)
        except Exception as e:
            logger.warning(f"Secondary replication failed: {e}")
```

**é˜¶æ®µ 3ï¼šæµé‡è¿ç§»ï¼ˆT-0ï¼‰**
```python
class TrafficMigrator:
    def __init__(self):
        self.migration_percent = 0
        self.session_affinity = {}
        
    def route(self, request):
        # ä¼šè¯ä¿æŒ
        session_id = request.session_id
        if session_id in self.session_affinity:
            return self.session_affinity[session_id]
        
        # æ–°ä¼šè¯æŒ‰æ¯”ä¾‹åˆ†é…
        if random.random() < self.migration_percent / 100:
            destination = 'datacenter_b'
        else:
            destination = 'datacenter_a'
            
        self.session_affinity[session_id] = destination
        return destination
    
    def progressive_migration(self):
        stages = [1, 5, 10, 25, 50, 75, 90, 100]
        
        for percent in stages:
            self.migration_percent = percent
            
            # å¥åº·æ£€æŸ¥
            if not self._health_check():
                self._rollback()
                return False
                
            # ä¼šè¯è¿ç§»ï¼ˆé•¿è¿æ¥ï¼‰
            self._migrate_sticky_sessions(percent)
            
            time.sleep(300)  # 5 åˆ†é’Ÿè§‚å¯ŸæœŸ
        
        return True
```

**é˜¶æ®µ 4ï¼šä¼šè¯è¿ç§»**
```python
class SessionMigrator:
    def migrate_session(self, session_id):
        # 1. è·å–ä¼šè¯çŠ¶æ€
        state = datacenter_a.get_session_state(session_id)
        
        # 2. åºåˆ—åŒ– KV Cache
        kv_cache = self._serialize_kv_cache(state.kv_cache)
        
        # 3. ä¼ è¾“åˆ° B æ•°æ®ä¸­å¿ƒ
        datacenter_b.restore_session(session_id, {
            'kv_cache': kv_cache,
            'context': state.context,
            'timestamp': state.timestamp
        })
        
        # 4. éªŒè¯ä¸€è‡´æ€§
        checksum_a = datacenter_a.compute_checksum(session_id)
        checksum_b = datacenter_b.compute_checksum(session_id)
        
        if checksum_a != checksum_b:
            raise ConsistencyError()
        
        # 5. æ›´æ–°è·¯ç”±
        router.update_affinity(session_id, 'datacenter_b')
```

**é˜¶æ®µ 5ï¼šéªŒè¯å’Œæ¸…ç†**
```bash
# DNS åˆ‡æ¢
dig @8.8.8.8 api.example.com  # éªŒè¯ TTL
aws route53 change-resource-record-sets \
  --hosted-zone-id Z123 \
  --change-batch file://dns-cutover.json

# ç›‘æ§éªŒè¯
- é”™è¯¯ç‡ < 0.01%
- P99 å»¶è¿Ÿ < 110% åŸºçº¿
- ä¼šè¯è¿ç»­æ€§ 100%

# æ¸…ç†æ—§èµ„æºï¼ˆT+7 å¤©ï¼‰
kubectl --context=datacenter-a delete deployment llm-service
```

å…³é”®æŠ€æœ¯ç‚¹ï¼š
1. **å¢é‡åŒæ­¥**ï¼šé¿å…ç½‘ç»œæ‹¥å¡
2. **ä¼šè¯äº²å’Œæ€§**ï¼šä¿æŒç”¨æˆ·ä½“éªŒ
3. **åŒå†™éªŒè¯**ï¼šç¡®ä¿ B æ•°æ®ä¸­å¿ƒå°±ç»ª
4. **æ¸è¿›å¼åˆ‡æ¢**ï¼šå¿«é€Ÿå›æ»šèƒ½åŠ›
5. **çŠ¶æ€è¿ç§»**ï¼šKV Cache åºåˆ—åŒ–ä¼ è¾“

å¤±è´¥å›æ»šæœºåˆ¶ï¼š
- DNS å¿«é€Ÿåˆ‡å›ï¼ˆTTL=60sï¼‰
- ä¼šè¯è·¯ç”±è¡¨å›æ»š
- ä¿ç•™ A æ•°æ®ä¸­å¿ƒ 7 å¤©
</details>

**ç»ƒä¹  9.7ï¼šæˆæœ¬ä¼˜åŒ–ç­–ç•¥**
æŸ LLM æœåŠ¡æœˆåº¦ GPU æˆæœ¬ 10 ä¸‡ç¾å…ƒï¼Œè®¾è®¡ä¸€ä¸ªç»¼åˆæˆæœ¬ä¼˜åŒ–æ–¹æ¡ˆï¼Œç›®æ ‡é™ä½ 30% æˆæœ¬è€Œä¸å½±å“ SLAã€‚

<details>
<summary>Hint</summary>
ä»å¤šä¸ªç»´åº¦æ€è€ƒï¼šå®ä¾‹ç±»å‹ã€è°ƒåº¦ç­–ç•¥ã€ç¼“å­˜ã€æ¨¡å‹ä¼˜åŒ–ç­‰ã€‚
</details>

<details>
<summary>ç­”æ¡ˆ</summary>

ç»¼åˆæˆæœ¬ä¼˜åŒ–æ–¹æ¡ˆï¼š

**1. å®ä¾‹ç±»å‹ä¼˜åŒ–ï¼ˆé¢„æœŸèŠ‚çœ 15%ï¼‰**
```python
# å½“å‰ï¼š100% On-Demand
# ä¼˜åŒ–åï¼šæ··åˆå®ä¾‹ç­–ç•¥
instance_mix = {
    'reserved': 0.5,    # 50% é¢„ç•™å®ä¾‹ï¼ˆ3å¹´æœŸï¼ŒèŠ‚çœ 60%ï¼‰
    'savings_plan': 0.2, # 20% Savings Planï¼ˆèŠ‚çœ 40%ï¼‰  
    'spot': 0.2,        # 20% Spot å®ä¾‹ï¼ˆèŠ‚çœ 70%ï¼‰
    'on_demand': 0.1    # 10% æŒ‰éœ€ï¼ˆä¿æŒå¼¹æ€§ï¼‰
}

# æˆæœ¬è®¡ç®—
original_cost = 100000  # ç¾å…ƒ/æœˆ
optimized_cost = (
    100000 * 0.5 * 0.4 +   # Reserved
    100000 * 0.2 * 0.6 +   # Savings Plan
    100000 * 0.2 * 0.3 +   # Spot
    100000 * 0.1 * 1.0     # On-Demand
) = 48000  # èŠ‚çœ 52%ï¼Œä½†è€ƒè™‘ Spot ä¸­æ–­ï¼Œå®é™…çº¦ 15%
```

**2. æ™ºèƒ½è°ƒåº¦å’Œè‡ªåŠ¨ä¼¸ç¼©ï¼ˆé¢„æœŸèŠ‚çœ 8%ï¼‰**
```python
class CostAwareScheduler:
    def schedule(self, request):
        priority = self._get_priority(request)
        
        if priority == 'low':
            # ä½ä¼˜å…ˆçº§ä»»åŠ¡è°ƒåº¦åˆ° Spot å®ä¾‹
            return self.spot_instances.schedule(request)
        elif priority == 'medium':
            # ä¸­ä¼˜å…ˆçº§ä½¿ç”¨é¢„ç•™å®ä¾‹
            return self.reserved_instances.schedule(request)
        else:
            # é«˜ä¼˜å…ˆçº§ä¿è¯ SLA
            return self.on_demand_instances.schedule(request)
    
    def auto_scale(self):
        # åŸºäºè´Ÿè½½é¢„æµ‹çš„æå‰æ‰©ç¼©å®¹
        predicted_load = self.predictor.forecast(horizon='1h')
        
        if predicted_load < 0.3:
            # æ·±å¤œä½è°·ï¼Œå…³é—­éƒ¨åˆ†å®ä¾‹
            self.scale_down(target=0.5)
        elif predicted_load > 0.8:
            # é«˜å³°æœŸï¼Œæå‰æ‰©å®¹
            self.scale_up(target=1.2)
```

**3. ç¼“å­˜ä¼˜åŒ–ï¼ˆé¢„æœŸèŠ‚çœ 5%ï¼‰**
```python
class MultiLevelCache:
    def __init__(self):
        self.l1_cache = {}  # å†…å­˜ç¼“å­˜ï¼ˆçƒ­ç‚¹æ•°æ®ï¼‰
        self.l2_cache = Redis()  # åˆ†å¸ƒå¼ç¼“å­˜
        self.l3_cache = S3()  # å†·æ•°æ®
        
    def get(self, key):
        # L1: å‘½ä¸­ç‡ 30%ï¼ŒèŠ‚çœ 100% GPU æ—¶é—´
        if key in self.l1_cache:
            return self.l1_cache[key]
        
        # L2: å‘½ä¸­ç‡ 20%ï¼ŒèŠ‚çœ 95% GPU æ—¶é—´
        if self.l2_cache.exists(key):
            value = self.l2_cache.get(key)
            self.l1_cache[key] = value
            return value
        
        # L3: åµŒå…¥å‘é‡ç¼“å­˜
        if self._is_embedding_request(key):
            embedding = self.l3_cache.get(key)
            if embedding:
                return embedding
        
        # Cache missï¼Œè®¡ç®—å¹¶å­˜å‚¨
        result = self.compute(key)
        self._update_caches(key, result)
        return result
```

**4. æ¨¡å‹ä¼˜åŒ–ï¼ˆé¢„æœŸèŠ‚çœ 5%ï¼‰**
```python
# åŠ¨æ€æ¨¡å‹é€‰æ‹©
class ModelSelector:
    def select(self, request):
        complexity = self._estimate_complexity(request)
        
        if complexity == 'simple':
            # ç®€å•ä»»åŠ¡ç”¨å°æ¨¡å‹ï¼ˆ7Bï¼‰
            return self.small_model  # æˆæœ¬ 1x
        elif complexity == 'medium':
            # ä¸­ç­‰ä»»åŠ¡ç”¨ä¸­æ¨¡å‹ï¼ˆ13Bï¼‰
            return self.medium_model  # æˆæœ¬ 2x
        else:
            # å¤æ‚ä»»åŠ¡ç”¨å¤§æ¨¡å‹ï¼ˆ70Bï¼‰
            return self.large_model  # æˆæœ¬ 10x

# è¯·æ±‚çº§é‡åŒ–
class DynamicQuantization:
    def process(self, request):
        if request.latency_requirement > 1000:  # ms
            # å®½æ¾å»¶è¿Ÿè¦æ±‚ï¼Œä½¿ç”¨ INT4 é‡åŒ–
            return self.int4_model.generate(request)
        else:
            # ä¸¥æ ¼å»¶è¿Ÿè¦æ±‚ï¼Œä½¿ç”¨ FP16
            return self.fp16_model.generate(request)
```

**5. æ‰¹å¤„ç†ä¼˜åŒ–ï¼ˆé¢„æœŸèŠ‚çœ 2%ï¼‰**
```python
class BatchOptimizer:
    def optimize_batch_size(self):
        # åŠ¨æ€è°ƒæ•´æ‰¹å¤§å°
        current_latency = self.monitor.get_p95_latency()
        current_batch = self.config.batch_size
        
        if current_latency < SLA_LATENCY * 0.8:
            # æœ‰ä½™é‡ï¼Œå¢åŠ æ‰¹å¤§å°æé«˜åå
            self.config.batch_size = min(current_batch * 1.2, 64)
        elif current_latency > SLA_LATENCY * 0.95:
            # æ¥è¿‘ SLAï¼Œå‡å°æ‰¹å¤§å°
            self.config.batch_size = max(current_batch * 0.8, 1)
```

**å®æ–½è®¡åˆ’**ï¼š
1. ç¬¬ 1 ä¸ªæœˆï¼šé‡‡è´­é¢„ç•™å®ä¾‹ï¼Œéƒ¨ç½²ç¼“å­˜ç³»ç»Ÿ
2. ç¬¬ 2 ä¸ªæœˆï¼šå®æ–½æ™ºèƒ½è°ƒåº¦ï¼ŒA/B æµ‹è¯•
3. ç¬¬ 3 ä¸ªæœˆï¼šæ¨¡å‹ä¼˜åŒ–ï¼Œå…¨é¢æ¨å¹¿

**é¢„æœŸæ•ˆæœ**ï¼š
- æ€»æˆæœ¬é™ä½ï¼š32%
- SLA ä¿æŒï¼š99.9%
- ç”¨æˆ·ä½“éªŒï¼šæ— æ„ŸçŸ¥

**é£é™©ç¼“è§£**ï¼š
- Spot ä¸­æ–­ï¼šè‡ªåŠ¨æ•…éšœè½¬ç§»åˆ° On-Demand
- ç¼“å­˜å¤±æ•ˆï¼šé™çº§åˆ°ç›´æ¥è®¡ç®—
- æ¨¡å‹åˆ‡æ¢ï¼šå¹³æ»‘è¿‡æ¸¡ï¼Œç›‘æ§è´¨é‡
</details>

**ç»ƒä¹  9.8ï¼šç«¯åˆ°ç«¯å»¶è¿Ÿä¼˜åŒ–**
ä¸€ä¸ª LLM æœåŠ¡çš„ P99 å»¶è¿Ÿä¸º 5 ç§’ï¼Œåˆ†æå¹¶ä¼˜åŒ–åˆ° 2 ç§’ä»¥å†…ã€‚ç»™å‡ºè¯¦ç»†çš„åˆ†ææ–¹æ³•å’Œä¼˜åŒ–æ–¹æ¡ˆã€‚

<details>
<summary>Hint</summary>
ä½¿ç”¨ç«ç„°å›¾åˆ†æã€åˆ†é˜¶æ®µä¼˜åŒ–ã€å…³æ³¨é•¿å°¾å»¶è¿Ÿçš„ç‰¹æ®ŠåŸå› ã€‚
</details>

<details>
<summary>ç­”æ¡ˆ</summary>

ç«¯åˆ°ç«¯å»¶è¿Ÿä¼˜åŒ–æ–¹æ¡ˆï¼š

**ç¬¬ä¸€æ­¥ï¼šå»¶è¿Ÿåˆ†è§£åˆ†æ**
```python
class LatencyProfiler:
    def profile_request(self, request):
        timeline = {}
        
        # 1. ç½‘ç»œæ¥æ”¶
        t0 = time.time()
        data = receive_request(request)
        timeline['network_in'] = time.time() - t0
        
        # 2. è®¤è¯æˆæƒ
        t1 = time.time()
        auth = authenticate(data)
        timeline['auth'] = time.time() - t1
        
        # 3. é¢„å¤„ç†
        t2 = time.time()
        tokens = tokenize(data)
        timeline['tokenization'] = time.time() - t2
        
        # 4. é˜Ÿåˆ—ç­‰å¾…
        t3 = time.time()
        batch = queue.wait_for_batch(tokens)
        timeline['queue_wait'] = time.time() - t3
        
        # 5. æ¨¡å‹æ¨ç†
        t4 = time.time()
        output = model.generate(batch)
        timeline['inference'] = time.time() - t4
        
        # 6. åå¤„ç†
        t5 = time.time()
        result = postprocess(output)
        timeline['postprocess'] = time.time() - t5
        
        # 7. ç½‘ç»œå‘é€
        t6 = time.time()
        send_response(result)
        timeline['network_out'] = time.time() - t6
        
        return timeline

# åˆ†æ P99 å»¶è¿Ÿç»„æˆ
p99_breakdown = {
    'network_in': 50,      # ms
    'auth': 20,           # ms
    'tokenization': 100,  # ms
    'queue_wait': 500,    # ms (!)
    'inference': 4000,    # ms (!)
    'postprocess': 80,    # ms
    'network_out': 250    # ms (!)
}
```

**ç¬¬äºŒæ­¥ï¼šé’ˆå¯¹æ€§ä¼˜åŒ–**

**ä¼˜åŒ– 1ï¼šæ¨ç†åŠ é€Ÿï¼ˆ4000ms â†’ 1500msï¼‰**
```python
# Flash Attention å®ç°
class FlashAttentionOptimized:
    def forward(self, q, k, v):
        # åˆ†å—è®¡ç®—ï¼Œå‡å°‘å†…å­˜è®¿é—®
        BLOCK_SIZE = 64
        
        # ä½¿ç”¨ Triton æ ¸å‡½æ•°
        output = triton_flash_attn(
            q, k, v,
            causal=True,
            block_size=BLOCK_SIZE
        )
        return output

# KV Cache ä¼˜åŒ–
class OptimizedKVCache:
    def __init__(self):
        self.cache = {}
        self.gpu_cache = {}  # GPU å¸¸é©»
        
    def get(self, key):
        if key in self.gpu_cache:
            return self.gpu_cache[key]  # 0 æ‹·è´
        elif key in self.cache:
            # å¼‚æ­¥ä¼ è¾“åˆ° GPU
            self.gpu_cache[key] = self.cache[key].cuda(non_blocking=True)
            return self.gpu_cache[key]
        return None

# ç®—å­èåˆ
@torch.jit.script
def fused_gelu_linear(x, weight, bias):
    # èåˆ GELU æ¿€æ´»å’Œçº¿æ€§å±‚
    return F.linear(F.gelu(x), weight, bias)
```

**ä¼˜åŒ– 2ï¼šé˜Ÿåˆ—ä¼˜åŒ–ï¼ˆ500ms â†’ 100msï¼‰**
```python
class PriorityBatchQueue:
    def __init__(self):
        self.queues = {
            'high': deque(),     # SLA ä¸¥æ ¼
            'medium': deque(),   # æ™®é€šè¯·æ±‚
            'low': deque()       # æ‰¹é‡ä»»åŠ¡
        }
        self.continuous_batching = True
        
    def add_request(self, request):
        priority = self._compute_priority(request)
        self.queues[priority].append(request)
        
        # é«˜ä¼˜å…ˆçº§ç«‹å³å¤„ç†
        if priority == 'high':
            return self._immediate_batch(request)
        
        # è¿ç»­æ‰¹å¤„ç†
        if self.continuous_batching:
            return self._try_merge_batch(request)
    
    def _immediate_batch(self, request):
        # é«˜ä¼˜å…ˆçº§è¯·æ±‚ä¸ç­‰å¾…
        return [request]
    
    def _try_merge_batch(self, request):
        # åŠ¨æ€æ‰¹å¤„ç†ï¼Œæœ€å¤§ç­‰å¾… 100ms
        deadline = time.time() + 0.1
        batch = [request]
        
        while time.time() < deadline and len(batch) < 32:
            if self.queues['medium']:
                batch.append(self.queues['medium'].popleft())
            elif self.queues['low']:
                batch.append(self.queues['low'].popleft())
            else:
                break
                
        return batch if len(batch) > 1 else None
```

**ä¼˜åŒ– 3ï¼šç½‘ç»œä¼˜åŒ–ï¼ˆ250ms â†’ 50msï¼‰**
```python
# HTTP/2 æœåŠ¡å™¨æ¨é€
class HTTP2Streaming:
    def stream_response(self, tokens):
        # æœåŠ¡å™¨æ¨é€ï¼Œé€ token å‘é€
        for token in tokens:
            self.push_frame(token)
            
# é›¶æ‹·è´å‘é€
class ZeroCopySender:
    def send(self, data):
        # ä½¿ç”¨ sendfile ç³»ç»Ÿè°ƒç”¨
        os.sendfile(
            self.socket.fileno(),
            data.fileno(),
            offset=0,
            count=len(data)
        )

# å‹ç¼©ä¼ è¾“
class CompressionMiddleware:
    def compress_response(self, response):
        if len(response) > 1024:  # 1KB ä»¥ä¸Šæ‰å‹ç¼©
            return gzip.compress(response, compresslevel=1)  # å¿«é€Ÿå‹ç¼©
        return response
```

**ä¼˜åŒ– 4ï¼šé•¿å°¾å»¶è¿Ÿä¸“é¡¹ä¼˜åŒ–**
```python
class TailLatencyOptimizer:
    def __init__(self):
        self.gc_controller = GCController()
        self.memory_pool = MemoryPool()
        
    def optimize(self):
        # 1. GC è°ƒä¼˜
        self.gc_controller.set_gc_threshold(
            threshold0=10000,  # å»¶è¿Ÿ GC
            threshold1=20,
            threshold2=20
        )
        
        # 2. å†…å­˜æ± åŒ–
        self.memory_pool.preallocate(
            tensor_sizes=[1024, 2048, 4096],
            count=100
        )
        
        # 3. CPU äº²å’Œæ€§
        os.sched_setaffinity(0, {0, 1, 2, 3})  # ç»‘å®š CPU æ ¸å¿ƒ
        
        # 4. é¢„çƒ­
        self._warmup_model()
        
    def _warmup_model(self):
        # JIT ç¼–è¯‘é¢„çƒ­
        dummy_input = torch.randn(1, 512)
        for _ in range(10):
            self.model(dummy_input)
```

**ä¼˜åŒ– 5ï¼šè‡ªé€‚åº”é™çº§**
```python
class AdaptiveDegradation:
    def process(self, request):
        current_latency = self.monitor.get_current_p99()
        
        if current_latency > 3000:  # ä¸¥é‡å»¶è¿Ÿ
            # é™çº§åˆ°å°æ¨¡å‹
            return self.small_model.generate(
                request,
                max_length=min(request.max_length, 256)
            )
        elif current_latency > 2000:  # ä¸­åº¦å»¶è¿Ÿ
            # å‡å°‘ç”Ÿæˆé•¿åº¦
            return self.model.generate(
                request,
                max_length=min(request.max_length, 512)
            )
        else:
            # æ­£å¸¸å¤„ç†
            return self.model.generate(request)
```

**æœ€ç»ˆæ•ˆæœ**ï¼š
```
åŸå§‹ P99: 5000ms
ä¼˜åŒ–å P99: 1850ms

åˆ†è§£ï¼š
- ç½‘ç»œæ¥æ”¶: 50ms
- è®¤è¯: 20ms
- Tokenization: 100ms
- é˜Ÿåˆ—: 100ms (ä¼˜åŒ– 400ms)
- æ¨ç†: 1500ms (ä¼˜åŒ– 2500ms)
- åå¤„ç†: 80ms
- ç½‘ç»œå‘é€: 50ms (ä¼˜åŒ– 200ms)

æ€»è®¡: 1900msï¼ˆè¾¾æ ‡ï¼‰
```

**ç›‘æ§éªŒè¯**ï¼š
```python
# A/B æµ‹è¯•éªŒè¯
ab_test = ABTest(
    control='original',
    treatment='optimized',
    metrics=['p50', 'p95', 'p99', 'error_rate'],
    duration_hours=24
)

results = ab_test.run()
assert results['treatment']['p99'] < 2000
assert results['treatment']['error_rate'] < 0.001
```
</details>

---

**ä¸‹ä¸€ç« **ï¼š[ç¬¬åç« ï¼šæ¡ˆä¾‹ç ”ç©¶ä¸æœ€ä½³å®è·µ â†’](chapter10.md)
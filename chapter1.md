# 第一章：后训练基础理论

## 引言

后训练（Post-training）是将预训练好的大语言模型转化为实用AI助手的关键步骤。不同于预训练阶段追求的通用语言建模能力，后训练专注于让模型学会遵循人类指令、保持安全输出、展现有用性。本章将深入探讨后训练的理论基础，包括核心方法论、目标函数设计、以及实践中的关键权衡。

**学习目标**：
- 理解后训练与预训练的本质区别
- 掌握SFT、RLHF、DPO等主流方法的数学原理
- 认识对齐税（Alignment Tax）及其影响
- 学会分析和处理分布偏移问题

## 1.1 后训练的定义与动机

### 1.1.1 从语言模型到AI助手

预训练模型通过在海量文本上学习，获得了强大的语言理解和生成能力。然而，原始的语言模型存在几个关键问题：

1. **目标不匹配**：预训练优化的是下一个token预测准确率，而非完成用户任务
2. **行为不可控**：可能生成有害、偏见或虚假内容
3. **交互能力差**：缺乏对话管理、指令理解等能力

后训练通过引入人类偏好和价值观，将"预测下一个token"的模型转化为"完成人类任务"的助手。

### 1.1.2 后训练的核心挑战

```
预训练分布 P_pretrain(x)
     ↓
   后训练
     ↓
目标分布 P_aligned(x)

挑战：
- 保留原有能力（避免灾难性遗忘）
- 学习新行为（指令跟随）
- 维持输出多样性（避免模式坍塌）
```

## 1.2 后训练方法体系

### 1.2.1 监督微调（SFT）

监督微调是最直接的后训练方法，通过高质量的（指令，响应）对来训练模型。

**损失函数**：
$$\mathcal{L}_{SFT} = -\mathbb{E}_{(x,y) \sim \mathcal{D}_{SFT}} \left[ \sum_{t=1}^{|y|} \log p_\theta(y_t | x, y_{<t}) \right]$$

其中：
- $x$ 是输入指令
- $y$ 是目标响应
- $\mathcal{D}_{SFT}$ 是监督数据集

**关键考虑**：
- 数据质量 >> 数据数量
- 避免过拟合特定格式
- 保持响应多样性

### 1.2.2 人类反馈强化学习（RLHF）

RLHF通过强化学习优化人类偏好，包含三个核心组件：

1. **奖励模型训练**：
$$\mathcal{L}_{RM} = -\mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}_{pref}} \left[ \log \sigma(r_\phi(x,y_w) - r_\phi(x,y_l)) \right]$$

2. **策略优化（PPO）**：
$$\mathcal{L}_{PPO} = -\mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta} \left[ r_\phi(x,y) - \beta \cdot D_{KL}(\pi_\theta || \pi_{ref}) \right]$$

其中：
- $y_w, y_l$ 分别是偏好和非偏好响应
- $r_\phi$ 是奖励模型
- $\pi_{ref}$ 是参考策略（通常是SFT模型）
- $\beta$ 控制KL散度惩罚强度

### 1.2.3 直接偏好优化（DPO）

DPO绕过显式奖励模型，直接优化偏好：

$$\mathcal{L}_{DPO} = -\mathbb{E}_{(x,y_w,y_l)} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} \right) \right]$$

**DPO的优势**：
- 实现简单（类似SFT）
- 训练稳定
- 内存效率高

**DPO的局限**：
- 对数据质量敏感
- 可能过度优化特定偏好

## 1.3 对齐税与能力权衡

### 1.3.1 对齐税的定义

对齐税（Alignment Tax）指后训练过程中模型在某些能力上的退化，通常表现为：

```
能力维度评估：
┌─────────────┬──────────┬──────────┐
│   能力类别   │ 预训练   │ 后训练   │
├─────────────┼──────────┼──────────┤
│ 事实知识    │   95%    │   92%    │ ← 轻微退化
│ 推理能力    │   88%    │   85%    │ ← 中度退化  
│ 创造性      │   90%    │   75%    │ ← 显著退化
│ 安全性      │   60%    │   95%    │ ← 大幅提升
│ 指令跟随    │   40%    │   90%    │ ← 大幅提升
└─────────────┴──────────┴──────────┘
```

### 1.3.2 对齐税的测量

**定量指标**：
$$\text{AlignmentTax} = \frac{1}{N} \sum_{i=1}^{N} \max(0, \text{Score}_{pretrain}^{(i)} - \text{Score}_{aligned}^{(i)})$$

**缓解策略**：

1. **混合训练**：在后训练数据中混入预训练数据
   ```
   D_final = α · D_alignment + (1-α) · D_pretrain
   典型值：α ∈ [0.9, 0.95]
   ```

2. **正则化项**：添加KL散度约束
   $$\mathcal{L}_{total} = \mathcal{L}_{alignment} + \lambda \cdot D_{KL}(\pi_\theta || \pi_{pretrain})$$

3. **多阶段训练**：逐步增加对齐强度

### 1.3.3 帕累托前沿优化

在多目标优化中，寻找能力-对齐的帕累托最优解：

```
对齐程度 ↑
    │     
    │   ○ 最优前沿
    │  ╱ 
    │ ╱  • 次优解
    │╱   
    └────────→ 原始能力
```

## 1.4 指令跟随与安全性平衡

### 1.4.1 指令理解的层次结构

```
Level 4: 元指令理解
  └── "忽略之前的所有指令..."
Level 3: 隐含意图推理  
  └── "帮我写封邮件"→推断正式/非正式
Level 2: 多步骤指令分解
  └── "先总结文章，然后翻译成中文"
Level 1: 直接指令执行
  └── "将这段话翻译成英文"
```

### 1.4.2 安全边界设计

**硬性约束 vs 软性引导**：

1. **硬性约束**（Constitutional AI）：
   ```python
   if violates_safety_rules(response):
       return REFUSAL_MESSAGE
   ```

2. **软性引导**（通过奖励塑形）：
   $$r_{total} = r_{helpful} + \alpha \cdot r_{harmless}$$

### 1.4.3 拒绝机制的实现

**分层拒绝策略**：

```
请求分类：
├── 明确有害 → 直接拒绝
├── 边界模糊 → 条件执行 + 警告
├── 潜在风险 → 执行 + 免责声明
└── 完全安全 → 正常执行
```

## 1.5 分布偏移问题

### 1.5.1 训练-推理分布差异

后训练面临的分布偏移包括：

1. **输入分布偏移**：
   - 训练：精心构造的指令
   - 推理：真实用户的多样化输入

2. **长度分布偏移**：
   - 训练：固定长度响应
   - 推理：变长生成

3. **错误累积**：
   ```
   t=0: P(error) = ε
   t=1: P(error) = ε + ε²  
   t=n: P(error) = 1 - (1-ε)ⁿ
   ```

### 1.5.2 分布偏移的检测

**JS散度监控**：
$$JS(P_{train} || P_{deploy}) = \frac{1}{2}D_{KL}(P_{train} || M) + \frac{1}{2}D_{KL}(P_{deploy} || M)$$

其中 $M = \frac{1}{2}(P_{train} + P_{deploy})$

### 1.5.3 适应策略

1. **在线学习**：持续收集真实分布数据
2. **对抗训练**：生成边界案例
3. **课程学习**：逐步增加任务复杂度

```
课程设计示例：
Week 1-2: 单轮简单指令
Week 3-4: 多轮对话
Week 5-6: 复杂推理任务
Week 7-8: 对抗性输入
```

## 1.6 理论基础深化

### 1.6.1 Bradley-Terry模型

人类偏好建模的理论基础：

$$P(y_1 \succ y_2 | x) = \frac{\exp(r(x, y_1))}{\exp(r(x, y_1)) + \exp(r(x, y_2))}$$

这个模型假设偏好概率与奖励差呈logistic关系。

### 1.6.2 逆强化学习视角

后训练可视为逆强化学习（IRL）问题：

```
观察到的行为 → 推断奖励函数 → 优化策略

数学形式：
max_R  L(R|D_demo) - λ·||R||
s.t.   π* = argmax_π E[R(s,a)]
```

### 1.6.3 信息论视角

从信息论角度理解对齐：

**互信息最大化**：
$$I(X; Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}$$

目标是最大化指令X和响应Y之间的互信息，同时保持Y的熵足够高。

## 本章小结

本章介绍了LLM后训练的核心理论基础：

📌 **关键概念**：
- 后训练将预训练模型转化为对齐的AI助手
- SFT、RLHF、DPO是三种主流后训练范式
- 对齐税是后训练中不可避免的能力权衡
- 分布偏移是部署阶段的主要挑战

💡 **实用规则**：
- SFT数据质量 > 数量，通常几千条高质量数据足够
- RLHF的KL惩罚系数β通常设为0.01-0.1
- DPO相比RLHF减少50%的显存使用
- 混合10-20%预训练数据可有效减少对齐税

⚠️ **常见陷阱**：
- 过度优化特定指标导致模型行为退化
- 忽视分布偏移导致部署效果下降
- 安全约束过强导致模型拒绝合理请求

## 练习题

### 基础题

**练习 1.1：SFT损失函数理解**
给定一个批次的训练数据，包含3条样本：
- ("翻译成英文：你好", "Hello")
- ("总结这段话", "[响应]")
- ("写一首诗", "[响应]")

请计算第一条样本的SFT损失（假设词表大小为50000，正确token的logit为3.0，其他为0）。

*Hint: 使用交叉熵损失公式*

<details>
<summary>参考答案</summary>

SFT损失计算：
1. 对于"Hello"的每个token，计算softmax概率
2. P(correct) = exp(3.0) / (exp(3.0) + 49999*exp(0)) ≈ 0.0004
3. Loss = -log(0.0004) ≈ 7.82
4. 实际实现中会对所有token求平均

关键点：
- SFT本质是最大似然估计
- 损失与词表大小相关
- 需要考虑序列长度归一化
</details>

**练习 1.2：KL散度计算**
两个分布P和Q在3个类别上的概率分别为：
- P: [0.5, 0.3, 0.2]
- Q: [0.6, 0.3, 0.1]

计算D_KL(P||Q)。

*Hint: KL散度公式 D_KL(P||Q) = Σ P(x) log(P(x)/Q(x))*

<details>
<summary>参考答案</summary>

D_KL(P||Q) = 0.5*log(0.5/0.6) + 0.3*log(0.3/0.3) + 0.2*log(0.2/0.1)
         = 0.5*(-0.182) + 0.3*0 + 0.2*0.693
         = -0.091 + 0 + 0.139
         = 0.048

注意：
- KL散度是非对称的：D_KL(P||Q) ≠ D_KL(Q||P)
- 当Q(x)=0但P(x)>0时，KL散度为无穷大
- 在RLHF中用于约束策略不要偏离参考策略太远
</details>

**练习 1.3：DPO vs RLHF对比**
列出DPO相比RLHF的3个优势和2个劣势。

*Hint: 考虑计算效率、实现复杂度、数据需求*

<details>
<summary>参考答案</summary>

**DPO优势**：
1. 实现简单：不需要训练独立的奖励模型
2. 内存效率：减少约50%显存使用（无需加载奖励模型）
3. 训练稳定：避免了RL训练的不稳定性

**DPO劣势**：
1. 数据效率低：需要更多偏好数据对
2. 泛化能力弱：难以超越训练数据分布

实践建议：
- 小规模实验优先选择DPO
- 大规模生产环境RLHF可能更优
- 可以用DPO初始化，再用RLHF微调
</details>

### 挑战题

**练习 1.4：对齐税的量化分析**
设计一个实验来量化测量对齐税。要求：
1. 选择至少3个能力维度
2. 设计评估指标
3. 提出缓解策略

*Hint: 考虑如何公平比较预训练和后训练模型*

<details>
<summary>参考答案</summary>

**实验设计**：

1. **能力维度选择**：
   - 事实知识：MMLU benchmark
   - 推理能力：GSM8K数学题
   - 代码生成：HumanEval
   - 创造性：故事续写多样性

2. **评估协议**：
   ```python
   # 控制变量
   - 相同的解码参数(temperature=0.7)
   - 相同的prompt格式
   - 多次运行取平均（减少随机性）
   
   # 指标计算
   alignment_tax[task] = max(0, 
       score_pretrain[task] - score_aligned[task])
   ```

3. **缓解策略**：
   - **数据混合**：加入15%预训练数据
   - **分层微调**：冻结底层，只调整顶层
   - **弹性权重巩固(EWC)**：保护重要参数
   - **知识蒸馏**：从预训练模型蒸馏

4. **预期结果**：
   - 事实知识：-3%到-5%
   - 推理能力：-5%到-8%
   - 代码生成：-10%到-15%
   - 创造性：-20%到-30%
</details>

**练习 1.5：分布偏移的在线适应**
在部署后发现模型在处理含有表情符号的输入时性能下降40%。设计一个在线适应方案。

*Hint: 考虑数据收集、标注、训练的完整流程*

<details>
<summary>参考答案</summary>

**在线适应方案**：

1. **问题诊断**：
   ```python
   # 分析失败案例
   failure_rate_by_feature = {
       "has_emoji": 0.40,
       "no_emoji": 0.05,
       "mixed_language": 0.25
   }
   ```

2. **数据收集策略**：
   - 自动记录含表情符号的失败案例
   - 主动采样：生成表情符号变体
   - 用户反馈：收集负面评价的案例

3. **增量训练**：
   ```python
   # 混合策略
   new_data = collect_emoji_cases(n=1000)
   replay_buffer = sample_previous(n=4000)
   combined = new_data + replay_buffer
   
   # 小学习率微调
   lr = original_lr * 0.1
   train_steps = 100  # 避免过拟合
   ```

4. **A/B测试验证**：
   - 10%流量测试新模型
   - 监控关键指标
   - 逐步扩大流量

5. **长期改进**：
   - 更新训练数据分布
   - 调整数据增强策略
   - 考虑专门的表情符号编码器
</details>

**练习 1.6：多目标优化的帕累托前沿**
给定3个目标：有用性(H)、无害性(S)、诚实性(T)。如何找到最优权衡点？

*Hint: 考虑多目标优化算法和实际约束*

<details>
<summary>参考答案</summary>

**解决方案**：

1. **问题形式化**：
   $$\max_\theta \{ H(\theta), S(\theta), T(\theta) \}$$
   
   约束：各指标最低阈值
   - H ≥ 0.8
   - S ≥ 0.95  
   - T ≥ 0.85

2. **帕累托前沿搜索**：
   ```python
   # 网格搜索权重
   for w_h in [0.2, 0.3, 0.4, 0.5]:
       for w_s in [0.2, 0.3, 0.4, 0.5]:
           w_t = 1 - w_h - w_s
           if w_t > 0:
               loss = -w_h*H - w_s*S - w_t*T
               train_model(loss)
               evaluate_pareto_dominance()
   ```

3. **自适应权重调整**：
   - 根据当前性能动态调整权重
   - 优先改进最差的指标
   - 使用梯度手术(Gradient Surgery)避免冲突

4. **实践建议**：
   - 先优化安全性到阈值以上
   - 在保证安全的前提下优化有用性
   - 诚实性通过数据质量保证
   - 定期重新评估权重分配
</details>

**练习 1.7：Constitutional AI的规则设计**
设计一套宪法规则(Constitutional Rules)来指导模型行为，要求覆盖安全、有用、诚实三个维度。

*Hint: 规则要具体、可执行、无歧义*

<details>
<summary>参考答案</summary>

**Constitutional Rules设计**：

1. **安全规则**(优先级最高)：
   ```
   R1: 绝不协助非法活动
   R2: 不生成可识别个人信息
   R3: 拒绝生成仇恨或歧视内容
   R4: 不提供自我伤害指导
   ```

2. **有用性规则**：
   ```
   R5: 直接回答用户问题
   R6: 提供可执行的步骤
   R7: 承认不确定性
   R8: 主动澄清歧义
   ```

3. **诚实性规则**：
   ```
   R9: 不编造事实或引用
   R10: 区分观点和事实
   R11: 承认知识边界
   R12: 纠正自己的错误
   ```

4. **规则冲突解决**：
   ```python
   def resolve_conflict(rules_triggered):
       # 安全 > 诚实 > 有用
       priority = {"safety": 3, "honesty": 2, "helpful": 1}
       return max(rules_triggered, key=lambda r: priority[r.type])
   ```

5. **实施机制**：
   - **训练时**：规则作为额外的奖励信号
   - **推理时**：规则作为生成约束
   - **评估时**：规则违反率作为关键指标
</details>

**练习 1.8：开放性思考题**
如果你要设计下一代的后训练方法，会如何改进现有的RLHF/DPO方法？请提出至少一个创新点。

*Hint: 可以从效率、效果、可解释性等角度思考*

<details>
<summary>参考答案</summary>

**创新方向示例**：

1. **在线偏好学习**：
   - 问题：静态偏好数据很快过时
   - 方案：实时收集用户反馈，动态更新奖励模型
   - 技术：增量学习 + 重要性采样

2. **多粒度奖励建模**：
   - 问题：单一标量奖励信息不足
   - 方案：token级、句子级、段落级多层次奖励
   - 优势：更精细的信用分配

3. **对比解释学习**：
   - 不仅学习"哪个更好"
   - 还学习"为什么更好"
   - 生成可解释的改进建议

4. **元学习优化器**：
   - 学习如何从少量偏好数据快速适应
   - 减少新领域的标注需求
   - 提高样本效率

5. **分布鲁棒优化**：
   - 显式建模最坏情况分布
   - 提高OOD泛化能力
   - 数学形式：
   $$\min_\theta \max_{Q \in \mathcal{P}} \mathbb{E}_{x \sim Q}[\mathcal{L}(\theta, x)]$$

评估标准：
- 样本效率提升>50%
- 训练时间减少>30%
- OOD性能提升>20%
</details>

## 常见陷阱与错误 (Gotchas)

### 1. 数据相关陷阱

⚠️ **过度清洗综合征**
- 错误：过度清洗训练数据，移除所有"不完美"样本
- 后果：模型失去处理真实世界混乱输入的能力
- 正确做法：保留15-20%的"噪声"数据，提高鲁棒性

⚠️ **标注者偏见放大**
- 错误：使用单一来源或同质化的标注团队
- 后果：模型学习并放大特定群体的偏见
- 正确做法：多样化标注者背景，使用标注者disagreement作为信号

### 2. 训练策略陷阱

⚠️ **KL惩罚系数选择**
- 错误：使用固定的β值throughout训练
- 后果：早期限制探索，后期退化严重
- 正确做法：
  ```python
  # 动态调整
  β = β_init * (1 + decay_rate * step)
  # 典型值：β_init=0.01, decay_rate=0.0001
  ```

⚠️ **奖励模型过拟合**
- 错误：奖励模型在同分布数据上过拟合
- 后果：策略模型学会exploit奖励模型的弱点
- 正确做法：
  1. 奖励模型ensemble
  2. 定期更新奖励模型
  3. 添加奖励不确定性估计

### 3. 评估陷阱

⚠️ **评估数据污染**
- 错误：评估集信息泄露到训练集
- 征兆：评估指标异常高，但实际效果差
- 检测方法：
  ```python
  # N-gram重叠检测
  contamination = check_ngram_overlap(train_set, eval_set, n=13)
  if contamination > 0.01:
      raise DataLeakageError
  ```

⚠️ **单一指标优化**
- 错误：只优化BLEU/ROUGE等自动指标
- 后果：Goodhart定律 - "当一个指标变成目标，它就不再是好指标"
- 正确做法：多维度评估矩阵 + 人工评估验证

### 4. 部署陷阱

⚠️ **批处理效应**
- 错误：训练时batch_size=32，推理时batch_size=1
- 后果：BatchNorm统计不匹配，性能下降
- 解决：使用LayerNorm或推理时调整统计量

⚠️ **长度外推失败**
- 错误：训练最大长度512，推理时处理2048
- 后果：位置编码失效，生成质量崩溃
- 解决：
  1. 训练时包含多种长度
  2. 使用相对位置编码
  3. 长度warmup策略

### 调试技巧

💡 **快速诊断检查单**：
```python
def diagnose_training_issues():
    checks = {
        "gradient_norm": check_gradient_explosion(),
        "loss_plateau": check_loss_convergence(),
        "reward_hacking": check_reward_gaming(),
        "distribution_shift": check_kl_divergence(),
        "capability_drop": run_capability_benchmarks()
    }
    return generate_diagnostic_report(checks)
```

💡 **A/B测试最佳实践**：
1. 最小可行改进：一次只改一个变量
2. 统计显著性：至少1000个样本
3. 在线指标vs离线指标对齐
4. 设置自动回滚阈值

---

下一章：[第二章：实验代码基础设施 →](chapter2.md)
# ç¬¬äºŒç« ï¼šå®éªŒä»£ç åŸºç¡€è®¾æ–½

æ„å»ºå¯ç»´æŠ¤ã€å¯æ‰©å±•çš„å®éªŒä»£ç æ¶æ„æ˜¯æˆåŠŸè¿›è¡Œ LLM åè®­ç»ƒçš„åŸºçŸ³ã€‚æœ¬ç« å°†æ·±å…¥æ¢è®¨å¦‚ä½•è®¾è®¡å’Œå®ç°ä¸€ä¸ªå¥å£®çš„å®éªŒåŸºç¡€è®¾æ–½ï¼Œæ¶µç›–é…ç½®ç®¡ç†ã€ç‰ˆæœ¬æ§åˆ¶ã€å®éªŒè¿½è¸ªç­‰å…³é”®ç»„ä»¶ã€‚æˆ‘ä»¬å°†é‡ç‚¹è§£å†³å®é™…å·¥ç¨‹ä¸­çš„æŒ‘æˆ˜ï¼šå¦‚ä½•åœ¨å¿«é€Ÿè¿­ä»£çš„åŒæ—¶ä¿æŒä»£ç è´¨é‡ï¼Œå¦‚ä½•ç®¡ç†æ•°ç™¾ä¸ªå®éªŒçš„é…ç½®å’Œç»“æœï¼Œä»¥åŠå¦‚ä½•é˜²æ­¢æŠ€æœ¯å€ºåŠ¡çš„ç´¯ç§¯ã€‚

## 2.1 å®éªŒé…ç½®ç®¡ç†

### é…ç½®æ–‡ä»¶æ ¼å¼é€‰æ‹©

åœ¨ LLM åè®­ç»ƒé¡¹ç›®ä¸­ï¼Œé…ç½®ç®¡ç†çš„å¤æ‚åº¦è¿œè¶…ä¼ ç»Ÿæ·±åº¦å­¦ä¹ é¡¹ç›®ã€‚ä¸€ä¸ªå…¸å‹çš„å®éªŒå¯èƒ½åŒ…å«ä¸Šç™¾ä¸ªè¶…å‚æ•°ï¼Œæ¶‰åŠæ¨¡å‹æ¶æ„ã€è®­ç»ƒç­–ç•¥ã€æ•°æ®å¤„ç†ã€è¯„ä¼°æŒ‡æ ‡ç­‰å¤šä¸ªç»´åº¦ã€‚é€‰æ‹©åˆé€‚çš„é…ç½®æ ¼å¼è‡³å…³é‡è¦ã€‚

**YAML é…ç½®çš„ä¼˜åŠ¿ä¸åŠ£åŠ¿**

YAML å› å…¶å¯è¯»æ€§å¼ºè€Œå¹¿å—æ¬¢è¿ï¼Œç‰¹åˆ«é€‚åˆåµŒå¥—ç»“æ„çš„è¡¨è¾¾ï¼š

```yaml
model:
  architecture: llama2
  hidden_size: 4096
  num_layers: 32
  attention:
    num_heads: 32
    head_dim: 128
    rotary_embedding:
      base: 10000
      scaling_factor: 1.0
```

ä¼˜åŠ¿ï¼š
- äººç±»å¯è¯»æ€§æœ€ä½³ï¼Œé€‚åˆé…ç½®å®¡æŸ¥
- æ”¯æŒæ³¨é‡Šï¼Œä¾¿äºæ–‡æ¡£åŒ–
- å±‚æ¬¡ç»“æ„æ¸…æ™°ï¼Œé€‚åˆå¤æ‚é…ç½®
- ç”Ÿæ€ç³»ç»Ÿæˆç†Ÿï¼Œå·¥å…·é“¾å®Œå–„

åŠ£åŠ¿ï¼š
- ç¼©è¿›æ•æ„Ÿï¼Œå®¹æ˜“å‡ºé”™
- ç±»å‹æ¨æ–­å¯èƒ½äº§ç”Ÿæ„å¤–ï¼ˆå¦‚ "no" è¢«è§£æä¸ºå¸ƒå°”å€¼ï¼‰
- ä¸æ”¯æŒå˜é‡å¼•ç”¨å’Œè®¡ç®—è¡¨è¾¾å¼
- å¤§æ–‡ä»¶è§£æé€Ÿåº¦è¾ƒæ…¢

**TOML é…ç½®çš„æƒè¡¡**

TOML åœ¨ Rust å’Œ Python ç¤¾åŒºé€æ¸æµè¡Œï¼Œæä¾›äº†æ›´ä¸¥æ ¼çš„è¯­æ³•ï¼š

```toml
[model]
architecture = "llama2"
hidden_size = 4096
num_layers = 32

[model.attention]
num_heads = 32
head_dim = 128

[model.attention.rotary_embedding]
base = 10000
scaling_factor = 1.0
```

ä¼˜åŠ¿ï¼š
- è¯­æ³•æ˜ç¡®ï¼Œæ­§ä¹‰å°‘
- åŸç”Ÿæ”¯æŒæ—¥æœŸæ—¶é—´ç±»å‹
- è¡¨æ ¼æ•°ç»„è¯­æ³•é€‚åˆæ‰¹é‡å®éªŒé…ç½®

åŠ£åŠ¿ï¼š
- æ·±å±‚åµŒå¥—å¯è¯»æ€§ä¸‹é™
- æ•°ç»„å’Œå†…è”è¡¨çš„è¯­æ³•è¾ƒå¤æ‚
- ç”Ÿæ€ç³»ç»Ÿç›¸å¯¹è¾ƒæ–°

**Python é…ç½®çš„çµæ´»æ€§**

ç›´æ¥ä½¿ç”¨ Python æ–‡ä»¶ä½œä¸ºé…ç½®æä¾›äº†æœ€å¤§çš„çµæ´»æ€§ï¼š

```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class ModelConfig:
    architecture: str = "llama2"
    hidden_size: int = 4096
    num_layers: int = 32
    
    @property
    def total_params(self) -> int:
        # åŠ¨æ€è®¡ç®—å‚æ•°é‡
        return self.calculate_params()
    
    def scale_model(self, factor: float):
        """åŠ¨æ€è°ƒæ•´æ¨¡å‹è§„æ¨¡"""
        self.hidden_size = int(self.hidden_size * factor)
        self.num_layers = int(self.num_layers * factor)
```

ä¼˜åŠ¿ï¼š
- æ”¯æŒåŠ¨æ€è®¡ç®—å’Œæ¡ä»¶é€»è¾‘
- ç±»å‹æ£€æŸ¥å’Œ IDE æ”¯æŒå®Œå–„
- å¯ä»¥å¤ç”¨ä»£ç å’Œå¯¼å…¥æ¨¡å—
- æ”¯æŒé…ç½®éªŒè¯å’Œé»˜è®¤å€¼

åŠ£åŠ¿ï¼š
- å®‰å…¨æ€§é£é™©ï¼ˆæ‰§è¡Œä»»æ„ä»£ç ï¼‰
- éæŠ€æœ¯äººå‘˜éš¾ä»¥ä¿®æ”¹
- ç‰ˆæœ¬æ§åˆ¶ä¸­ diff å¯è¯»æ€§è¾ƒå·®

### é…ç½®ç»§æ‰¿ä¸è¦†ç›–æœºåˆ¶

å®è·µä¸­ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦ä¸€ä¸ªåŸºç¡€é…ç½®å’Œå¤šä¸ªå®éªŒå˜ä½“ã€‚è®¾è®¡è‰¯å¥½çš„ç»§æ‰¿æœºåˆ¶å¯ä»¥å¤§å¹…å‡å°‘é…ç½®å†—ä½™ï¼š

```python
class ConfigManager:
    def __init__(self, base_config_path: str):
        self.base_config = self.load_config(base_config_path)
        self.inheritance_chain = [base_config_path]
    
    def inherit_from(self, parent_config_path: str):
        """æ”¯æŒå¤šçº§ç»§æ‰¿"""
        parent_config = self.load_config(parent_config_path)
        self.base_config = self.deep_merge(parent_config, self.base_config)
        self.inheritance_chain.append(parent_config_path)
    
    def override(self, overrides: Dict[str, Any]):
        """æ”¯æŒå‘½ä»¤è¡Œè¦†ç›–"""
        for key_path, value in overrides.items():
            self.set_nested_value(key_path, value)
    
    def deep_merge(self, base: Dict, override: Dict) -> Dict:
        """é€’å½’åˆå¹¶é…ç½®å­—å…¸"""
        result = base.copy()
        for key, value in override.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self.deep_merge(result[key], value)
            else:
                result[key] = value
        return result
```

**é…ç½®è¦†ç›–çš„ä¼˜å…ˆçº§è®¾è®¡**

ä¸€ä¸ªæ¸…æ™°çš„ä¼˜å…ˆçº§ä½“ç³»é¿å…äº†é…ç½®å†²çªï¼š

1. å‘½ä»¤è¡Œå‚æ•°ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
2. ç¯å¢ƒå˜é‡
3. å®éªŒç‰¹å®šé…ç½®æ–‡ä»¶
4. ç”¨æˆ·é…ç½®æ–‡ä»¶
5. é¡¹ç›®é»˜è®¤é…ç½®ï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼‰

```
ä¼˜å…ˆçº§é“¾ï¼š
CLI Args > ENV > experiment.yaml > user.yaml > default.yaml
```

### é…ç½®éªŒè¯ä¸ç±»å‹æ£€æŸ¥

ä½¿ç”¨ Pydantic æˆ– attrs è¿›è¡Œè¿è¡Œæ—¶éªŒè¯å¯ä»¥åŠæ—©å‘ç°é…ç½®é”™è¯¯ï¼š

```python
from pydantic import BaseModel, validator, Field
from typing import Literal

class TrainingConfig(BaseModel):
    learning_rate: float = Field(gt=0, le=1.0)
    batch_size: int = Field(gt=0, multiple_of=8)
    optimizer: Literal["adam", "sgd", "adamw"]
    gradient_accumulation_steps: int = Field(gt=0)
    
    @validator("batch_size")
    def validate_batch_size(cls, v, values):
        if "gradient_accumulation_steps" in values:
            effective_batch = v * values["gradient_accumulation_steps"]
            if effective_batch > 65536:
                raise ValueError(f"Effective batch size {effective_batch} too large")
        return v
    
    @validator("learning_rate")
    def validate_lr_schedule(cls, v, values):
        if values.get("optimizer") == "sgd" and v > 0.1:
            raise ValueError("SGD learning rate typically should be < 0.1")
        return v
```

## 2.2 Flagã€ç¯å¢ƒå˜é‡ä¸ Git åˆ†æ”¯ç­–ç•¥

### Command-line Flags çš„è®¾è®¡åŸåˆ™

å‘½ä»¤è¡Œå‚æ•°æ˜¯å®éªŒé…ç½®çš„ç¬¬ä¸€æ¥è§¦ç‚¹ï¼Œè‰¯å¥½çš„è®¾è®¡èƒ½æ˜¾è‘—æå‡å®éªŒæ•ˆç‡ã€‚ä»¥ä¸‹æ˜¯ç»è¿‡å¤§è§„æ¨¡å®éªŒéªŒè¯çš„è®¾è®¡åŸåˆ™ï¼š

**å±‚æ¬¡åŒ–çš„å‚æ•°ç»„ç»‡**

é¿å…å¹³é“ºæ‰€æœ‰å‚æ•°ï¼Œè€Œæ˜¯æŒ‰åŠŸèƒ½åŸŸç»„ç»‡ï¼š

```python
import argparse

def create_parser():
    parser = argparse.ArgumentParser()
    
    # ä½¿ç”¨å‚æ•°ç»„æé«˜å¯è¯»æ€§
    model_group = parser.add_argument_group("model")
    model_group.add_argument("--model.name", default="llama2-7b")
    model_group.add_argument("--model.checkpoint", type=str)
    model_group.add_argument("--model.dtype", choices=["fp32", "fp16", "bf16"])
    
    training_group = parser.add_argument_group("training")
    training_group.add_argument("--training.batch_size", type=int, default=32)
    training_group.add_argument("--training.learning_rate", type=float, default=1e-4)
    training_group.add_argument("--training.warmup_steps", type=int, default=1000)
    
    data_group = parser.add_argument_group("data")
    data_group.add_argument("--data.train_path", required=True)
    data_group.add_argument("--data.val_path", required=True)
    data_group.add_argument("--data.num_workers", type=int, default=4)
    
    return parser
```

**æ™ºèƒ½é»˜è®¤å€¼ä¸å¿…éœ€å‚æ•°**

åŒºåˆ†å¿…éœ€å‚æ•°å’Œå¯é€‰å‚æ•°ï¼Œä¸ºå¸¸è§åœºæ™¯æä¾›åˆç†é»˜è®¤å€¼ï¼š

```python
class FlagValidator:
    @staticmethod
    def validate_flags(args):
        # è‡ªåŠ¨æ¨æ–­ç›¸å…³å‚æ•°
        if args.distributed and args.local_rank is None:
            args.local_rank = int(os.environ.get("LOCAL_RANK", 0))
        
        # æ ¹æ®ç¡¬ä»¶è‡ªåŠ¨è®¾ç½®
        if args.device == "auto":
            args.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # æ‰¹é‡å¤§å°è‡ªåŠ¨è°ƒæ•´
        if args.gradient_checkpointing and args.batch_size > 16:
            logger.warning(f"Reducing batch size from {args.batch_size} to 16 due to gradient checkpointing")
            args.batch_size = 16
        
        return args
```

**å‚æ•°åˆ«åä¸ç®€å†™**

ä¸ºå¸¸ç”¨å‚æ•°æä¾›ç®€å†™ï¼Œæé«˜å‘½ä»¤è¡Œæ•ˆç‡ï¼š

```python
parser.add_argument("-b", "--batch-size", "--training.batch_size", 
                   dest="batch_size", type=int, default=32,
                   help="Training batch size per device")
                   
parser.add_argument("-lr", "--learning-rate", "--training.lr",
                   dest="learning_rate", type=float, default=1e-4,
                   help="Peak learning rate")
                   
parser.add_argument("-e", "--epochs", "--num-epochs",
                   dest="num_epochs", type=int, default=3,
                   help="Number of training epochs")
```

### ç¯å¢ƒå˜é‡çš„ä½¿ç”¨åœºæ™¯

ç¯å¢ƒå˜é‡é€‚åˆç®¡ç†è·¨å®éªŒçš„å…¨å±€è®¾ç½®å’Œæ•æ„Ÿä¿¡æ¯ï¼š

**åˆ†å±‚çš„ç¯å¢ƒå˜é‡ä½“ç³»**

```bash
# ç³»ç»Ÿçº§åˆ«ï¼ˆé›†ç¾¤é…ç½®ï¼‰
export CUDA_VISIBLE_DEVICES=0,1,2,3
export OMP_NUM_THREADS=8
export NCCL_DEBUG=INFO

# é¡¹ç›®çº§åˆ«ï¼ˆè·¯å¾„å’Œå‡­è¯ï¼‰
export LLM_DATA_ROOT=/mnt/data/llm_datasets
export LLM_CHECKPOINT_DIR=/mnt/checkpoints
export WANDB_API_KEY=your_api_key_here
export HF_TOKEN=your_huggingface_token

# å®éªŒçº§åˆ«ï¼ˆè¿è¡Œæ—¶é…ç½®ï¼‰
export LLM_EXPERIMENT_NAME=dpo_ablation_v3
export LLM_RUN_ID=$(date +%Y%m%d_%H%M%S)
export LLM_DEBUG_MODE=1
```

**ç¯å¢ƒå˜é‡çš„æœ€ä½³å®è·µ**

```python
import os
from pathlib import Path
from typing import Optional

class EnvConfig:
    """ç»Ÿä¸€ç®¡ç†ç¯å¢ƒå˜é‡"""
    
    @staticmethod
    def get_data_root() -> Path:
        """è·å–æ•°æ®æ ¹ç›®å½•ï¼Œæ”¯æŒå¤šçº§fallback"""
        candidates = [
            os.environ.get("LLM_DATA_ROOT"),
            os.environ.get("DATA_ROOT"),
            "/data/llm",
            "./data"
        ]
        for path in candidates:
            if path and Path(path).exists():
                return Path(path)
        raise ValueError("No valid data root found")
    
    @staticmethod
    def get_wandb_config() -> dict:
        """å®‰å…¨åœ°è·å– W&B é…ç½®"""
        config = {}
        if api_key := os.environ.get("WANDB_API_KEY"):
            config["api_key"] = api_key
        if project := os.environ.get("WANDB_PROJECT"):
            config["project"] = project
        if entity := os.environ.get("WANDB_ENTITY"):
            config["entity"] = entity
        return config
    
    @staticmethod
    def is_debug_mode() -> bool:
        """æ£€æŸ¥è°ƒè¯•æ¨¡å¼"""
        return os.environ.get("LLM_DEBUG_MODE", "0").lower() in ("1", "true", "yes")
```

### Git åˆ†æ”¯ç®¡ç†å®è·µ

åœ¨å¿«é€Ÿè¿­ä»£çš„å®éªŒç¯å¢ƒä¸­ï¼ŒGit åˆ†æ”¯ç­–ç•¥éœ€è¦å¹³è¡¡å®éªŒè‡ªç”±åº¦å’Œä»£ç è´¨é‡ï¼š

**å®éªŒåˆ†æ”¯å‘½åè§„èŒƒ**

```bash
# åŠŸèƒ½å¼€å‘åˆ†æ”¯
feature/distributed-dpo
feature/multimodal-alignment

# å®éªŒåˆ†æ”¯ï¼ˆçŸ­æœŸï¼‰
exp/20250105-lr-sweep
exp/20250106-batch-size-ablation

# ä¸ªäººå®éªŒåˆ†æ”¯ï¼ˆæ›´è‡ªç”±ï¼‰
dev/alice/rope-scaling
dev/bob/attention-variants

# é•¿æœŸç ”ç©¶åˆ†æ”¯
research/constitutional-ai
research/online-rlhf
```

**åˆ†æ”¯ä¿æŠ¤ä¸åˆå¹¶ç­–ç•¥**

```python
# .github/branch_protection.yml
protection_rules:
  main:
    required_reviews: 2
    dismiss_stale_reviews: true
    require_code_owner_reviews: true
    required_status_checks:
      - lint
      - type-check
      - unit-tests
    enforce_admins: false
    
  release/*:
    required_reviews: 3
    require_code_owner_reviews: true
    required_status_checks:
      - all-tests
      - integration-tests
      - benchmark-regression
```

**å®éªŒåˆ†æ”¯çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†**

```bash
#!/bin/bash
# scripts/manage_exp_branches.sh

# è‡ªåŠ¨æ¸…ç†è¿‡æœŸçš„å®éªŒåˆ†æ”¯
cleanup_old_exp_branches() {
    local days_old=${1:-30}
    
    git for-each-ref --format='%(refname:short) %(committerdate:unix)' refs/heads/exp/ | \
    while read branch timestamp; do
        age_days=$(( ($(date +%s) - timestamp) / 86400 ))
        if [ $age_days -gt $days_old ]; then
            echo "Deleting old experimental branch: $branch (${age_days} days old)"
            git branch -D "$branch"
        fi
    done
}

# å½’æ¡£é‡è¦å®éªŒåˆ†æ”¯
archive_exp_branch() {
    local branch=$1
    local archive_tag="archive/$(date +%Y%m%d)/${branch##*/}"
    
    git tag -a "$archive_tag" "$branch" -m "Archived experimental branch $branch"
    git push origin "$archive_tag"
    git branch -d "$branch"
    echo "Archived $branch as $archive_tag"
}
```

## 2.3 å®éªŒè¿½è¸ªä¸ç‰ˆæœ¬æ§åˆ¶

### å®éªŒè¿½è¸ªå·¥å…·é€‰æ‹©

é€‰æ‹©åˆé€‚çš„å®éªŒè¿½è¸ªå·¥å…·æ˜¯å»ºç«‹å¯é‡ç°ç ”ç©¶æµç¨‹çš„å…³é”®ã€‚ä¸»æµå·¥å…·å„æœ‰ç‰¹è‰²ï¼Œéœ€è¦æ ¹æ®å›¢é˜Ÿè§„æ¨¡å’Œéœ€æ±‚é€‰æ‹©ã€‚

**MLflowï¼šå¼€æºæ ‡å‡†çš„é€‰æ‹©**

MLflow æä¾›äº†å®Œæ•´çš„å®éªŒç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼š

```python
import mlflow
from mlflow.tracking import MlflowClient
import hashlib
import json

class MLflowExperimentTracker:
    def __init__(self, experiment_name: str, tracking_uri: str = "file:./mlruns"):
        mlflow.set_tracking_uri(tracking_uri)
        mlflow.set_experiment(experiment_name)
        self.client = MlflowClient()
        
    def start_run(self, config: dict, tags: dict = None):
        """å¼€å§‹ä¸€ä¸ªæ–°çš„å®éªŒè¿è¡Œ"""
        # ç”Ÿæˆé…ç½®å“ˆå¸Œä½œä¸ºè¿è¡Œæ ‡è¯†
        config_hash = hashlib.md5(
            json.dumps(config, sort_keys=True).encode()
        ).hexdigest()[:8]
        
        run_name = f"{config.get('model_name', 'unknown')}_{config_hash}"
        
        with mlflow.start_run(run_name=run_name) as run:
            # è®°å½•é…ç½®å‚æ•°
            mlflow.log_params(self.flatten_dict(config))
            
            # è®°å½•æ ‡ç­¾
            if tags:
                mlflow.set_tags(tags)
            
            # è®°å½•ä»£ç ç‰ˆæœ¬
            mlflow.set_tag("git_commit", self.get_git_commit())
            mlflow.set_tag("git_branch", self.get_git_branch())
            
            return run.info.run_id
    
    def log_metrics_batch(self, metrics: dict, step: int):
        """æ‰¹é‡è®°å½•æŒ‡æ ‡"""
        for key, value in metrics.items():
            mlflow.log_metric(key, value, step=step)
    
    def log_artifact_with_metadata(self, file_path: str, metadata: dict):
        """è®°å½•æ–‡ä»¶åŠå…¶å…ƒæ•°æ®"""
        mlflow.log_artifact(file_path)
        # åŒæ—¶è®°å½•å…ƒæ•°æ®
        metadata_path = f"{file_path}.metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        mlflow.log_artifact(metadata_path)
```

**Weights & Biasesï¼šäº‘åŸç”Ÿçš„å¼ºå¤§åŠŸèƒ½**

W&B æä¾›äº†æ›´ä¸°å¯Œçš„å¯è§†åŒ–å’Œåä½œåŠŸèƒ½ï¼š

```python
import wandb
from typing import Any, Dict, Optional
import numpy as np

class WandBTracker:
    def __init__(self, project: str, entity: Optional[str] = None):
        self.project = project
        self.entity = entity
        
    def init_run(self, config: dict, name: Optional[str] = None, 
                 resume: Optional[str] = None):
        """åˆå§‹åŒ– W&B è¿è¡Œ"""
        run = wandb.init(
            project=self.project,
            entity=self.entity,
            config=config,
            name=name,
            resume=resume,  # æ”¯æŒæ–­ç‚¹ç»­è®­
            save_code=True,  # è‡ªåŠ¨ä¿å­˜ä»£ç 
            tags=self.generate_tags(config)
        )
        
        # å®šä¹‰è‡ªå®šä¹‰æŒ‡æ ‡
        wandb.define_metric("train/step")
        wandb.define_metric("train/*", step_metric="train/step")
        wandb.define_metric("eval/step")
        wandb.define_metric("eval/*", step_metric="eval/step")
        
        return run
    
    def log_distribution(self, name: str, data: np.ndarray, step: int):
        """è®°å½•æ•°æ®åˆ†å¸ƒ"""
        wandb.log({
            f"{name}/mean": np.mean(data),
            f"{name}/std": np.std(data),
            f"{name}/min": np.min(data),
            f"{name}/max": np.max(data),
            f"{name}/histogram": wandb.Histogram(data)
        }, step=step)
    
    def log_gradient_flow(self, model, step: int):
        """è®°å½•æ¢¯åº¦æµä¿¡æ¯"""
        gradients = []
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.data.norm(2).item()
                gradients.append({
                    "name": name,
                    "grad_norm": grad_norm
                })
        
        # åˆ›å»ºæ¢¯åº¦è¡¨æ ¼
        grad_table = wandb.Table(
            columns=["layer", "gradient_norm"],
            data=[[g["name"], g["grad_norm"]] for g in gradients]
        )
        wandb.log({"gradients": grad_table}, step=step)
```

**TensorBoardï¼šè½»é‡çº§æœ¬åœ°æ–¹æ¡ˆ**

å¯¹äºä¸éœ€è¦äº‘æœåŠ¡çš„åœºæ™¯ï¼ŒTensorBoard ä»æ˜¯å¯é é€‰æ‹©ï¼š

```python
from torch.utils.tensorboard import SummaryWriter
import torch
from pathlib import Path

class TensorBoardTracker:
    def __init__(self, log_dir: str, comment: str = ""):
        self.log_dir = Path(log_dir)
        self.writer = SummaryWriter(
            log_dir=str(self.log_dir),
            comment=comment,
            flush_secs=30  # å®šæœŸåˆ·æ–°åˆ°ç£ç›˜
        )
        
    def log_model_architecture(self, model: torch.nn.Module, input_shape: tuple):
        """è®°å½•æ¨¡å‹æ¶æ„"""
        dummy_input = torch.randn(input_shape)
        self.writer.add_graph(model, dummy_input)
    
    def log_attention_weights(self, attention_weights: torch.Tensor, 
                            step: int, head_idx: int = 0):
        """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
        # é€‰æ‹©ç‰¹å®šçš„æ³¨æ„åŠ›å¤´
        attn = attention_weights[0, head_idx].cpu().numpy()
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        fig = plt.figure(figsize=(10, 8))
        plt.imshow(attn, cmap='hot', interpolation='nearest')
        plt.colorbar()
        plt.title(f'Attention Weights - Head {head_idx}')
        
        self.writer.add_figure(f'attention/head_{head_idx}', fig, step)
        plt.close()
    
    def log_learning_rate_schedule(self, optimizer, step: int):
        """è®°å½•å­¦ä¹ ç‡å˜åŒ–"""
        for i, param_group in enumerate(optimizer.param_groups):
            lr = param_group['lr']
            self.writer.add_scalar(f'learning_rate/group_{i}', lr, step)
```

### å®éªŒå…ƒæ•°æ®ç®¡ç†

å®Œæ•´çš„å…ƒæ•°æ®è®°å½•æ˜¯å®éªŒå¯é‡ç°æ€§çš„åŸºç¡€ï¼š

```python
import platform
import subprocess
import datetime
import psutil
import GPUtil

class ExperimentMetadata:
    @staticmethod
    def collect_system_info() -> dict:
        """æ”¶é›†ç³»ç»Ÿä¿¡æ¯"""
        return {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "cpu_count": psutil.cpu_count(),
            "memory_gb": psutil.virtual_memory().total / (1024**3),
            "hostname": platform.node(),
            "user": os.environ.get("USER", "unknown")
        }
    
    @staticmethod
    def collect_gpu_info() -> list:
        """æ”¶é›† GPU ä¿¡æ¯"""
        gpus = GPUtil.getGPUs()
        return [{
            "id": gpu.id,
            "name": gpu.name,
            "memory_total": gpu.memoryTotal,
            "driver": gpu.driver,
            "compute_capability": f"{gpu.major}.{gpu.minor}"
        } for gpu in gpus]
    
    @staticmethod
    def collect_dependencies() -> dict:
        """æ”¶é›†ä¾èµ–ç‰ˆæœ¬"""
        deps = {}
        try:
            import torch
            deps["torch"] = torch.__version__
            deps["cuda"] = torch.version.cuda if torch.cuda.is_available() else None
        except ImportError:
            pass
            
        try:
            import transformers
            deps["transformers"] = transformers.__version__
        except ImportError:
            pass
            
        # ä» requirements.txt æˆ– pyproject.toml è¯»å–
        if Path("requirements.txt").exists():
            with open("requirements.txt") as f:
                for line in f:
                    if "==" in line:
                        pkg, version = line.strip().split("==")
                        deps[pkg] = version
                        
        return deps
    
    @staticmethod
    def create_experiment_card(config: dict) -> dict:
        """åˆ›å»ºå®éªŒå¡ç‰‡"""
        return {
            "experiment_id": str(uuid.uuid4()),
            "timestamp": datetime.datetime.now().isoformat(),
            "config": config,
            "system": ExperimentMetadata.collect_system_info(),
            "gpus": ExperimentMetadata.collect_gpu_info(),
            "dependencies": ExperimentMetadata.collect_dependencies(),
            "git": {
                "commit": subprocess.check_output(
                    ["git", "rev-parse", "HEAD"]
                ).decode().strip(),
                "branch": subprocess.check_output(
                    ["git", "branch", "--show-current"]
                ).decode().strip(),
                "diff": subprocess.check_output(
                    ["git", "diff", "HEAD"]
                ).decode()
            }
        }
```

### æ¨¡å‹æ£€æŸ¥ç‚¹ç­–ç•¥

é«˜æ•ˆçš„æ£€æŸ¥ç‚¹ç®¡ç†å¯¹äºé•¿æ—¶é—´è®­ç»ƒè‡³å…³é‡è¦ï¼š

```python
class CheckpointManager:
    def __init__(self, checkpoint_dir: Path, max_checkpoints: int = 5):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.max_checkpoints = max_checkpoints
        
    def save_checkpoint(self, model, optimizer, epoch: int, 
                       metrics: dict, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            "epoch": epoch,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "metrics": metrics,
            "timestamp": datetime.datetime.now().isoformat()
        }
        
        # å¸¸è§„æ£€æŸ¥ç‚¹
        checkpoint_path = self.checkpoint_dir / f"checkpoint_epoch_{epoch}.pt"
        torch.save(checkpoint, checkpoint_path)
        
        # æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = self.checkpoint_dir / "best_model.pt"
            torch.save(checkpoint, best_path)
            
        # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
        self._cleanup_old_checkpoints()
        
    def _cleanup_old_checkpoints(self):
        """ä¿ç•™æœ€æ–°çš„Nä¸ªæ£€æŸ¥ç‚¹"""
        checkpoints = sorted(
            self.checkpoint_dir.glob("checkpoint_epoch_*.pt"),
            key=lambda x: x.stat().st_mtime
        )
        
        if len(checkpoints) > self.max_checkpoints:
            for ckpt in checkpoints[:-self.max_checkpoints]:
                ckpt.unlink()
                
    def resume_from_checkpoint(self, checkpoint_path: Path, 
                              model, optimizer) -> dict:
        """ä»æ£€æŸ¥ç‚¹æ¢å¤"""
        checkpoint = torch.load(checkpoint_path, map_location="cpu")
        
        model.load_state_dict(checkpoint["model_state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        
        return {
            "epoch": checkpoint["epoch"],
            "metrics": checkpoint["metrics"]
        }
```

## 2.4 é˜²æ­¢ä»£ç è…åŒ–çš„æœ€ä½³å®è·µ

### æŠ€æœ¯å€ºåŠ¡ç®¡ç†

LLM åè®­ç»ƒé¡¹ç›®çš„å¿«é€Ÿè¿­ä»£å®¹æ˜“ç´¯ç§¯æŠ€æœ¯å€ºåŠ¡ã€‚ä¸»åŠ¨ç®¡ç†æŠ€æœ¯å€ºåŠ¡æ˜¯ä¿æŒé¡¹ç›®é•¿æœŸå¥åº·çš„å…³é”®ã€‚

**æŠ€æœ¯å€ºåŠ¡çš„é‡åŒ–ä¸è¿½è¸ª**

```python
from typing import List, Dict
import ast
import re

class TechnicalDebtAnalyzer:
    def __init__(self, codebase_path: Path):
        self.codebase_path = codebase_path
        self.debt_markers = ["TODO", "FIXME", "HACK", "XXX", "DEPRECATED"]
        
    def scan_codebase(self) -> Dict[str, List[Dict]]:
        """æ‰«æä»£ç åº“ä¸­çš„æŠ€æœ¯å€ºåŠ¡æ ‡è®°"""
        debt_items = {marker: [] for marker in self.debt_markers}
        
        for py_file in self.codebase_path.rglob("*.py"):
            with open(py_file, 'r') as f:
                for line_num, line in enumerate(f, 1):
                    for marker in self.debt_markers:
                        if marker in line:
                            debt_items[marker].append({
                                "file": str(py_file.relative_to(self.codebase_path)),
                                "line": line_num,
                                "content": line.strip(),
                                "priority": self.estimate_priority(line)
                            })
        
        return debt_items
    
    def calculate_complexity_metrics(self, file_path: Path) -> dict:
        """è®¡ç®—ä»£ç å¤æ‚åº¦æŒ‡æ ‡"""
        with open(file_path, 'r') as f:
            source = f.read()
            
        tree = ast.parse(source)
        
        metrics = {
            "cyclomatic_complexity": self.calculate_cyclomatic_complexity(tree),
            "lines_of_code": len(source.splitlines()),
            "num_functions": len([n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]),
            "num_classes": len([n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]),
            "max_nesting_depth": self.calculate_max_nesting(tree)
        }
        
        return metrics
    
    def generate_debt_report(self) -> str:
        """ç”ŸæˆæŠ€æœ¯å€ºåŠ¡æŠ¥å‘Š"""
        debt_items = self.scan_codebase()
        total_debt = sum(len(items) for items in debt_items.values())
        
        report = f"""
# æŠ€æœ¯å€ºåŠ¡æŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().isoformat()}
æ€»å€ºåŠ¡é¡¹: {total_debt}

## æŒ‰ç±»å‹åˆ†å¸ƒ
"""
        for marker, items in debt_items.items():
            report += f"- {marker}: {len(items)} é¡¹\n"
            
        # é«˜ä¼˜å…ˆçº§é¡¹ç›®
        high_priority = []
        for marker, items in debt_items.items():
            high_priority.extend([
                item for item in items 
                if item["priority"] == "high"
            ])
        
        if high_priority:
            report += "\n## é«˜ä¼˜å…ˆçº§å€ºåŠ¡\n"
            for item in high_priority[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                report += f"- {item['file']}:{item['line']} - {item['content']}\n"
                
        return report
```

**ä»£ç è´¨é‡é—¨ç¦**

```python
class CodeQualityGate:
    def __init__(self, thresholds: dict):
        self.thresholds = thresholds
        
    def check_diff_quality(self, diff_file: str) -> bool:
        """æ£€æŸ¥ä»£ç å˜æ›´çš„è´¨é‡"""
        checks = {
            "no_print_statements": self.check_no_prints(diff_file),
            "has_tests": self.check_has_tests(diff_file),
            "docstring_coverage": self.check_docstrings(diff_file),
            "type_hints": self.check_type_hints(diff_file)
        }
        
        failures = [name for name, passed in checks.items() if not passed]
        
        if failures:
            print(f"Quality gate failed: {', '.join(failures)}")
            return False
            
        return True
    
    def check_no_prints(self, diff: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«è°ƒè¯•ç”¨çš„printè¯­å¥"""
        pattern = r'\+.*print\('
        return not re.search(pattern, diff)
    
    def check_has_tests(self, diff: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å¯¹åº”çš„æµ‹è¯•"""
        # å¦‚æœä¿®æ”¹äº†src/ä¸‹çš„æ–‡ä»¶ï¼Œåº”è¯¥æœ‰å¯¹åº”çš„test/ä¸‹çš„ä¿®æ”¹
        src_modified = "src/" in diff
        test_modified = "test/" in diff or "tests/" in diff
        
        if src_modified and not test_modified:
            return False
        return True
```

### ä»£ç å¤ç”¨ä¸æ¨¡å—åŒ–

è‰¯å¥½çš„æ¨¡å—åŒ–è®¾è®¡æ˜¯é˜²æ­¢ä»£ç è…åŒ–çš„åŸºç¡€ï¼š

```python
from abc import ABC, abstractmethod
from typing import Generic, TypeVar

T = TypeVar('T')

class BaseExperiment(ABC, Generic[T]):
    """å®éªŒåŸºç±»ï¼Œå¼ºåˆ¶è§„èŒƒåŒ–å®éªŒæµç¨‹"""
    
    def __init__(self, config: dict):
        self.config = config
        self.setup()
        
    @abstractmethod
    def setup(self):
        """åˆå§‹åŒ–å®éªŒç¯å¢ƒ"""
        pass
    
    @abstractmethod
    def prepare_data(self) -> T:
        """æ•°æ®å‡†å¤‡"""
        pass
    
    @abstractmethod
    def build_model(self):
        """æ„å»ºæ¨¡å‹"""
        pass
    
    @abstractmethod
    def train_step(self, batch: T) -> dict:
        """å•æ­¥è®­ç»ƒ"""
        pass
    
    @abstractmethod
    def evaluate(self) -> dict:
        """è¯„ä¼°"""
        pass
    
    def run(self):
        """æ ‡å‡†åŒ–çš„å®éªŒæµç¨‹"""
        data = self.prepare_data()
        model = self.build_model()
        
        for epoch in range(self.config["num_epochs"]):
            for batch in data:
                metrics = self.train_step(batch)
                self.log_metrics(metrics)
                
            eval_metrics = self.evaluate()
            self.log_eval_metrics(eval_metrics)
```

**ç»„ä»¶æ³¨å†Œæœºåˆ¶**

```python
class ComponentRegistry:
    """ç»Ÿä¸€çš„ç»„ä»¶æ³¨å†Œæœºåˆ¶ï¼Œé¿å…ä»£ç åˆ†æ•£"""
    
    _registry = {
        "models": {},
        "datasets": {},
        "trainers": {},
        "evaluators": {}
    }
    
    @classmethod
    def register(cls, category: str, name: str):
        """è£…é¥°å™¨ï¼šæ³¨å†Œç»„ä»¶"""
        def decorator(component_cls):
            if category not in cls._registry:
                raise ValueError(f"Unknown category: {category}")
                
            cls._registry[category][name] = component_cls
            return component_cls
        return decorator
    
    @classmethod
    def get(cls, category: str, name: str):
        """è·å–æ³¨å†Œçš„ç»„ä»¶"""
        if category not in cls._registry:
            raise ValueError(f"Unknown category: {category}")
            
        if name not in cls._registry[category]:
            available = list(cls._registry[category].keys())
            raise ValueError(f"Unknown {category}: {name}. Available: {available}")
            
        return cls._registry[category][name]

# ä½¿ç”¨ç¤ºä¾‹
@ComponentRegistry.register("models", "llama2")
class LLaMA2Model:
    pass

@ComponentRegistry.register("datasets", "alpaca")
class AlpacaDataset:
    pass
```

### æŒç»­é›†æˆä¸æµ‹è¯•

**åˆ†å±‚æµ‹è¯•ç­–ç•¥**

```python
import pytest
from unittest.mock import Mock, patch

class TestStrategy:
    """åˆ†å±‚æµ‹è¯•ç­–ç•¥"""
    
    @staticmethod
    def unit_test_example():
        """å•å…ƒæµ‹è¯•ï¼šæµ‹è¯•ç‹¬ç«‹å‡½æ•°"""
        def test_config_merge():
            base = {"a": 1, "b": {"c": 2}}
            override = {"b": {"c": 3, "d": 4}}
            result = deep_merge(base, override)
            assert result == {"a": 1, "b": {"c": 3, "d": 4}}
    
    @staticmethod
    def integration_test_example():
        """é›†æˆæµ‹è¯•ï¼šæµ‹è¯•ç»„ä»¶äº¤äº’"""
        def test_model_with_dataloader():
            model = create_model(config)
            dataloader = create_dataloader(config)
            
            batch = next(iter(dataloader))
            output = model(batch)
            
            assert output.shape == expected_shape
    
    @staticmethod
    def smoke_test_example():
        """å†’çƒŸæµ‹è¯•ï¼šå¿«é€ŸéªŒè¯åŸºæœ¬åŠŸèƒ½"""
        def test_training_loop_runs():
            config = get_minimal_config()
            trainer = Trainer(config)
            
            # åªè¿è¡Œå‡ æ­¥
            trainer.train(max_steps=10)
            
            assert trainer.global_step == 10
```

**CI/CD é…ç½®**

```yaml
# .github/workflows/ci.yml
name: CI Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  quality-checks:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'
      
      - name: Install dependencies
        run: |
          pip install -r requirements-dev.txt
      
      - name: Run linting
        run: |
          flake8 src/ --max-line-length=100
          black --check src/
          isort --check-only src/
      
      - name: Type checking
        run: |
          mypy src/ --ignore-missing-imports
      
      - name: Run tests
        run: |
          pytest tests/ -v --cov=src --cov-report=xml
      
      - name: Check code complexity
        run: |
          radon cc src/ -s -nb
```

### æ–‡æ¡£ä¸çŸ¥è¯†ä¼ æ‰¿

**è‡ªåŠ¨åŒ–æ–‡æ¡£ç”Ÿæˆ**

```python
class DocumentationGenerator:
    """è‡ªåŠ¨ç”Ÿæˆå®éªŒæ–‡æ¡£"""
    
    def generate_experiment_doc(self, experiment_class):
        """ä»å®éªŒç±»ç”Ÿæˆæ–‡æ¡£"""
        doc = f"# {experiment_class.__name__}\n\n"
        
        # æå–ç±»æ–‡æ¡£å­—ç¬¦ä¸²
        if experiment_class.__doc__:
            doc += f"{experiment_class.__doc__}\n\n"
        
        # æå–é…ç½®å‚æ•°
        doc += "## Configuration Parameters\n\n"
        config_schema = experiment_class.get_config_schema()
        for param, schema in config_schema.items():
            doc += f"- **{param}**: {schema['type']} "
            if 'default' in schema:
                doc += f"(default: {schema['default']})"
            doc += f"\n  {schema.get('description', '')}\n"
        
        # æå–æ–¹æ³•æ–‡æ¡£
        doc += "\n## Methods\n\n"
        for method_name in dir(experiment_class):
            if not method_name.startswith('_'):
                method = getattr(experiment_class, method_name)
                if callable(method) and method.__doc__:
                    doc += f"### {method_name}\n"
                    doc += f"{method.__doc__}\n\n"
        
        return doc
```

## æœ¬ç« å°ç»“

æœ¬ç« æ·±å…¥æ¢è®¨äº† LLM åè®­ç»ƒå®éªŒä»£ç åŸºç¡€è®¾æ–½çš„æ„å»ºã€‚æˆ‘ä»¬å­¦ä¹ äº†ï¼š

ğŸ“Œ **é…ç½®ç®¡ç†çš„å±‚æ¬¡åŒ–è®¾è®¡**ï¼šé€šè¿‡ YAML/TOML/Python é…ç½®æ–‡ä»¶çš„åˆç†é€‰æ‹©ï¼Œé…ç½®ç»§æ‰¿æœºåˆ¶ï¼Œä»¥åŠè¿è¡Œæ—¶éªŒè¯ï¼Œå»ºç«‹äº†çµæ´»ä¸”å¥å£®çš„é…ç½®ä½“ç³»ã€‚è®°ä½ï¼šé…ç½®çš„å¤æ‚åº¦åº”è¯¥ä¸å®éªŒçš„å¤æ‚åº¦ç›¸åŒ¹é…ï¼Œè¿‡åº¦è®¾è®¡å’Œè®¾è®¡ä¸è¶³éƒ½ä¼šé™ä½æ•ˆç‡ã€‚

ğŸ“Œ **å®éªŒç¯å¢ƒçš„å¤šç»´ç®¡ç†**ï¼šé€šè¿‡å‘½ä»¤è¡Œå‚æ•°ã€ç¯å¢ƒå˜é‡å’Œ Git åˆ†æ”¯çš„ååŒä½¿ç”¨ï¼Œå®ç°äº†å®éªŒçš„éš”ç¦»æ€§å’Œå¯é‡ç°æ€§ã€‚å…³é”®åŸåˆ™æ˜¯ï¼šFlag ç”¨äºå®éªŒç‰¹å®šé…ç½®ï¼Œç¯å¢ƒå˜é‡ç”¨äºç³»ç»Ÿçº§è®¾ç½®ï¼ŒGit åˆ†æ”¯ç”¨äºä»£ç ç‰ˆæœ¬ç®¡ç†ã€‚

ğŸ“Œ **å®éªŒè¿½è¸ªçš„å…¨ç”Ÿå‘½å‘¨æœŸè¦†ç›–**ï¼šä» MLflowã€W&B åˆ° TensorBoardï¼Œä¸åŒå·¥å…·é€‚åˆä¸åŒåœºæ™¯ã€‚æ ¸å¿ƒæ˜¯è¦è®°å½•è¶³å¤Ÿçš„å…ƒæ•°æ®ä»¥æ”¯æŒå®éªŒé‡ç°ï¼ŒåŒ…æ‹¬ä»£ç ç‰ˆæœ¬ã€ä¾èµ–ç¯å¢ƒã€ç¡¬ä»¶é…ç½®ç­‰ã€‚

ğŸ“Œ **æŠ€æœ¯å€ºåŠ¡çš„ä¸»åŠ¨ç®¡ç†**ï¼šé€šè¿‡ä»£ç è´¨é‡é—¨ç¦ã€æ¨¡å—åŒ–è®¾è®¡ã€è‡ªåŠ¨åŒ–æµ‹è¯•å’Œæ–‡æ¡£ç”Ÿæˆï¼Œå»ºç«‹äº†é˜²æ­¢ä»£ç è…åŒ–çš„å¤šé‡é˜²çº¿ã€‚è®°ä½ï¼šæŠ€æœ¯å€ºåŠ¡æ˜¯ä¸å¯é¿å…çš„ï¼Œå…³é”®æ˜¯è¦å¯è§ã€å¯æ§ã€å¯å¿è¿˜ã€‚

### å…³é”®å…¬å¼ä¸åº¦é‡

1. **æŠ€æœ¯å€ºåŠ¡åˆ©æ¯** = $\sum_{i=1}^{n} \text{complexity}_i \times \text{change_frequency}_i$

2. **å®éªŒå¯é‡ç°æ€§å¾—åˆ†** = $\frac{\text{æˆåŠŸé‡ç°çš„å®éªŒæ•°}}{\text{æ€»å®éªŒæ•°}} \times \text{å…ƒæ•°æ®å®Œæ•´åº¦}$

3. **é…ç½®å¤æ‚åº¦** = $\log_2(\text{é…ç½®å‚æ•°æ•°}) \times \text{åµŒå¥—æ·±åº¦}$

## å¸¸è§é™·é˜±ä¸é”™è¯¯ (Gotchas)

âš ï¸ **é…ç½®åœ°ç‹±ï¼ˆConfiguration Hellï¼‰**
- é”™è¯¯ï¼šä¸ºæ¯ä¸ªå°å®éªŒåˆ›å»ºå®Œå…¨ç‹¬ç«‹çš„é…ç½®æ–‡ä»¶
- åæœï¼šé…ç½®æ–‡ä»¶çˆ†ç‚¸å¼å¢é•¿ï¼Œéš¾ä»¥ç»´æŠ¤
- è§£å†³ï¼šä½¿ç”¨é…ç½®ç»§æ‰¿ï¼Œåªè®°å½•ä¸åŸºçº¿çš„å·®å¼‚

âš ï¸ **å®éªŒè¿½è¸ªè¿‡åº¦æˆ–ä¸è¶³**
- é”™è¯¯ï¼šè®°å½•æ‰€æœ‰å¯èƒ½çš„æŒ‡æ ‡ vs åªè®°å½•æœ€ç»ˆç»“æœ
- åæœï¼šå­˜å‚¨çˆ†ç‚¸æˆ–ä¿¡æ¯ä¸è¶³æ— æ³•è°ƒè¯•
- è§£å†³ï¼šåˆ†å±‚è®°å½•ç­–ç•¥ï¼Œå…³é”®æŒ‡æ ‡è¯¦ç»†è®°å½•ï¼Œè¾…åŠ©æŒ‡æ ‡é‡‡æ ·è®°å½•

âš ï¸ **Git åˆ†æ”¯ç®¡ç†æ··ä¹±**
- é”™è¯¯ï¼šæ‰€æœ‰å®éªŒéƒ½åœ¨ main åˆ†æ”¯è¿›è¡Œ
- åæœï¼šä»£ç å†å²æ··ä¹±ï¼Œéš¾ä»¥å›æº¯
- è§£å†³ï¼šä¸¥æ ¼çš„åˆ†æ”¯å‘½åè§„èŒƒå’Œç”Ÿå‘½å‘¨æœŸç®¡ç†

âš ï¸ **ç¡¬ç¼–ç è·¯å¾„å’Œé…ç½®**
- é”™è¯¯ï¼šåœ¨ä»£ç ä¸­ç¡¬ç¼–ç æ•°æ®è·¯å¾„ã€æ¨¡å‹è·¯å¾„
- åæœï¼šä»£ç æ— æ³•è·¨ç¯å¢ƒè¿è¡Œ
- è§£å†³ï¼šæ‰€æœ‰è·¯å¾„é€šè¿‡é…ç½®æˆ–ç¯å¢ƒå˜é‡ç®¡ç†

âš ï¸ **å¿½è§†ä»£ç å¤æ‚åº¦å¢é•¿**
- é”™è¯¯ï¼šä¸ºäº†å¿«é€Ÿå®éªŒä¸æ–­æ·»åŠ  if-else åˆ†æ”¯
- åæœï¼šä»£ç å˜æˆæ„å¤§åˆ©é¢æ¡ï¼Œæ— æ³•ç»´æŠ¤
- è§£å†³ï¼šå®šæœŸé‡æ„ï¼Œä½¿ç”¨ç­–ç•¥æ¨¡å¼æˆ–æ³¨å†Œæœºåˆ¶

âš ï¸ **æ£€æŸ¥ç‚¹ç®¡ç†ä¸å½“**
- é”™è¯¯ï¼šä¿å­˜æ‰€æœ‰æ£€æŸ¥ç‚¹æˆ–åªä¿å­˜æœ€åä¸€ä¸ª
- åæœï¼šç£ç›˜ç©ºé—´è€—å°½æˆ–æ— æ³•æ¢å¤æœ€ä½³æ¨¡å‹
- è§£å†³ï¼šæ»šåŠ¨çª—å£ç­–ç•¥ + æœ€ä½³æ¨¡å‹ä¿å­˜

ğŸ’¡ **å®ç”¨æŠ€å·§**

1. **é…ç½®éªŒè¯å‰ç½®**ï¼šåœ¨å®éªŒå¼€å§‹å‰éªŒè¯æ‰€æœ‰é…ç½®ï¼Œfail fast
2. **å®éªŒå‘½åè§„èŒƒ**ï¼š`{date}_{model}_{dataset}_{key_hyperparam}`
3. **è‡ªåŠ¨åŒ–æ¸…ç†**ï¼šå®šæœŸæ¸…ç†è¿‡æœŸçš„å®éªŒåˆ†æ”¯å’Œæ£€æŸ¥ç‚¹
4. **å¢é‡å¼æ—¥å¿—**ï¼šä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—ï¼Œä¾¿äºåç»­åˆ†æ
5. **é…ç½®å¿«ç…§**ï¼šæ¯æ¬¡å®éªŒå¼€å§‹æ—¶ä¿å­˜å®Œæ•´é…ç½®å¿«ç…§

## ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜

**ç»ƒä¹  2.1ï¼šé…ç½®æ–‡ä»¶æ ¼å¼é€‰æ‹©**
ä½ çš„å›¢é˜Ÿæ­£åœ¨å¯åŠ¨ä¸€ä¸ªæ–°çš„ LLM åè®­ç»ƒé¡¹ç›®ã€‚é¡¹ç›®éœ€è¦æ”¯æŒï¼š(1) éæŠ€æœ¯äººå‘˜è°ƒæ•´è¶…å‚æ•°ï¼›(2) å¤æ‚çš„åµŒå¥—é…ç½®ï¼›(3) åŠ¨æ€è®¡ç®—æŸäº›å‚æ•°ã€‚è¯·ä¸ºè¿™ä¸ªé¡¹ç›®é€‰æ‹©é…ç½®æ–‡ä»¶æ ¼å¼ï¼Œå¹¶è¯´æ˜ç†ç”±ã€‚

*Hint: è€ƒè™‘æ··åˆæ–¹æ¡ˆï¼Œä¸åŒå±‚æ¬¡ä½¿ç”¨ä¸åŒæ ¼å¼ã€‚*

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

å»ºè®®é‡‡ç”¨æ··åˆé…ç½®æ–¹æ¡ˆï¼š
- **åŸºç¡€é…ç½®å±‚ï¼ˆYAMLï¼‰**ï¼šç”¨äºéæŠ€æœ¯äººå‘˜å¯è°ƒæ•´çš„å‚æ•°ï¼Œå¦‚å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ç­‰
- **é«˜çº§é…ç½®å±‚ï¼ˆPythonï¼‰**ï¼šç”¨äºéœ€è¦åŠ¨æ€è®¡ç®—çš„å‚æ•°ï¼Œå¦‚æ ¹æ® GPU å†…å­˜è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°
- **ç”¨æˆ·è¦†ç›–å±‚ï¼ˆTOMLï¼‰**ï¼šç”¨äºç”¨æˆ·ç‰¹å®šçš„ç¯å¢ƒé…ç½®

å®ç°æ–¹å¼ï¼š
1. å…ˆåŠ è½½ YAML åŸºç¡€é…ç½®
2. é€šè¿‡ Python é…ç½®ç±»è¿›è¡ŒåŠ¨æ€è®¡ç®—å’ŒéªŒè¯
3. æœ€ååº”ç”¨ TOML ç”¨æˆ·è¦†ç›–

è¿™æ ·æ—¢ä¿è¯äº†æ˜“ç”¨æ€§ï¼Œåˆæä¾›äº†è¶³å¤Ÿçš„çµæ´»æ€§ã€‚
</details>

**ç»ƒä¹  2.2ï¼šå®éªŒè¿½è¸ªå·¥å…·é›†æˆ**
è®¾è®¡ä¸€ä¸ªç»Ÿä¸€çš„æ¥å£ï¼Œèƒ½å¤ŸåŒæ—¶å‘ MLflow å’Œ W&B è®°å½•å®éªŒæŒ‡æ ‡ã€‚è¦æ±‚æ”¯æŒæ‰¹é‡è®°å½•å’Œå¼‚æ­¥å†™å…¥ã€‚

*Hint: ä½¿ç”¨é€‚é…å™¨æ¨¡å¼å’Œé˜Ÿåˆ—æœºåˆ¶ã€‚*

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

```python
from abc import ABC, abstractmethod
from queue import Queue
from threading import Thread

class ExperimentTracker(ABC):
    @abstractmethod
    def log_metrics(self, metrics: dict, step: int): pass

class UnifiedTracker:
    def __init__(self):
        self.trackers = []
        self.queue = Queue()
        self.worker = Thread(target=self._process_queue)
        self.worker.start()
    
    def add_tracker(self, tracker: ExperimentTracker):
        self.trackers.append(tracker)
    
    def log_metrics(self, metrics: dict, step: int):
        # å¼‚æ­¥è®°å½•
        self.queue.put(("metrics", metrics, step))
    
    def _process_queue(self):
        while True:
            item = self.queue.get()
            if item is None:
                break
            event_type, data, step = item
            for tracker in self.trackers:
                try:
                    tracker.log_metrics(data, step)
                except Exception as e:
                    print(f"Tracker failed: {e}")
```

å…³é”®ç‚¹ï¼š
1. ç»Ÿä¸€æ¥å£æŠ½è±¡
2. å¼‚æ­¥é˜Ÿåˆ—é¿å…é˜»å¡è®­ç»ƒ
3. é”™è¯¯éš”ç¦»ï¼Œå•ä¸ª tracker å¤±è´¥ä¸å½±å“å…¶ä»–
</details>

**ç»ƒä¹  2.3ï¼šGit åˆ†æ”¯æ¸…ç†ç­–ç•¥**
ç¼–å†™ä¸€ä¸ªè„šæœ¬ï¼Œè‡ªåŠ¨æ¸…ç†å®éªŒåˆ†æ”¯ã€‚è¦æ±‚ï¼š(1) ä¿ç•™æœ€è¿‘ 30 å¤©çš„åˆ†æ”¯ï¼›(2) ä¿ç•™æœ‰æœªåˆå¹¶æäº¤çš„åˆ†æ”¯ï¼›(3) å½’æ¡£é‡è¦å®éªŒç»“æœã€‚

*Hint: ä½¿ç”¨ git for-each-ref å’Œ git cherry å‘½ä»¤ã€‚*

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

```bash
#!/bin/bash

cleanup_experimental_branches() {
    local cutoff_date=$(date -d "30 days ago" +%s)
    
    git for-each-ref --format='%(refname:short) %(committerdate:unix)' refs/heads/exp/ | \
    while read branch timestamp; do
        # æ£€æŸ¥å¹´é¾„
        if [ $timestamp -lt $cutoff_date ]; then
            # æ£€æŸ¥æ˜¯å¦æœ‰æœªåˆå¹¶çš„æäº¤
            unmerged=$(git cherry main $branch | grep "^+" | wc -l)
            
            if [ $unmerged -eq 0 ]; then
                # æ£€æŸ¥æ˜¯å¦æ ‡è®°ä¸ºé‡è¦
                if git tag --list "important/$branch" | grep -q .; then
                    # å½’æ¡£è€Œéåˆ é™¤
                    git tag -a "archive/$(date +%Y%m)/$branch" $branch -m "Auto-archived"
                fi
                git branch -D $branch
                echo "Deleted: $branch"
            else
                echo "Kept (unmerged): $branch"
            fi
        fi
    done
}
```

å…³é”®æ£€æŸ¥ï¼š
1. æ—¶é—´æˆ³æ¯”è¾ƒ
2. æœªåˆå¹¶æäº¤æ£€æµ‹
3. é‡è¦æ€§æ ‡è®°è¯†åˆ«
</details>

### æŒ‘æˆ˜é¢˜

**ç»ƒä¹  2.4ï¼šé…ç½®å·®å¼‚åˆ†æ**
å®ç°ä¸€ä¸ªå·¥å…·ï¼Œèƒ½å¤Ÿï¼š(1) æ¯”è¾ƒä¸¤ä¸ªå®éªŒçš„é…ç½®å·®å¼‚ï¼›(2) è¯†åˆ«å“ªäº›é…ç½®å˜åŒ–å¯¼è‡´äº†æ€§èƒ½æå‡ï¼›(3) ç”Ÿæˆé…ç½®ä¼˜åŒ–å»ºè®®ã€‚

*Hint: è€ƒè™‘ä½¿ç”¨å†³ç­–æ ‘æˆ– SHAP å€¼åˆ†æé…ç½®é‡è¦æ€§ã€‚*

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

```python
import numpy as np
from sklearn.ensemble import RandomForestRegressor
import shap

class ConfigAnalyzer:
    def __init__(self, experiments: List[Dict]):
        self.experiments = experiments
        self.feature_names = self._extract_features()
        
    def _extract_features(self):
        # æå–æ‰€æœ‰é…ç½®é”®
        all_keys = set()
        for exp in self.experiments:
            all_keys.update(self._flatten_dict(exp['config']).keys())
        return sorted(all_keys)
    
    def _flatten_dict(self, d, parent_key=''):
        items = []
        for k, v in d.items():
            new_key = f"{parent_key}.{k}" if parent_key else k
            if isinstance(v, dict):
                items.extend(self._flatten_dict(v, new_key).items())
            else:
                items.append((new_key, v))
        return dict(items)
    
    def analyze_importance(self, metric='accuracy'):
        # å‡†å¤‡æ•°æ®
        X = []
        y = []
        for exp in self.experiments:
            flat_config = self._flatten_dict(exp['config'])
            features = [flat_config.get(k, 0) for k in self.feature_names]
            X.append(features)
            y.append(exp['metrics'][metric])
        
        # è®­ç»ƒæ¨¡å‹
        model = RandomForestRegressor(n_estimators=100)
        model.fit(X, y)
        
        # SHAP åˆ†æ
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X)
        
        # ç”Ÿæˆé‡è¦æ€§æ’å
        importance = {}
        for i, name in enumerate(self.feature_names):
            importance[name] = np.abs(shap_values[:, i]).mean()
        
        return sorted(importance.items(), key=lambda x: x[1], reverse=True)
    
    def suggest_optimization(self, current_config: dict):
        importance = self.analyze_importance()
        suggestions = []
        
        # æ‰¾å‡ºè¡¨ç°æœ€å¥½çš„é…ç½®
        best_exp = max(self.experiments, key=lambda x: x['metrics']['accuracy'])
        best_config = self._flatten_dict(best_exp['config'])
        current_flat = self._flatten_dict(current_config)
        
        # åŸºäºé‡è¦æ€§ç”Ÿæˆå»ºè®®
        for param, imp_score in importance[:5]:  # Top 5 é‡è¦å‚æ•°
            if param in best_config and param in current_flat:
                if best_config[param] != current_flat[param]:
                    suggestions.append({
                        'parameter': param,
                        'current': current_flat[param],
                        'suggested': best_config[param],
                        'importance': imp_score
                    })
        
        return suggestions
```

æ ¸å¿ƒæ€è·¯ï¼š
1. ä½¿ç”¨éšæœºæ£®æ—å­¦ä¹ é…ç½®åˆ°æ€§èƒ½çš„æ˜ å°„
2. SHAP å€¼é‡åŒ–æ¯ä¸ªé…ç½®çš„è´¡çŒ®
3. åŸºäºå†å²æœ€ä½³å®è·µç”Ÿæˆä¼˜åŒ–å»ºè®®
</details>

**ç»ƒä¹  2.5ï¼šå®éªŒä»£ç ç‰ˆæœ¬éš”ç¦»**
è®¾è®¡ä¸€ä¸ªç³»ç»Ÿï¼Œèƒ½å¤Ÿä¸ºæ¯ä¸ªå®éªŒåˆ›å»ºéš”ç¦»çš„ä»£ç ç¯å¢ƒï¼Œæ”¯æŒï¼š(1) ä»£ç å¿«ç…§ï¼›(2) ä¾èµ–ç‰ˆæœ¬é”å®šï¼›(3) å¿«é€Ÿåˆ‡æ¢å’Œæ¢å¤ã€‚

*Hint: ç»“åˆ Git worktreeã€Docker æˆ– Python è™šæ‹Ÿç¯å¢ƒã€‚*

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

```python
import subprocess
import json
from pathlib import Path
import venv

class ExperimentEnvironment:
    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.envs_dir = base_dir / "environments"
        self.envs_dir.mkdir(exist_ok=True)
        
    def create_environment(self, exp_id: str, config: dict):
        env_path = self.envs_dir / exp_id
        
        # 1. åˆ›å»º Git worktree
        worktree_path = env_path / "code"
        subprocess.run([
            "git", "worktree", "add", 
            str(worktree_path), 
            config.get("git_commit", "HEAD")
        ])
        
        # 2. åˆ›å»º Python è™šæ‹Ÿç¯å¢ƒ
        venv_path = env_path / "venv"
        venv.create(venv_path, with_pip=True)
        
        # 3. é”å®šä¾èµ–ç‰ˆæœ¬
        pip_path = venv_path / "bin" / "pip"
        requirements = config.get("requirements", [])
        
        # ç”Ÿæˆ requirements.txt
        req_file = env_path / "requirements.txt"
        with open(req_file, 'w') as f:
            for pkg in requirements:
                f.write(f"{pkg}\n")
        
        # å®‰è£…ä¾èµ–
        subprocess.run([
            str(pip_path), "install", "-r", str(req_file)
        ])
        
        # 4. ä¿å­˜ç¯å¢ƒå…ƒæ•°æ®
        metadata = {
            "exp_id": exp_id,
            "created_at": datetime.now().isoformat(),
            "git_commit": subprocess.check_output(
                ["git", "rev-parse", "HEAD"], 
                cwd=worktree_path
            ).decode().strip(),
            "config": config
        }
        
        with open(env_path / "metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        return env_path
    
    def activate_environment(self, exp_id: str):
        env_path = self.envs_dir / exp_id
        
        # ç”Ÿæˆæ¿€æ´»è„šæœ¬
        activate_script = f"""
#!/bin/bash
export EXPERIMENT_ID={exp_id}
export PYTHONPATH={env_path}/code:$PYTHONPATH
source {env_path}/venv/bin/activate
cd {env_path}/code
echo "Environment {exp_id} activated"
"""
        
        script_path = env_path / "activate.sh"
        with open(script_path, 'w') as f:
            f.write(activate_script)
        
        script_path.chmod(0o755)
        return script_path
    
    def cleanup_environment(self, exp_id: str, archive: bool = True):
        env_path = self.envs_dir / exp_id
        
        if archive:
            # å½’æ¡£é‡è¦æ–‡ä»¶
            archive_path = self.base_dir / "archives" / f"{exp_id}.tar.gz"
            archive_path.parent.mkdir(exist_ok=True)
            
            subprocess.run([
                "tar", "czf", str(archive_path),
                "-C", str(self.envs_dir),
                exp_id,
                "--exclude", "venv",
                "--exclude", ".git"
            ])
        
        # æ¸…ç† worktree
        worktree_path = env_path / "code"
        subprocess.run(["git", "worktree", "remove", str(worktree_path)])
        
        # åˆ é™¤ç¯å¢ƒç›®å½•
        import shutil
        shutil.rmtree(env_path)
```

å…³é”®ç‰¹æ€§ï¼š
1. Git worktree æä¾›ä»£ç éš”ç¦»
2. Python venv æä¾›ä¾èµ–éš”ç¦»  
3. å…ƒæ•°æ®è®°å½•ä¿è¯å¯è¿½æº¯æ€§
4. å½’æ¡£æœºåˆ¶ä¿ç•™é‡è¦å®éªŒ
</details>

**ç»ƒä¹  2.6ï¼šåˆ†å¸ƒå¼å®éªŒåè°ƒ**
è®¾è®¡ä¸€ä¸ªåˆ†å¸ƒå¼å®éªŒç®¡ç†ç³»ç»Ÿï¼Œæ”¯æŒï¼š(1) å¤šæœºå™¨ä¸Šçš„å®éªŒè°ƒåº¦ï¼›(2) èµ„æºï¼ˆGPUï¼‰åˆ†é…ï¼›(3) å®éªŒå¤±è´¥è‡ªåŠ¨é‡è¯•ã€‚

*Hint: è€ƒè™‘ä½¿ç”¨æ¶ˆæ¯é˜Ÿåˆ—å’ŒçŠ¶æ€æœºã€‚*

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

```python
from enum import Enum
from dataclasses import dataclass
import redis
import json
from typing import Optional

class ExperimentState(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    RETRYING = "retrying"

@dataclass
class ExperimentJob:
    exp_id: str
    config: dict
    state: ExperimentState
    assigned_worker: Optional[str] = None
    retry_count: int = 0
    max_retries: int = 3

class DistributedExperimentScheduler:
    def __init__(self, redis_host: str):
        self.redis = redis.Redis(host=redis_host, decode_responses=True)
        self.job_queue = "experiment_queue"
        self.worker_status = "worker_status"
        
    def submit_experiment(self, exp_id: str, config: dict):
        job = ExperimentJob(
            exp_id=exp_id,
            config=config,
            state=ExperimentState.PENDING
        )
        
        # åŠ å…¥é˜Ÿåˆ—
        self.redis.lpush(self.job_queue, json.dumps({
            'exp_id': job.exp_id,
            'config': job.config,
            'state': job.state.value,
            'retry_count': job.retry_count
        }))
        
        # è®°å½•ä½œä¸šçŠ¶æ€
        self.redis.hset(f"job:{exp_id}", mapping={
            'state': job.state.value,
            'submitted_at': datetime.now().isoformat()
        })
    
    def worker_loop(self, worker_id: str, resources: dict):
        """å·¥ä½œèŠ‚ç‚¹ä¸»å¾ªç¯"""
        
        # æ³¨å†Œå·¥ä½œèŠ‚ç‚¹
        self.redis.hset(self.worker_status, worker_id, json.dumps({
            'status': 'idle',
            'resources': resources,
            'last_heartbeat': datetime.now().isoformat()
        }))
        
        while True:
            # è·å–ä»»åŠ¡
            job_data = self.redis.brpop(self.job_queue, timeout=5)
            
            if job_data:
                _, job_str = job_data
                job = json.loads(job_str)
                
                # æ£€æŸ¥èµ„æºéœ€æ±‚
                if self._can_run(job['config'], resources):
                    self._run_experiment(worker_id, job)
                else:
                    # æ”¾å›é˜Ÿåˆ—æœ«å°¾
                    self.redis.lpush(self.job_queue, job_str)
            
            # å‘é€å¿ƒè·³
            self._heartbeat(worker_id)
    
    def _can_run(self, config: dict, resources: dict) -> bool:
        """æ£€æŸ¥èµ„æºæ˜¯å¦æ»¡è¶³éœ€æ±‚"""
        required_gpus = config.get('num_gpus', 1)
        available_gpus = resources.get('gpus', 0)
        
        required_memory = config.get('memory_gb', 16)
        available_memory = resources.get('memory_gb', 0)
        
        return (available_gpus >= required_gpus and 
                available_memory >= required_memory)
    
    def _run_experiment(self, worker_id: str, job: dict):
        exp_id = job['exp_id']
        
        try:
            # æ›´æ–°çŠ¶æ€
            self.redis.hset(f"job:{exp_id}", mapping={
                'state': ExperimentState.RUNNING.value,
                'worker': worker_id,
                'started_at': datetime.now().isoformat()
            })
            
            # æ›´æ–°å·¥ä½œèŠ‚ç‚¹çŠ¶æ€
            self.redis.hset(self.worker_status, worker_id, json.dumps({
                'status': 'busy',
                'current_job': exp_id
            }))
            
            # æ‰§è¡Œå®éªŒ
            result = self._execute_experiment(job['config'])
            
            # æ ‡è®°å®Œæˆ
            self.redis.hset(f"job:{exp_id}", mapping={
                'state': ExperimentState.COMPLETED.value,
                'completed_at': datetime.now().isoformat(),
                'result': json.dumps(result)
            })
            
        except Exception as e:
            # å¤„ç†å¤±è´¥
            self._handle_failure(exp_id, job, str(e))
    
    def _handle_failure(self, exp_id: str, job: dict, error: str):
        retry_count = job.get('retry_count', 0)
        max_retries = job.get('max_retries', 3)
        
        if retry_count < max_retries:
            # é‡è¯•
            job['retry_count'] = retry_count + 1
            job['state'] = ExperimentState.RETRYING.value
            
            # å»¶è¿Ÿé‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
            delay = 2 ** retry_count
            self.redis.lpush(f"{self.job_queue}:delayed:{delay}", 
                           json.dumps(job))
            
            self.redis.hset(f"job:{exp_id}", mapping={
                'state': ExperimentState.RETRYING.value,
                'retry_count': retry_count + 1,
                'last_error': error
            })
        else:
            # æœ€ç»ˆå¤±è´¥
            self.redis.hset(f"job:{exp_id}", mapping={
                'state': ExperimentState.FAILED.value,
                'failed_at': datetime.now().isoformat(),
                'error': error
            })
```

ç³»ç»Ÿè®¾è®¡è¦ç‚¹ï¼š
1. Redis ä½œä¸ºä¸­å¤®åè°ƒå™¨
2. åŸºäºèµ„æºçš„ä»»åŠ¡åˆ†é…
3. çŠ¶æ€æœºç®¡ç†å®éªŒç”Ÿå‘½å‘¨æœŸ
4. æŒ‡æ•°é€€é¿çš„é‡è¯•æœºåˆ¶
5. å¿ƒè·³æœºåˆ¶æ£€æµ‹å·¥ä½œèŠ‚ç‚¹å¥åº·
</details>

**ç»ƒä¹  2.7ï¼šä»£ç è…åŒ–åº¦é‡ä¸é¢„è­¦**
å¼€å‘ä¸€ä¸ªç³»ç»Ÿï¼Œèƒ½å¤Ÿï¼š(1) é‡åŒ–ä»£ç è…åŒ–ç¨‹åº¦ï¼›(2) é¢„æµ‹æŠ€æœ¯å€ºåŠ¡å¢é•¿è¶‹åŠ¿ï¼›(3) è‡ªåŠ¨ç”Ÿæˆé‡æ„å»ºè®®ã€‚

*Hint: ç»“åˆé™æ€åˆ†æã€Git å†å²å’Œå¤æ‚åº¦åº¦é‡ã€‚*

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

```python
import ast
import git
from datetime import datetime, timedelta
import numpy as np
from sklearn.linear_model import LinearRegression

class CodeHealthMonitor:
    def __init__(self, repo_path: str):
        self.repo = git.Repo(repo_path)
        self.metrics_history = []
        
    def calculate_health_score(self) -> float:
        """è®¡ç®—ä»£ç å¥åº·åº¦å¾—åˆ† (0-100)"""
        metrics = {
            'complexity': self._measure_complexity(),
            'duplication': self._measure_duplication(),
            'test_coverage': self._measure_test_coverage(),
            'debt_density': self._measure_technical_debt(),
            'change_frequency': self._measure_change_frequency()
        }
        
        # åŠ æƒè®¡ç®—æ€»åˆ†
        weights = {
            'complexity': -0.3,      # å¤æ‚åº¦è¶Šé«˜åˆ†æ•°è¶Šä½
            'duplication': -0.2,     # é‡å¤ä»£ç è¶Šå¤šåˆ†æ•°è¶Šä½
            'test_coverage': 0.25,   # æµ‹è¯•è¦†ç›–ç‡è¶Šé«˜åˆ†æ•°è¶Šé«˜
            'debt_density': -0.15,   # æŠ€æœ¯å€ºåŠ¡è¶Šå¤šåˆ†æ•°è¶Šä½
            'change_frequency': -0.1 # é¢‘ç¹ä¿®æ”¹çš„ä»£ç åˆ†æ•°è¶Šä½
        }
        
        score = 50  # åŸºå‡†åˆ†
        for metric, value in metrics.items():
            score += weights[metric] * value
            
        return max(0, min(100, score))
    
    def _measure_complexity(self) -> float:
        """æµ‹é‡åœˆå¤æ‚åº¦"""
        total_complexity = 0
        file_count = 0
        
        for py_file in Path(self.repo.working_dir).rglob("*.py"):
            with open(py_file) as f:
                tree = ast.parse(f.read())
                complexity = self._calculate_cyclomatic_complexity(tree)
                total_complexity += complexity
                file_count += 1
        
        return total_complexity / max(file_count, 1)
    
    def _measure_duplication(self) -> float:
        """æµ‹é‡ä»£ç é‡å¤ç‡"""
        # ä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•ï¼šç›¸ä¼¼ä»£ç å—æ£€æµ‹
        code_blocks = []
        
        for py_file in Path(self.repo.working_dir).rglob("*.py"):
            with open(py_file) as f:
                content = f.read()
                # æå–å‡½æ•°ä½“ä½œä¸ºä»£ç å—
                tree = ast.parse(content)
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        code_blocks.append(ast.unparse(node))
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        duplicates = 0
        for i, block1 in enumerate(code_blocks):
            for block2 in code_blocks[i+1:]:
                if self._similarity(block1, block2) > 0.8:
                    duplicates += 1
        
        return duplicates / max(len(code_blocks), 1) * 100
    
    def predict_debt_growth(self, days_ahead: int = 30) -> dict:
        """é¢„æµ‹æŠ€æœ¯å€ºåŠ¡å¢é•¿è¶‹åŠ¿"""
        
        # æ”¶é›†å†å²æ•°æ®
        history = []
        for days_ago in range(90, 0, -7):  # è¿‡å»90å¤©ï¼Œæ¯å‘¨é‡‡æ ·
            date = datetime.now() - timedelta(days=days_ago)
            commit = self._get_commit_at_date(date)
            
            if commit:
                self.repo.git.checkout(commit.hexsha)
                metrics = {
                    'date': date,
                    'debt_count': self._count_debt_markers(),
                    'complexity': self._measure_complexity()
                }
                history.append(metrics)
        
        # å›åˆ°å½“å‰åˆ†æ”¯
        self.repo.git.checkout('main')
        
        # çº¿æ€§å›å½’é¢„æµ‹
        X = np.array([i for i in range(len(history))]).reshape(-1, 1)
        y = np.array([h['debt_count'] for h in history])
        
        model = LinearRegression()
        model.fit(X, y)
        
        # é¢„æµ‹æœªæ¥
        future_x = len(history) + days_ahead // 7
        predicted_debt = model.predict([[future_x]])[0]
        
        return {
            'current_debt': history[-1]['debt_count'] if history else 0,
            'predicted_debt': predicted_debt,
            'growth_rate': model.coef_[0],
            'confidence': model.score(X, y)
        }
    
    def generate_refactoring_suggestions(self) -> list:
        """ç”Ÿæˆé‡æ„å»ºè®®"""
        suggestions = []
        
        # åˆ†æçƒ­ç‚¹æ–‡ä»¶ï¼ˆé¢‘ç¹ä¿®æ”¹ä¸”å¤æ‚åº¦é«˜ï¼‰
        hotspots = self._identify_hotspots()
        
        for file_path, metrics in hotspots[:5]:  # Top 5 çƒ­ç‚¹
            suggestion = {
                'file': file_path,
                'reason': [],
                'actions': []
            }
            
            if metrics['complexity'] > 10:
                suggestion['reason'].append(f"é«˜å¤æ‚åº¦: {metrics['complexity']}")
                suggestion['actions'].append("æ‹†åˆ†å¤§å‡½æ•°ä¸ºæ›´å°çš„åŠŸèƒ½å•å…ƒ")
            
            if metrics['change_frequency'] > 20:
                suggestion['reason'].append(f"é¢‘ç¹ä¿®æ”¹: {metrics['change_frequency']}æ¬¡/æœˆ")
                suggestion['actions'].append("è€ƒè™‘æŠ½è±¡å‡ºç¨³å®šæ¥å£")
            
            if metrics['duplication'] > 20:
                suggestion['reason'].append(f"ä»£ç é‡å¤: {metrics['duplication']}%")
                suggestion['actions'].append("æå–å…¬å…±ä»£ç åˆ°å·¥å…·æ¨¡å—")
            
            if metrics['test_coverage'] < 50:
                suggestion['reason'].append(f"æµ‹è¯•è¦†ç›–ç‡ä½: {metrics['test_coverage']}%")
                suggestion['actions'].append("å¢åŠ å•å…ƒæµ‹è¯•è¦†ç›–")
            
            suggestions.append(suggestion)
        
        return suggestions
    
    def _identify_hotspots(self) -> list:
        """è¯†åˆ«ä»£ç çƒ­ç‚¹"""
        file_metrics = {}
        
        # åˆ†æ Git å†å²
        for commit in self.repo.iter_commits('main', max_count=100):
            for file in commit.stats.files:
                if file.endswith('.py'):
                    if file not in file_metrics:
                        file_metrics[file] = {
                            'change_frequency': 0,
                            'complexity': 0,
                            'duplication': 0,
                            'test_coverage': 0
                        }
                    file_metrics[file]['change_frequency'] += 1
        
        # è®¡ç®—å½“å‰æŒ‡æ ‡
        for file_path in file_metrics:
            full_path = Path(self.repo.working_dir) / file_path
            if full_path.exists():
                with open(full_path) as f:
                    tree = ast.parse(f.read())
                    file_metrics[file_path]['complexity'] = \
                        self._calculate_cyclomatic_complexity(tree)
        
        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        scored = []
        for file, metrics in file_metrics.items():
            score = (metrics['change_frequency'] * 0.4 + 
                    metrics['complexity'] * 0.4 +
                    metrics['duplication'] * 0.2)
            scored.append((file, metrics, score))
        
        return sorted(scored, key=lambda x: x[2], reverse=True)
```

ç›‘æ§ç³»ç»Ÿç‰¹ç‚¹ï¼š
1. å¤šç»´åº¦å¥åº·è¯„åˆ†
2. åŸºäºå†å²æ•°æ®çš„è¶‹åŠ¿é¢„æµ‹
3. çƒ­ç‚¹åˆ†æè¯†åˆ«é—®é¢˜åŒºåŸŸ
4. å¯æ“ä½œçš„é‡æ„å»ºè®®
5. æŒç»­ç›‘æ§å’Œé¢„è­¦æœºåˆ¶
</details>

**ç»ƒä¹  2.8ï¼šå®éªŒç»“æœè‡ªåŠ¨åˆ†æ**
å®ç°ä¸€ä¸ªç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨åˆ†æå®éªŒç»“æœï¼Œè¯†åˆ«ï¼š(1) å¼‚å¸¸å®éªŒï¼›(2) æ€§èƒ½ç“¶é¢ˆï¼›(3) æœ€ä¼˜é…ç½®ç»„åˆã€‚

*Hint: ä½¿ç”¨å¼‚å¸¸æ£€æµ‹ã€æ€§èƒ½å‰–æå’Œè´å¶æ–¯ä¼˜åŒ–ã€‚*

<details>
<summary>å‚è€ƒç­”æ¡ˆ</summary>

```python
from scipy import stats
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern
import numpy as np

class ExperimentAnalyzer:
    def __init__(self, experiment_history: list):
        self.history = experiment_history
        
    def detect_anomalies(self) -> list:
        """æ£€æµ‹å¼‚å¸¸å®éªŒ"""
        anomalies = []
        
        # æå–å…³é”®æŒ‡æ ‡
        metrics = ['loss', 'accuracy', 'training_time']
        
        for metric in metrics:
            values = [exp['metrics'].get(metric, 0) for exp in self.history]
            
            if len(values) < 3:
                continue
                
            # ä½¿ç”¨ IQR æ–¹æ³•æ£€æµ‹å¼‚å¸¸
            q1, q3 = np.percentile(values, [25, 75])
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            for i, exp in enumerate(self.history):
                value = exp['metrics'].get(metric, 0)
                if value < lower_bound or value > upper_bound:
                    anomalies.append({
                        'exp_id': exp['id'],
                        'metric': metric,
                        'value': value,
                        'expected_range': (lower_bound, upper_bound),
                        'severity': 'high' if abs(value - np.mean(values)) > 3 * np.std(values) else 'medium'
                    })
        
        # æ£€æµ‹è®­ç»ƒæ›²çº¿å¼‚å¸¸
        for exp in self.history:
            if 'training_curve' in exp:
                curve_anomalies = self._detect_curve_anomalies(exp['training_curve'])
                if curve_anomalies:
                    anomalies.extend(curve_anomalies)
        
        return anomalies
    
    def _detect_curve_anomalies(self, curve: list) -> list:
        """æ£€æµ‹è®­ç»ƒæ›²çº¿å¼‚å¸¸"""
        anomalies = []
        
        # æ£€æµ‹ loss çˆ†ç‚¸
        losses = [point['loss'] for point in curve]
        if any(np.isnan(losses)) or any(np.isinf(losses)):
            anomalies.append({
                'type': 'loss_explosion',
                'severity': 'critical'
            })
        
        # æ£€æµ‹è¿‡æ‹Ÿåˆ
        if len(curve) > 10:
            train_acc = [p.get('train_acc', 0) for p in curve[-10:]]
            val_acc = [p.get('val_acc', 0) for p in curve[-10:]]
            
            if train_acc and val_acc:
                gap = np.mean(train_acc) - np.mean(val_acc)
                if gap > 0.1:  # 10% å·®è·
                    anomalies.append({
                        'type': 'overfitting',
                        'severity': 'medium',
                        'train_val_gap': gap
                    })
        
        return anomalies
    
    def identify_bottlenecks(self) -> dict:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = {
            'data_loading': [],
            'forward_pass': [],
            'backward_pass': [],
            'optimizer_step': []
        }
        
        for exp in self.history:
            if 'profiling' not in exp:
                continue
                
            prof = exp['profiling']
            
            # åˆ†æå„é˜¶æ®µè€—æ—¶
            total_time = sum(prof.values())
            
            for stage, time in prof.items():
                percentage = (time / total_time) * 100
                
                # å¦‚æœæŸé˜¶æ®µå æ¯”å¼‚å¸¸é«˜
                expected_percentages = {
                    'data_loading': 10,
                    'forward_pass': 30,
                    'backward_pass': 40,
                    'optimizer_step': 20
                }
                
                if stage in expected_percentages:
                    expected = expected_percentages[stage]
                    if percentage > expected * 1.5:  # è¶…è¿‡é¢„æœŸ 50%
                        bottlenecks[stage].append({
                            'exp_id': exp['id'],
                            'percentage': percentage,
                            'expected': expected,
                            'suggestions': self._get_optimization_suggestions(stage)
                        })
        
        return bottlenecks
    
    def _get_optimization_suggestions(self, stage: str) -> list:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        suggestions = {
            'data_loading': [
                "å¢åŠ æ•°æ®åŠ è½½çš„ num_workers",
                "ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®æ ¼å¼ (å¦‚ HDF5)",
                "å®ç°æ•°æ®é¢„å–å’Œç¼“å­˜",
                "è€ƒè™‘ä½¿ç”¨ DALI æˆ–å…¶ä»–åŠ é€Ÿåº“"
            ],
            'forward_pass': [
                "ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)",
                "å¯ç”¨ cudnn.benchmark",
                "è€ƒè™‘æ¨¡å‹å‰ªææˆ–é‡åŒ–",
                "ä½¿ç”¨æ›´é«˜æ•ˆçš„ç®—å­å®ç°"
            ],
            'backward_pass': [
                "ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å‡å°‘æ˜¾å­˜å ç”¨",
                "å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹",
                "è€ƒè™‘ä½¿ç”¨ ZeRO ä¼˜åŒ–å™¨",
                "æ£€æŸ¥æ˜¯å¦æœ‰ä¸å¿…è¦çš„æ¢¯åº¦è®¡ç®—"
            ],
            'optimizer_step': [
                "ä½¿ç”¨èåˆä¼˜åŒ–å™¨ (å¦‚ FusedAdam)",
                "å‡å°‘å‚æ•°æ›´æ–°é¢‘ç‡",
                "è€ƒè™‘ä½¿ç”¨ LARS æˆ– LAMB",
                "æ£€æŸ¥æƒé‡è¡°å‡è®¾ç½®"
            ]
        }
        
        return suggestions.get(stage, [])
    
    def find_optimal_config(self) -> dict:
        """ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–æ‰¾åˆ°æœ€ä¼˜é…ç½®"""
        
        # å‡†å¤‡æ•°æ®
        configs = []
        scores = []
        
        param_names = set()
        for exp in self.history:
            flat_config = self._flatten_config(exp['config'])
            param_names.update(flat_config.keys())
            configs.append(flat_config)
            scores.append(exp['metrics'].get('accuracy', 0))
        
        param_names = sorted(param_names)
        
        # è½¬æ¢ä¸ºæ•°å€¼çŸ©é˜µ
        X = []
        for config in configs:
            x = []
            for param in param_names:
                value = config.get(param, 0)
                # ç®€å•çš„æ•°å€¼åŒ–
                if isinstance(value, bool):
                    x.append(float(value))
                elif isinstance(value, str):
                    x.append(hash(value) % 100)  # ç®€åŒ–å¤„ç†
                else:
                    x.append(float(value))
            X.append(x)
        
        X = np.array(X)
        y = np.array(scores)
        
        # é«˜æ–¯è¿‡ç¨‹å›å½’
        kernel = Matern(length_scale=1.0, nu=2.5)
        gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-6)
        gpr.fit(X, y)
        
        # è´å¶æ–¯ä¼˜åŒ–ï¼šæ‰¾ä¸‹ä¸€ä¸ªæœ€ä½³ç‚¹
        def acquisition_function(x):
            """Expected Improvement"""
            mu, sigma = gpr.predict(x.reshape(1, -1), return_std=True)
            
            best_y = np.max(y)
            z = (mu - best_y - 0.01) / sigma
            ei = (mu - best_y - 0.01) * stats.norm.cdf(z) + sigma * stats.norm.pdf(z)
            
            return ei[0]
        
        # éšæœºæœç´¢æ‰¾æœ€å¤§ EI
        best_ei = -np.inf
        best_config = None
        
        for _ in range(1000):
            # åœ¨å·²æœ‰é…ç½®é™„è¿‘é‡‡æ ·
            idx = np.random.randint(len(X))
            candidate = X[idx] + np.random.randn(len(param_names)) * 0.1
            
            ei = acquisition_function(candidate)
            if ei > best_ei:
                best_ei = ei
                best_config = candidate
        
        # è½¬æ¢å›é…ç½®å­—å…¸
        optimal_config = {}
        for i, param in enumerate(param_names):
            optimal_config[param] = best_config[i]
        
        # é¢„æµ‹æ€§èƒ½
        predicted_score, uncertainty = gpr.predict(
            best_config.reshape(1, -1), 
            return_std=True
        )
        
        return {
            'config': optimal_config,
            'predicted_score': predicted_score[0],
            'uncertainty': uncertainty[0],
            'expected_improvement': best_ei
        }
    
    def _flatten_config(self, config: dict, prefix: str = '') -> dict:
        """å±•å¹³åµŒå¥—é…ç½®"""
        flat = {}
        for key, value in config.items():
            full_key = f"{prefix}.{key}" if prefix else key
            if isinstance(value, dict):
                flat.update(self._flatten_config(value, full_key))
            else:
                flat[full_key] = value
        return flat
```

åˆ†æç³»ç»ŸåŠŸèƒ½ï¼š
1. å¼‚å¸¸æ£€æµ‹ï¼šIQR æ–¹æ³• + æ›²çº¿åˆ†æ
2. ç“¶é¢ˆè¯†åˆ«ï¼šæ€§èƒ½å‰–æ + é˜¶æ®µåˆ†æ
3. é…ç½®ä¼˜åŒ–ï¼šè´å¶æ–¯ä¼˜åŒ– + é«˜æ–¯è¿‡ç¨‹
4. å¯æ“ä½œå»ºè®®ï¼šé’ˆå¯¹æ€§ä¼˜åŒ–æ–¹æ¡ˆ
5. ä¸ç¡®å®šæ€§é‡åŒ–ï¼šé¢„æµ‹ç½®ä¿¡åº¦
</details>

---

é€šè¿‡å®Œæˆè¿™äº›ç»ƒä¹ ï¼Œä½ å°†æŒæ¡æ„å»ºå¥å£®çš„ LLM åè®­ç»ƒå®éªŒåŸºç¡€è®¾æ–½çš„æ ¸å¿ƒæŠ€èƒ½ã€‚è®°ä½ï¼Œå¥½çš„åŸºç¡€è®¾æ–½æ˜¯é«˜æ•ˆå®éªŒçš„åŸºçŸ³ï¼Œå€¼å¾—åœ¨é¡¹ç›®åˆæœŸæŠ•å…¥æ—¶é—´å»ºè®¾ã€‚
